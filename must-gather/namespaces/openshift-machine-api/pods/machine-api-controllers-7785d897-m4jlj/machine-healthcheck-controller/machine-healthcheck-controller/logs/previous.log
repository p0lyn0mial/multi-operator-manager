2024-10-24T13:05:41.000839798Z I1024 13:05:41.000502       1 leaderelection.go:121] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2024-10-24T13:05:41.001013848Z I1024 13:05:41.000872       1 main.go:30] Go Version: go1.22.7 (Red Hat 1.22.7-1.el9_5) X:strictfipsruntime
2024-10-24T13:05:41.001013848Z I1024 13:05:41.000881       1 main.go:31] Go OS/Arch: linux/amd64
2024-10-24T13:05:41.001139768Z I1024 13:05:41.001072       1 leaderelection.go:121] The leader election gives 4 retries and allows for 13s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m26s. Worst graceful lease acquisition is {26s}.
2024-10-24T13:05:41.001213618Z I1024 13:05:41.001175       1 main.go:113] Watching machine-api objects only in namespace "openshift-machine-api" for reconciliation.
2024-10-24T13:05:41.002224018Z I1024 13:05:41.002140       1 main.go:121] Registering Components.
2024-10-24T13:05:41.009024098Z I1024 13:05:41.008970       1 main.go:144] Starting the Cmd.
2024-10-24T13:05:41.009512008Z I1024 13:05:41.009457       1 server.go:208] "Starting metrics server" logger="controller-runtime.metrics"
2024-10-24T13:05:41.009620708Z I1024 13:05:41.009570       1 server.go:83] "starting server" name="health probe" addr="[::]:9442"
2024-10-24T13:05:41.009735828Z I1024 13:05:41.009712       1 server.go:247] "Serving metrics server" logger="controller-runtime.metrics" bindAddress=":8083" secure=false
2024-10-24T13:05:41.009789968Z I1024 13:05:41.009764       1 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
2024-10-24T13:05:41.009789968Z I1024 13:05:41.009783       1 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
2024-10-24T13:05:41.009833118Z I1024 13:05:41.009811       1 reflector.go:305] Starting reflector *v1beta1.Machine (10h48m11.34682445s) from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.009833118Z I1024 13:05:41.009827       1 reflector.go:341] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.014269318Z I1024 13:05:41.014183       1 reflector.go:368] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.110851539Z I1024 13:05:41.110785       1 leaderelection.go:254] attempting to acquire leader lease openshift-machine-api/cluster-api-provider-healthcheck-leader...
2024-10-24T13:05:41.124773799Z I1024 13:05:41.124723       1 leaderelection.go:268] successfully acquired lease openshift-machine-api/cluster-api-provider-healthcheck-leader
2024-10-24T13:05:41.125261079Z I1024 13:05:41.125198       1 recorder.go:104] "machine-api-controllers-7785d897-m4jlj_688500ac-76ce-44d3-89ce-49bf800eafcf became leader" logger="events" type="Normal" object={"kind":"Lease","namespace":"openshift-machine-api","name":"cluster-api-provider-healthcheck-leader","uid":"9ea7a49c-3bf2-40de-bae7-a7442853ecbf","apiVersion":"coordination.k8s.io/v1","resourceVersion":"11945"} reason="LeaderElection"
2024-10-24T13:05:41.125342999Z I1024 13:05:41.125318       1 controller.go:175] "Starting EventSource" controller="machinehealthcheck-controller" source="kind source: *v1beta1.MachineHealthCheck"
2024-10-24T13:05:41.125355089Z I1024 13:05:41.125346       1 controller.go:175] "Starting EventSource" controller="machinehealthcheck-controller" source="kind source: *v1beta1.Machine"
2024-10-24T13:05:41.125364239Z I1024 13:05:41.125358       1 controller.go:175] "Starting EventSource" controller="machinehealthcheck-controller" source="kind source: *v1.Node"
2024-10-24T13:05:41.125388559Z I1024 13:05:41.125366       1 controller.go:183] "Starting Controller" controller="machinehealthcheck-controller"
2024-10-24T13:05:41.128179149Z I1024 13:05:41.128110       1 reflector.go:305] Starting reflector *v1.Node (9h8m6.919193101s) from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.128179149Z I1024 13:05:41.128153       1 reflector.go:341] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.128401689Z I1024 13:05:41.128357       1 reflector.go:305] Starting reflector *v1beta1.MachineHealthCheck (10h48m16.029929824s) from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.128401689Z I1024 13:05:41.128373       1 reflector.go:341] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.130631599Z I1024 13:05:41.130569       1 reflector.go:368] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.131518379Z I1024 13:05:41.131465       1 reflector.go:368] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:05:41.230536159Z W1024 13:05:41.230483       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:05:41.230749719Z W1024 13:05:41.230729       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:05:41.230918319Z W1024 13:05:41.230899       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:05:41.231044619Z I1024 13:05:41.231022       1 controller.go:217] "Starting workers" controller="machinehealthcheck-controller" worker count=1
2024-10-24T13:05:43.717822222Z W1024 13:05:43.717422       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:05:43.717944442Z W1024 13:05:43.717923       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:05:50.736589749Z W1024 13:05:50.734527       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:05:50.736589749Z W1024 13:05:50.734621       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:10.496429943Z W1024 13:06:10.496024       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:06:10.496429943Z W1024 13:06:10.496239       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:06:11.024875276Z W1024 13:06:11.024786       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:11.024875276Z W1024 13:06:11.024848       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:14.632048254Z W1024 13:06:14.631537       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:06:14.632205244Z W1024 13:06:14.632185       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:06:21.250862828Z W1024 13:06:21.250780       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:21.250917928Z W1024 13:06:21.250855       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:40.960596850Z W1024 13:06:40.960402       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:06:40.960631550Z W1024 13:06:40.960615       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:06:41.495182458Z W1024 13:06:41.495080       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:41.495243768Z W1024 13:06:41.495198       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:45.288647604Z W1024 13:06:45.287255       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:06:45.288647604Z W1024 13:06:45.287785       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:06:46.605946954Z W1024 13:06:46.605901       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:46.606089994Z W1024 13:06:46.606069       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:51.655908604Z W1024 13:06:51.655841       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:51.656099334Z W1024 13:06:51.656064       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:06:55.562956179Z W1024 13:06:55.562910       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:06:55.563099399Z W1024 13:06:55.563068       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:01.773230085Z W1024 13:07:01.773174       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:07:01.773392834Z W1024 13:07:01.773350       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:07:16.122275661Z W1024 13:07:16.122182       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:16.122275661Z W1024 13:07:16.122250       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:26.508636087Z W1024 13:07:26.508152       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:26.508788196Z W1024 13:07:26.508768       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:31.950942212Z W1024 13:07:31.950856       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:07:31.951001212Z W1024 13:07:31.950941       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:07:32.429069325Z W1024 13:07:32.429008       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:07:32.429112085Z W1024 13:07:32.429103       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:07:36.731607267Z W1024 13:07:36.731535       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:36.731636327Z W1024 13:07:36.731609       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:42.192783814Z W1024 13:07:42.192690       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:07:42.192783814Z W1024 13:07:42.192753       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:07:46.884566061Z W1024 13:07:46.884498       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:46.884700081Z W1024 13:07:46.884680       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:07:52.385177279Z W1024 13:07:52.385082       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:07:52.385385319Z W1024 13:07:52.385319       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:08:00.317273224Z W1024 13:08:00.317171       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:08:00.317273224Z W1024 13:08:00.317238       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:08:00.347066292Z W1024 13:08:00.346992       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:08:00.347362812Z W1024 13:08:00.347196       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:08:00.391381429Z W1024 13:08:00.391284       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:08:00.391381429Z W1024 13:08:00.391353       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:08:02.839023505Z W1024 13:08:02.838965       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:08:02.839095414Z W1024 13:08:02.839042       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-0": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-0, got: []
2024-10-24T13:08:02.839383315Z W1024 13:08:02.839325       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:08:02.839536655Z W1024 13:08:02.839517       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-2, got: []
2024-10-24T13:08:02.839941545Z W1024 13:08:02.839909       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:08:02.839978274Z W1024 13:08:02.839957       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-master-1": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-master-1, got: []
2024-10-24T13:09:40.201951795Z E1024 13:09:40.201878       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:09:40.202460345Z E1024 13:09:40.202418       1 leaderelection.go:436] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:10:06.203963867Z E1024 13:10:06.203869       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:10:06.204705077Z E1024 13:10:06.204614       1 leaderelection.go:436] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:11:29.793557202Z I1024 13:11:29.793109       1 reflector.go:341] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:11:29.796539582Z I1024 13:11:29.796462       1 reflector.go:368] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:11:35.325801514Z I1024 13:11:35.325729       1 reflector.go:341] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:11:35.327788774Z I1024 13:11:35.327719       1 reflector.go:368] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:11:36.272772005Z I1024 13:11:36.272666       1 reflector.go:341] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:11:36.278019165Z I1024 13:11:36.277911       1 reflector.go:368] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:12:42.253299814Z E1024 13:12:42.253183       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:12:42.254392284Z E1024 13:12:42.254332       1 leaderelection.go:436] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:08.256943638Z E1024 13:13:08.256851       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:13:08.258771207Z E1024 13:13:08.258687       1 leaderelection.go:436] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:14:25.856141258Z I1024 13:14:25.856044       1 reflector.go:341] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:14:25.873778835Z I1024 13:14:25.873716       1 reflector.go:368] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:14:25.874681705Z W1024 13:14:25.874437       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-worker-a-8hj4x": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-worker-a-8hj4x, got: []
2024-10-24T13:14:33.343836609Z E1024 13:14:33.343719       1 leaderelection.go:429] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:14:40.201753618Z I1024 13:14:40.201691       1 reflector.go:341] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:14:40.217590855Z I1024 13:14:40.217534       1 reflector.go:368] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:14:48.438671056Z I1024 13:14:48.438337       1 reflector.go:341] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:15:18.426004692Z I1024 13:15:18.425947       1 trace.go:236] Trace[589624590]: "Reflector ListAndWatch" name:sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106 (24-Oct-2024 13:14:48.438) (total time: 29987ms):
2024-10-24T13:15:18.426004692Z Trace[589624590]: ---"Objects listed" error:<nil> 29987ms (13:15:18.425)
2024-10-24T13:15:18.426004692Z Trace[589624590]: [29.987474335s] [29.987474335s] END
2024-10-24T13:15:18.426626112Z I1024 13:15:18.426600       1 reflector.go:368] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:16:08.209281030Z I1024 13:16:08.209220       1 reflector.go:341] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:16:08.218581740Z I1024 13:16:08.218520       1 reflector.go:368] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:16:15.573819144Z I1024 13:16:15.573760       1 reflector.go:341] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:16:15.598161133Z I1024 13:16:15.596693       1 reflector.go:368] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106
2024-10-24T13:16:15.620755962Z W1024 13:16:15.611718       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-worker-b-hj8l2": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-worker-b-hj8l2, got: []
2024-10-24T13:16:15.620755962Z W1024 13:16:15.614283       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-2fcpj5j6-f6035-2lklf-worker-c-z8hfz": expecting one machine for node ci-op-2fcpj5j6-f6035-2lklf-worker-c-z8hfz, got: []
2024-10-24T13:16:17.621668730Z E1024 13:16:17.621593       1 leaderelection.go:429] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:17:17.623519931Z E1024 13:17:17.623461       1 leaderelection.go:436] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io cluster-api-provider-healthcheck-leader)
2024-10-24T13:17:50.634321849Z E1024 13:17:50.634242       1 leaderelection.go:429] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:17:57.549482208Z E1024 13:17:57.549414       1 leaderelection.go:436] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": context deadline exceeded
2024-10-24T13:17:57.549571488Z I1024 13:17:57.549548       1 leaderelection.go:297] failed to renew lease openshift-machine-api/cluster-api-provider-healthcheck-leader: timed out waiting for the condition
2024-10-24T13:17:57.549681697Z F1024 13:17:57.549661       1 main.go:148] leader election lost
