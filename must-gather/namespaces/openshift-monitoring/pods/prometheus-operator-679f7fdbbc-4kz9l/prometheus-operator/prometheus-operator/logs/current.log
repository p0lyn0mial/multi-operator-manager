2024-10-24T13:22:06.274893946Z ts=2024-10-24T13:22:06.274728076Z level=info caller=/go/src/github.com/coreos/prometheus-operator/cmd/operator/main.go:215 msg="Starting Prometheus Operator" version="(version=0.77.2, branch=master, revision=b52d35bcc)" build_context="(go=go1.22.7 (Red Hat 1.22.7-1.el9_5) X:strictfipsruntime, platform=linux/amd64, user=root, date=20241022-15:06:16, tags=strictfipsruntime)" feature_gates="PrometheusAgentDaemonSet=false"
2024-10-24T13:22:06.275345666Z ts=2024-10-24T13:22:06.275292826Z level=info caller=/go/src/github.com/coreos/prometheus-operator/internal/goruntime/cpu.go:27 msg="Leaving GOMAXPROCS=6: CPU quota undefined"
2024-10-24T13:22:06.275358646Z ts=2024-10-24T13:22:06.275339536Z level=info caller=/go/src/github.com/coreos/prometheus-operator/cmd/operator/main.go:228 msg="namespaces filtering configuration " config="{allow_list=\"\",deny_list=\"\",prometheus_allow_list=\"openshift-monitoring\",alertmanager_allow_list=\"openshift-monitoring\",alertmanagerconfig_allow_list=\"\",thanosruler_allow_list=\"openshift-monitoring\"}"
2024-10-24T13:22:06.289482275Z ts=2024-10-24T13:22:06.289344805Z level=info caller=/go/src/github.com/coreos/prometheus-operator/cmd/operator/main.go:269 msg="connection established" kubernetes_version=1.31.1-dirty
2024-10-24T13:22:06.305664343Z ts=2024-10-24T13:22:06.305589903Z level=warn caller=/go/src/github.com/coreos/prometheus-operator/cmd/operator/main.go:83 msg="resource \"scrapeconfigs\" (group: \"monitoring.coreos.com/v1alpha1\") not installed in the cluster"
2024-10-24T13:22:06.305805163Z ts=2024-10-24T13:22:06.305730773Z level=info caller=/go/src/github.com/coreos/prometheus-operator/cmd/operator/main.go:350 msg="Kubernetes API capabilities" endpointslices=true
2024-10-24T13:22:06.342394200Z ts=2024-10-24T13:22:06.34227295Z level=warn caller=/go/src/github.com/coreos/prometheus-operator/cmd/operator/main.go:83 msg="resource \"prometheusagents\" (group: \"monitoring.coreos.com/v1alpha1\") not installed in the cluster"
2024-10-24T13:22:06.418950673Z ts=2024-10-24T13:22:06.418810193Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/server/server.go:299 msg="starting insecure server" address=127.0.0.1:8080
2024-10-24T13:22:06.419974102Z ts=2024-10-24T13:22:06.419854233Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for alertmanager"
2024-10-24T13:22:06.420080553Z ts=2024-10-24T13:22:06.419966973Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for thanos"
2024-10-24T13:22:06.420518072Z ts=2024-10-24T13:22:06.419107813Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/kubelet/controller.go:207 msg="Starting controller" component=kubelet_endpoints kubelet_object=kube-system/kubelet
2024-10-24T13:22:06.420907353Z ts=2024-10-24T13:22:06.420865893Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.521771033Z ts=2024-10-24T13:22:06.521641363Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.521886093Z ts=2024-10-24T13:22:06.521774423Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for thanos"
2024-10-24T13:22:06.521958864Z ts=2024-10-24T13:22:06.521919433Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.521971153Z ts=2024-10-24T13:22:06.521962864Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.521999793Z ts=2024-10-24T13:22:06.521989344Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522006313Z ts=2024-10-24T13:22:06.521998564Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522041913Z ts=2024-10-24T13:22:06.522011624Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522041913Z ts=2024-10-24T13:22:06.522022973Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522066153Z ts=2024-10-24T13:22:06.522035684Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522078973Z ts=2024-10-24T13:22:06.522059633Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for thanos"
2024-10-24T13:22:06.522130164Z ts=2024-10-24T13:22:06.522088253Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for thanos"
2024-10-24T13:22:06.522163433Z ts=2024-10-24T13:22:06.522136484Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for thanos"
2024-10-24T13:22:06.522163433Z ts=2024-10-24T13:22:06.522155073Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for thanos"
2024-10-24T13:22:06.522188964Z ts=2024-10-24T13:22:06.522169344Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for thanos"
2024-10-24T13:22:06.522217424Z ts=2024-10-24T13:22:06.522191284Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for thanos"
2024-10-24T13:22:06.522247203Z ts=2024-10-24T13:22:06.522222453Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for thanos"
2024-10-24T13:22:06.522258043Z ts=2024-10-24T13:22:06.522242943Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for thanos"
2024-10-24T13:22:06.522279643Z ts=2024-10-24T13:22:06.522262603Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for thanos"
2024-10-24T13:22:06.522291033Z ts=2024-10-24T13:22:06.522279193Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for thanos"
2024-10-24T13:22:06.522319663Z ts=2024-10-24T13:22:06.522294093Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/thanos/operator.go:297 msg="successfully synced all caches" component=thanos-controller
2024-10-24T13:22:06.522451523Z ts=2024-10-24T13:22:06.522391783Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522469014Z ts=2024-10-24T13:22:06.522430213Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for alertmanager"
2024-10-24T13:22:06.522518314Z ts=2024-10-24T13:22:06.522489043Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for alertmanager"
2024-10-24T13:22:06.522518314Z ts=2024-10-24T13:22:06.522508163Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for alertmanager"
2024-10-24T13:22:06.522604954Z ts=2024-10-24T13:22:06.522532874Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for alertmanager"
2024-10-24T13:22:06.522604954Z ts=2024-10-24T13:22:06.522555514Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for alertmanager"
2024-10-24T13:22:06.522604954Z ts=2024-10-24T13:22:06.522575163Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for alertmanager"
2024-10-24T13:22:06.522604954Z ts=2024-10-24T13:22:06.522588514Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for alertmanager"
2024-10-24T13:22:06.522628903Z ts=2024-10-24T13:22:06.522609554Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522657194Z ts=2024-10-24T13:22:06.522636563Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522674094Z ts=2024-10-24T13:22:06.522659463Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522706923Z ts=2024-10-24T13:22:06.522674423Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522731083Z ts=2024-10-24T13:22:06.522709883Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522748493Z ts=2024-10-24T13:22:06.522733343Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522759343Z ts=2024-10-24T13:22:06.522749043Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for alertmanager"
2024-10-24T13:22:06.522783673Z ts=2024-10-24T13:22:06.522764553Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for alertmanager"
2024-10-24T13:22:06.522924853Z ts=2024-10-24T13:22:06.522785123Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522924853Z ts=2024-10-24T13:22:06.522845113Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522924853Z ts=2024-10-24T13:22:06.522887003Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for prometheus"
2024-10-24T13:22:06.522924853Z ts=2024-10-24T13:22:06.522904143Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for prometheus"
2024-10-24T13:22:06.522955783Z ts=2024-10-24T13:22:06.522914503Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:407 msg="successfully synced all caches" component=prometheus-controller
2024-10-24T13:22:06.522955783Z ts=2024-10-24T13:22:06.522933983Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:313 msg="Waiting for caches to sync for alertmanager"
2024-10-24T13:22:06.522985243Z ts=2024-10-24T13:22:06.522958933Z level=info caller=/go/src/github.com/coreos/prometheus-operator/vendor/k8s.io/client-go/tools/cache/shared_informer.go:320 msg="Caches are synced for alertmanager"
2024-10-24T13:22:06.523011793Z ts=2024-10-24T13:22:06.522980833Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:325 msg="successfully synced all caches" component=alertmanager-controller
2024-10-24T13:22:10.697975924Z ts=2024-10-24T13:22:10.697439884Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:10.870024589Z ts=2024-10-24T13:22:10.869930379Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:663 msg="StatefulSet not found" component=alertmanager-controller key=openshift-monitoring/alertmanager-main
2024-10-24T13:22:10.900899896Z ts=2024-10-24T13:22:10.900811076Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:11.280650041Z ts=2024-10-24T13:22:11.280575361Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:13.044886571Z ts=2024-10-24T13:22:13.044733142Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:13.253492112Z ts=2024-10-24T13:22:13.253418732Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:13.528359767Z ts=2024-10-24T13:22:13.528296547Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:13.634313748Z ts=2024-10-24T13:22:13.634223348Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:15.851286256Z ts=2024-10-24T13:22:15.851198767Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:31.477147605Z ts=2024-10-24T13:22:31.477042125Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:22:31.593322804Z ts=2024-10-24T13:22:31.593202654Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:25:51.825667080Z ts=2024-10-24T13:25:51.82550438Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:373 msg="get namespace to enqueue Alertmanager instances failed: namespace \"openshift-network-console\" does not exist" component=alertmanager-controller
2024-10-24T13:27:15.189785242Z ts=2024-10-24T13:27:15.189673102Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:15.318572316Z ts=2024-10-24T13:27:15.318479516Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:15.455890951Z ts=2024-10-24T13:27:15.455722371Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:17.820414267Z ts=2024-10-24T13:27:17.820321177Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:18.033558269Z ts=2024-10-24T13:27:18.033447609Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:20.725130762Z ts=2024-10-24T13:27:20.725045212Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:20.825909398Z ts=2024-10-24T13:27:20.825796458Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:20.844532527Z ts=2024-10-24T13:27:20.844048957Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:27:21.128178136Z ts=2024-10-24T13:27:21.128076336Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:21.214994313Z ts=2024-10-24T13:27:21.214893823Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:27:21.216041183Z ts=2024-10-24T13:27:21.215936313Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:27:21.572630919Z ts=2024-10-24T13:27:21.572525558Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:27:21.866812637Z ts=2024-10-24T13:27:21.866717627Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:28:06.423223865Z ts=2024-10-24T13:28:06.423074225Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/kubelet/controller.go:367 msg="Failed to list nodes" component=kubelet_endpoints kubelet_object=kube-system/kubelet err="Get \"https://172.30.0.1:443/api/v1/nodes\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.525582330Z ts=2024-10-24T13:28:06.525347Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.526477959Z ts=2024-10-24T13:28:06.52631694Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.535096119Z ts=2024-10-24T13:28:06.534955479Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.535173289Z ts=2024-10-24T13:28:06.535075119Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.548990558Z ts=2024-10-24T13:28:06.548894648Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.554112898Z ts=2024-10-24T13:28:06.554016618Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.573856587Z ts=2024-10-24T13:28:06.573693827Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.577775847Z ts=2024-10-24T13:28:06.577653267Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.617100824Z ts=2024-10-24T13:28:06.616963374Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.620407054Z ts=2024-10-24T13:28:06.620331994Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.700981479Z ts=2024-10-24T13:28:06.700865229Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.703336789Z ts=2024-10-24T13:28:06.703264649Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.864508870Z ts=2024-10-24T13:28:06.86441432Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:06.866915920Z ts=2024-10-24T13:28:06.86679511Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:07.188337871Z ts=2024-10-24T13:28:07.188228111Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:07.189491401Z ts=2024-10-24T13:28:07.189404661Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:07.831285293Z ts=2024-10-24T13:28:07.831193713Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:07.833906653Z ts=2024-10-24T13:28:07.833773643Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:09.114979138Z ts=2024-10-24T13:28:09.114887248Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:09.117391628Z ts=2024-10-24T13:28:09.117308527Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:11.680481447Z ts=2024-10-24T13:28:11.680307197Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:11.680481447Z ts=2024-10-24T13:28:11.680307357Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:16.804532345Z ts=2024-10-24T13:28:16.804361105Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:16.804532345Z ts=2024-10-24T13:28:16.804438075Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:27.048204113Z ts=2024-10-24T13:28:27.048114563Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:27.048204113Z ts=2024-10-24T13:28:27.048146823Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:47.532344409Z ts=2024-10-24T13:28:47.532199339Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/k8s\" failed: failed to get prometheus status: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=app.kubernetes.io%2Finstance%3Dk8s%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dprometheus%2Coperator.prometheus.io%2Fname%3Dk8s%2Coperator.prometheus.io%2Fshard%3D0%2Cprometheus%3Dk8s\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:28:47.532344409Z ts=2024-10-24T13:28:47.532268149Z level=error caller=/go/src/github.com/coreos/prometheus-operator/pkg/operator/resource_reconciler.go:579 msg="Unhandled Error" logger=UnhandledError err="status \"openshift-monitoring/main\" failed: failed to retrieve statefulset state: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods?labelSelector=alertmanager%3Dmain%2Capp.kubernetes.io%2Finstance%3Dmain%2Capp.kubernetes.io%2Fmanaged-by%3Dprometheus-operator%2Capp.kubernetes.io%2Fname%3Dalertmanager\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:33:14.079722185Z ts=2024-10-24T13:33:14.079629385Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:33:38.922062800Z ts=2024-10-24T13:33:38.92196515Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:46:28.619633839Z ts=2024-10-24T13:46:28.619542899Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T13:46:28.620339339Z ts=2024-10-24T13:46:28.620235719Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:46:28.719104767Z ts=2024-10-24T13:46:28.719029487Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T13:46:29.001747850Z ts=2024-10-24T13:46:29.00166789Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:08:04.641370612Z ts=2024-10-24T14:08:04.641240532Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:08:04.947536229Z ts=2024-10-24T14:08:04.947458209Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:05.054254775Z ts=2024-10-24T14:08:05.054004855Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:08:05.061626835Z ts=2024-10-24T14:08:05.061519275Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:17.560489090Z ts=2024-10-24T14:08:17.56042424Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:08:17.562027140Z ts=2024-10-24T14:08:17.56195924Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:17.651253461Z ts=2024-10-24T14:08:17.65111434Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:17.753066651Z ts=2024-10-24T14:08:17.752961251Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:17.846962291Z ts=2024-10-24T14:08:17.846891471Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:17.907901552Z ts=2024-10-24T14:08:17.907729792Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:08:17.948283592Z ts=2024-10-24T14:08:17.948168792Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.046513732Z ts=2024-10-24T14:08:18.046428862Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.156642512Z ts=2024-10-24T14:08:18.156544702Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.256748593Z ts=2024-10-24T14:08:18.256654343Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.298800443Z ts=2024-10-24T14:08:18.298716183Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:08:18.347265333Z ts=2024-10-24T14:08:18.347165143Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.458984484Z ts=2024-10-24T14:08:18.458893293Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.579536254Z ts=2024-10-24T14:08:18.576766824Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:08:18.699190774Z ts=2024-10-24T14:08:18.699024955Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:23:29.479511322Z ts=2024-10-24T14:23:29.479347092Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:23:29.479511322Z ts=2024-10-24T14:23:29.479426542Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:23:29.599750271Z ts=2024-10-24T14:23:29.599619491Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:23:29.717715041Z ts=2024-10-24T14:23:29.717573341Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:23:29.844346560Z ts=2024-10-24T14:23:29.8442667Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:24:18.387681965Z ts=2024-10-24T14:24:18.387600355Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:24:18.388709815Z ts=2024-10-24T14:24:18.388577735Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:24:18.510745254Z ts=2024-10-24T14:24:18.510661374Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:24:18.603939913Z ts=2024-10-24T14:24:18.603658283Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:24:18.772628902Z ts=2024-10-24T14:24:18.772497142Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:42:36.541558980Z ts=2024-10-24T14:42:36.54145523Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:42:36.541859060Z ts=2024-10-24T14:42:36.54142311Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
2024-10-24T14:42:36.674719799Z ts=2024-10-24T14:42:36.674629229Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/alertmanager/operator.go:544 msg="sync alertmanager" component=alertmanager-controller key=openshift-monitoring/main
2024-10-24T14:42:36.927483778Z ts=2024-10-24T14:42:36.927403518Z level=info caller=/go/src/github.com/coreos/prometheus-operator/pkg/prometheus/server/operator.go:756 msg="sync prometheus" component=prometheus-controller key=openshift-monitoring/k8s
