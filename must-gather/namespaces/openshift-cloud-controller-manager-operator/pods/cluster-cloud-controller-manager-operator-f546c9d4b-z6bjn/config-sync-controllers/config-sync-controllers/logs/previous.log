2024-10-24T13:00:39.745205533Z I1024 13:00:39.745005       1 leaderelection.go:121] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2024-10-24T13:00:39.771284891Z I1024 13:00:39.771221       1 main.go:170] "starting manager" logger="CCCMOConfigSyncControllers.setup"
2024-10-24T13:00:39.771459331Z I1024 13:00:39.771417       1 server.go:83] "starting server" logger="CCCMOConfigSyncControllers" name="health probe" addr="127.0.0.1:9260"
2024-10-24T13:00:39.771577151Z I1024 13:00:39.771539       1 leaderelection.go:250] attempting to acquire leader lease openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader...
2024-10-24T13:00:39.780399114Z I1024 13:00:39.780339       1 leaderelection.go:260] successfully acquired lease openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader
2024-10-24T13:00:39.780731524Z I1024 13:00:39.780698       1 controller.go:173] "Starting EventSource" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" source="kind source: *v1.ConfigMap"
2024-10-24T13:00:39.780827975Z I1024 13:00:39.780734       1 controller.go:173] "Starting EventSource" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" source="kind source: *v1.ConfigMap"
2024-10-24T13:00:39.780922665Z I1024 13:00:39.780892       1 controller.go:173] "Starting EventSource" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" source="kind source: *v1.Proxy"
2024-10-24T13:00:39.781004585Z I1024 13:00:39.780960       1 controller.go:181] "Starting Controller" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap"
2024-10-24T13:00:39.781045615Z I1024 13:00:39.780796       1 controller.go:173] "Starting EventSource" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" source="kind source: *v1.Infrastructure"
2024-10-24T13:00:39.781645036Z I1024 13:00:39.781593       1 controller.go:173] "Starting EventSource" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" source="kind source: *v1.Network"
2024-10-24T13:00:39.781645036Z I1024 13:00:39.781627       1 controller.go:181] "Starting Controller" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap"
2024-10-24T13:00:39.983020049Z I1024 13:00:39.982931       1 controller.go:215] "Starting workers" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" worker count=1
2024-10-24T13:00:39.983079049Z I1024 13:00:39.982969       1 controller.go:215] "Starting workers" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" worker count=1
2024-10-24T13:00:39.986539505Z I1024 13:00:39.986455       1 trusted_ca_bundle_controller.go:156] cloud-config was not found: ConfigMap "cloud-conf" not found
2024-10-24T13:00:40.106121990Z E1024 13:00:40.106031       1 controller.go:324] "Reconciler error" err="failed to set conditions for trusted CA bundle controller: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"cloud-controller-manager\": the object has been modified; please apply your changes to the latest version and try again" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" ConfigMap="openshift-config/kube-root-ca.crt" namespace="openshift-config" name="kube-root-ca.crt" reconcileID="4166c639-b7fd-4758-836d-452fd9004dcc"
2024-10-24T13:00:40.110593836Z E1024 13:00:40.110521       1 controller.go:324] "Reconciler error" err="failed to set conditions for cloud config controller: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"cloud-controller-manager\": the object has been modified; please apply your changes to the latest version and try again" logger="CCCMOConfigSyncControllers" controller="configmap" controllerGroup="" controllerKind="ConfigMap" ConfigMap="openshift-config/admin-kubeconfig-client-ca" namespace="openshift-config" name="admin-kubeconfig-client-ca" reconcileID="003d58cc-9ffb-4f66-b6d4-4f14b4d4dd10"
2024-10-24T13:08:39.800061448Z W1024 13:08:39.799577       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:08:39.800393438Z W1024 13:08:39.800371       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:08:39.800714278Z E1024 13:08:39.800676       1 leaderelection.go:340] Failed to update lock optimitically: Put "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": http2: client connection lost, falling back to slow path
2024-10-24T13:08:39.801231518Z W1024 13:08:39.801200       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.ClusterOperator ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:08:39.801311748Z W1024 13:08:39.801228       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:08:39.801392438Z W1024 13:08:39.801237       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.Infrastructure ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:08:39.801524098Z W1024 13:08:39.801489       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.Network ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:08:39.801524098Z W1024 13:08:39.801513       1 reflector.go:470] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:106: watch of *v1.Proxy ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
2024-10-24T13:09:31.843333173Z E1024 13:09:31.843248       1 leaderelection.go:340] Failed to update lock optimitically: Put "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.2:6443: connect: connection refused, falling back to slow path
2024-10-24T13:09:31.847222683Z E1024 13:09:31.847106       1 leaderelection.go:347] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: Get "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.2:6443: connect: connection refused
2024-10-24T13:09:57.854603454Z E1024 13:09:57.854539       1 leaderelection.go:340] Failed to update lock optimitically: Put "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.2:6443: connect: connection refused, falling back to slow path
2024-10-24T13:12:27.282369112Z E1024 13:12:27.282280       1 leaderelection.go:340] Failed to update lock optimitically: Put "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.2:6443: connect: connection refused, falling back to slow path
2024-10-24T13:12:27.286782412Z E1024 13:12:27.286686       1 leaderelection.go:347] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: Get "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.2:6443: connect: connection refused
2024-10-24T13:13:23.289164812Z E1024 13:13:23.289090       1 leaderelection.go:340] Failed to update lock optimitically: Put "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.2:6443: i/o timeout, falling back to slow path
2024-10-24T13:14:32.697366670Z E1024 13:14:32.697315       1 leaderelection.go:340] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:16:17.605169971Z E1024 13:16:17.605118       1 leaderelection.go:340] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:17:17.609064449Z E1024 13:17:17.608999       1 leaderelection.go:347] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io cluster-cloud-config-sync-leader)
2024-10-24T13:17:50.616301324Z E1024 13:17:50.616265       1 leaderelection.go:340] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:17:57.545064083Z E1024 13:17:57.545001       1 leaderelection.go:347] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: Get "https://api-int.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": context deadline exceeded
2024-10-24T13:17:57.545257813Z I1024 13:17:57.545203       1 leaderelection.go:285] failed to renew lease openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: timed out waiting for the condition
2024-10-24T13:17:57.546503072Z E1024 13:17:57.546403       1 main.go:172] "problem running manager" err="leader election lost" logger="CCCMOConfigSyncControllers.setup"
