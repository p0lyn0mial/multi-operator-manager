2024-10-24T13:04:59.719116244Z I1024 13:04:59.718980       1 cmd.go:250] Using service-serving-cert provided certificates
2024-10-24T13:04:59.719116244Z I1024 13:04:59.719092       1 leaderelection.go:121] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2024-10-24T13:04:59.719493824Z I1024 13:04:59.719462       1 observer_polling.go:159] Starting file observer
2024-10-24T13:04:59.742556794Z I1024 13:04:59.742502       1 builder.go:298] kube-apiserver-operator version v4.0.0-alpha.0-2044-gdf3657c-df3657cc2
2024-10-24T13:04:59.988017215Z I1024 13:04:59.987958       1 secure_serving.go:57] Forcing use of http/1.1 only
2024-10-24T13:04:59.988017215Z W1024 13:04:59.987997       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2024-10-24T13:04:59.988017215Z W1024 13:04:59.988005       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2024-10-24T13:04:59.988017215Z W1024 13:04:59.988010       1 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_128_GCM_SHA256' detected.
2024-10-24T13:04:59.988102064Z W1024 13:04:59.988013       1 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_256_GCM_SHA384' detected.
2024-10-24T13:04:59.988102064Z W1024 13:04:59.988018       1 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_128_CBC_SHA' detected.
2024-10-24T13:04:59.988102064Z W1024 13:04:59.988021       1 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_256_CBC_SHA' detected.
2024-10-24T13:04:59.993660874Z I1024 13:04:59.993617       1 requestheader_controller.go:172] Starting RequestHeaderAuthRequestController
2024-10-24T13:04:59.993795394Z I1024 13:04:59.993728       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2024-10-24T13:04:59.993896144Z I1024 13:04:59.993879       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2024-10-24T13:04:59.993968055Z I1024 13:04:59.993794       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
2024-10-24T13:04:59.994061035Z I1024 13:04:59.994041       1 secure_serving.go:213] Serving securely on [::]:8443
2024-10-24T13:04:59.994138524Z I1024 13:04:59.994121       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2024-10-24T13:04:59.994202025Z I1024 13:04:59.994171       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
2024-10-24T13:04:59.994386525Z I1024 13:04:59.994362       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2024-10-24T13:04:59.994464804Z I1024 13:04:59.994420       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2024-10-24T13:04:59.995158004Z I1024 13:04:59.995129       1 leaderelection.go:254] attempting to acquire leader lease openshift-kube-apiserver-operator/kube-apiserver-operator-lock...
2024-10-24T13:05:00.008347675Z I1024 13:05:00.008308       1 leaderelection.go:268] successfully acquired lease openshift-kube-apiserver-operator/kube-apiserver-operator-lock
2024-10-24T13:05:00.008666535Z I1024 13:05:00.008625       1 event.go:377] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator-lock", UID:"4ae29599-1ac2-4955-b398-22769183a220", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"10746", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' kube-apiserver-operator-749f4b99b7-fqnd2_34364051-8647-4ed0-8ad9-e71ff0027088 became leader
2024-10-24T13:05:00.009671314Z I1024 13:05:00.009637       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2024-10-24T13:05:00.013437325Z I1024 13:05:00.013373       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AWSEFSDriverVolumeMetrics", "AdminNetworkPolicy", "AlibabaPlatform", "AzureWorkloadIdentity", "BareMetalLoadBalancer", "BuildCSIVolumes", "ChunkSizeMiB", "CloudDualStackNodeIPs", "DisableKubeletCloudCredentialProviders", "GCPLabelsTags", "HardwareSpeed", "IngressControllerLBSubnetsAWS", "KMSv1", "ManagedBootImages", "MetricsServer", "MultiArchInstallAWS", "MultiArchInstallGCP", "NetworkDiagnosticsConfig", "NetworkLiveMigration", "NodeDisruptionPolicy", "OpenShiftPodSecurityAdmission", "PrivateHostedZoneAWS", "SetEIPForNLBIngressController", "VSphereControlPlaneMachineSet", "VSphereDriverConfiguration", "VSphereStaticIPs", "ValidatingAdmissionPolicy"}, Disabled:[]v1.FeatureGateName{"AWSClusterHostedDNS", "AdditionalRoutingCapabilities", "AutomatedEtcdBackup", "BootcNodeManagement", "CSIDriverSharedResource", "ClusterAPIInstall", "ClusterAPIInstallIBMCloud", "ClusterMonitoringConfig", "DNSNameResolver", "DynamicResourceAllocation", "EtcdBackendQuota", "EventedPLEG", "Example", "ExternalOIDC", "GCPClusterHostedDNS", "GatewayAPI", "ImageStreamImportMode", "IngressControllerDynamicConfigurationManager", "InsightsConfig", "InsightsConfigAPI", "InsightsOnDemandDataGather", "InsightsRuntimeExtractor", "MachineAPIMigration", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MachineConfigNodes", "ManagedBootImagesAWS", "MaxUnavailableStatefulSet", "MetricsCollectionProfiles", "MixedCPUsAllocation", "MultiArchInstallAzure", "NetworkSegmentation", "NewOLM", "NodeSwap", "OVNObservability", "OnClusterBuild", "PersistentIPsForVirtualization", "PinnedImages", "PlatformOperators", "ProcMountType", "RouteAdvertisements", "RouteExternalCertificate", "ServiceAccountTokenNodeBinding", "SignatureStores", "SigstoreImageVerification", "TranslateStreamCloseWebsocketRequests", "UpgradeStatus", "UserNamespacesPodSecurityStandards", "UserNamespacesSupport", "VSphereMultiNetworks", "VSphereMultiVCenters", "VolumeGroupSnapshot"}}
2024-10-24T13:05:00.013437325Z I1024 13:05:00.013382       1 starter.go:164] FeatureGates initialized: knownFeatureGates=[AWSEFSDriverVolumeMetrics AdminNetworkPolicy AlibabaPlatform AzureWorkloadIdentity BareMetalLoadBalancer BuildCSIVolumes ChunkSizeMiB CloudDualStackNodeIPs DisableKubeletCloudCredentialProviders GCPLabelsTags HardwareSpeed IngressControllerLBSubnetsAWS KMSv1 ManagedBootImages MetricsServer MultiArchInstallAWS MultiArchInstallGCP NetworkDiagnosticsConfig NetworkLiveMigration NodeDisruptionPolicy OpenShiftPodSecurityAdmission PrivateHostedZoneAWS SetEIPForNLBIngressController VSphereControlPlaneMachineSet VSphereDriverConfiguration VSphereStaticIPs ValidatingAdmissionPolicy AWSClusterHostedDNS AdditionalRoutingCapabilities AutomatedEtcdBackup BootcNodeManagement CSIDriverSharedResource ClusterAPIInstall ClusterAPIInstallIBMCloud ClusterMonitoringConfig DNSNameResolver DynamicResourceAllocation EtcdBackendQuota EventedPLEG Example ExternalOIDC GCPClusterHostedDNS GatewayAPI ImageStreamImportMode IngressControllerDynamicConfigurationManager InsightsConfig InsightsConfigAPI InsightsOnDemandDataGather InsightsRuntimeExtractor MachineAPIMigration MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MachineConfigNodes ManagedBootImagesAWS MaxUnavailableStatefulSet MetricsCollectionProfiles MixedCPUsAllocation MultiArchInstallAzure NetworkSegmentation NewOLM NodeSwap OVNObservability OnClusterBuild PersistentIPsForVirtualization PinnedImages PlatformOperators ProcMountType RouteAdvertisements RouteExternalCertificate ServiceAccountTokenNodeBinding SignatureStores SigstoreImageVerification TranslateStreamCloseWebsocketRequests UpgradeStatus UserNamespacesPodSecurityStandards UserNamespacesSupport VSphereMultiNetworks VSphereMultiVCenters VolumeGroupSnapshot]
2024-10-24T13:05:00.045340315Z I1024 13:05:00.045301       1 base_controller.go:68] Waiting for caches to sync for SCCReconcileController
2024-10-24T13:05:00.045907865Z I1024 13:05:00.045879       1 base_controller.go:68] Waiting for caches to sync for ServiceAccountIssuerController
2024-10-24T13:05:00.045943125Z I1024 13:05:00.045913       1 base_controller.go:68] Waiting for caches to sync for PodSecurityReadinessController
2024-10-24T13:05:00.045943125Z I1024 13:05:00.045920       1 base_controller.go:74] Caches are synced for PodSecurityReadinessController 
2024-10-24T13:05:00.045943125Z I1024 13:05:00.045927       1 base_controller.go:111] Starting #1 worker of PodSecurityReadinessController controller ...
2024-10-24T13:05:00.045973405Z I1024 13:05:00.045965       1 base_controller.go:68] Waiting for caches to sync for highCPUUsageAlertController
2024-10-24T13:05:00.047493955Z I1024 13:05:00.047411       1 base_controller.go:68] Waiting for caches to sync for kube-apiserver
2024-10-24T13:05:00.047575175Z I1024 13:05:00.047553       1 base_controller.go:68] Waiting for caches to sync for ConnectivityCheckController
2024-10-24T13:05:00.047704574Z I1024 13:05:00.047688       1 base_controller.go:68] Waiting for caches to sync for RemoveStaleConditionsController
2024-10-24T13:05:00.048017324Z I1024 13:05:00.047988       1 base_controller.go:68] Waiting for caches to sync for KubeAPIServerStaticResources-StaticResources
2024-10-24T13:05:00.048128385Z I1024 13:05:00.048081       1 base_controller.go:68] Waiting for caches to sync for TargetConfigController
2024-10-24T13:05:00.048227805Z I1024 13:05:00.048178       1 base_controller.go:68] Waiting for caches to sync for NodeKubeconfigController
2024-10-24T13:05:00.048333165Z I1024 13:05:00.048274       1 base_controller.go:68] Waiting for caches to sync for ConfigObserver
2024-10-24T13:05:00.048564524Z I1024 13:05:00.048538       1 base_controller.go:68] Waiting for caches to sync for StatusSyncer_kube-apiserver
2024-10-24T13:05:00.048599315Z I1024 13:05:00.048565       1 certrotationcontroller.go:819] Starting CertRotation
2024-10-24T13:05:00.048599315Z I1024 13:05:00.048570       1 certrotationcontroller.go:784] Waiting for CertRotation
2024-10-24T13:05:00.048683245Z I1024 13:05:00.048645       1 base_controller.go:68] Waiting for caches to sync for EncryptionConditionController
2024-10-24T13:05:00.049043355Z I1024 13:05:00.049019       1 base_controller.go:68] Waiting for caches to sync for EventWatchController
2024-10-24T13:05:00.049129435Z I1024 13:05:00.049111       1 base_controller.go:68] Waiting for caches to sync for webhookSupportabilityController
2024-10-24T13:05:00.049193055Z I1024 13:05:00.049178       1 base_controller.go:68] Waiting for caches to sync for BoundSATokenSignerController
2024-10-24T13:05:00.049531915Z I1024 13:05:00.049255       1 base_controller.go:68] Waiting for caches to sync for KubeletVersionSkewController
2024-10-24T13:05:00.049584224Z I1024 13:05:00.049557       1 termination_observer.go:145] Starting TerminationObserver
2024-10-24T13:05:00.049664424Z I1024 13:05:00.049640       1 base_controller.go:68] Waiting for caches to sync for WorkerLatencyProfile
2024-10-24T13:05:00.049698625Z I1024 13:05:00.049506       1 base_controller.go:68] Waiting for caches to sync for CertRotationTimeUpgradeableController
2024-10-24T13:05:00.049903814Z I1024 13:05:00.049881       1 base_controller.go:68] Waiting for caches to sync for MissingStaticPodController
2024-10-24T13:05:00.049969595Z I1024 13:05:00.049954       1 base_controller.go:68] Waiting for caches to sync for auditPolicyController
2024-10-24T13:05:00.050062775Z I1024 13:05:00.050047       1 base_controller.go:68] Waiting for caches to sync for RevisionController
2024-10-24T13:05:00.050390105Z I1024 13:05:00.050370       1 base_controller.go:68] Waiting for caches to sync for kube-apiserver-Installer
2024-10-24T13:05:00.050519615Z I1024 13:05:00.050464       1 base_controller.go:68] Waiting for caches to sync for EncryptionKeyController
2024-10-24T13:05:00.050632964Z I1024 13:05:00.050603       1 base_controller.go:68] Waiting for caches to sync for PruneController
2024-10-24T13:05:00.050739564Z I1024 13:05:00.050719       1 base_controller.go:68] Waiting for caches to sync for EncryptionStateController
2024-10-24T13:05:00.050825945Z I1024 13:05:00.050800       1 base_controller.go:68] Waiting for caches to sync for EncryptionPruneController
2024-10-24T13:05:00.050907145Z I1024 13:05:00.050882       1 base_controller.go:68] Waiting for caches to sync for EncryptionMigrationController
2024-10-24T13:05:00.050964625Z I1024 13:05:00.050940       1 base_controller.go:68] Waiting for caches to sync for StartupMonitorPodCondition
2024-10-24T13:05:00.051203044Z I1024 13:05:00.051184       1 base_controller.go:68] Waiting for caches to sync for StaticPodStateFallback
2024-10-24T13:05:00.051442575Z I1024 13:05:00.051424       1 base_controller.go:68] Waiting for caches to sync for kube-apiserver-Node
2024-10-24T13:05:00.051823995Z I1024 13:05:00.051804       1 base_controller.go:68] Waiting for caches to sync for BackingResourceController-StaticResources
2024-10-24T13:05:00.051933745Z I1024 13:05:00.051917       1 base_controller.go:68] Waiting for caches to sync for UnsupportedConfigOverridesController
2024-10-24T13:05:00.052028465Z I1024 13:05:00.052012       1 base_controller.go:68] Waiting for caches to sync for LoggingSyncer
2024-10-24T13:05:00.052184225Z I1024 13:05:00.052160       1 base_controller.go:68] Waiting for caches to sync for GuardController
2024-10-24T13:05:00.052318024Z I1024 13:05:00.050504       1 base_controller.go:68] Waiting for caches to sync for kube-apiserver-InstallerState
2024-10-24T13:05:00.052355564Z I1024 13:05:00.050560       1 base_controller.go:68] Waiting for caches to sync for kube-apiserver-StaticPodState
2024-10-24T13:05:00.052772195Z I1024 13:05:00.052697       1 base_controller.go:74] Caches are synced for kube-apiserver-InstallerState 
2024-10-24T13:05:00.052847864Z I1024 13:05:00.052814       1 base_controller.go:111] Starting #1 worker of kube-apiserver-InstallerState controller ...
2024-10-24T13:05:00.094947725Z I1024 13:05:00.094886       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
2024-10-24T13:05:00.094947725Z I1024 13:05:00.094910       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2024-10-24T13:05:00.095070495Z I1024 13:05:00.095015       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2024-10-24T13:05:00.146935945Z I1024 13:05:00.146320       1 base_controller.go:74] Caches are synced for highCPUUsageAlertController 
2024-10-24T13:05:00.146935945Z I1024 13:05:00.146350       1 base_controller.go:111] Starting #1 worker of highCPUUsageAlertController controller ...
2024-10-24T13:05:00.146935945Z I1024 13:05:00.146366       1 base_controller.go:74] Caches are synced for SCCReconcileController 
2024-10-24T13:05:00.146935945Z I1024 13:05:00.146388       1 base_controller.go:111] Starting #1 worker of SCCReconcileController controller ...
2024-10-24T13:05:00.148627305Z I1024 13:05:00.148585       1 base_controller.go:74] Caches are synced for RemoveStaleConditionsController 
2024-10-24T13:05:00.148627305Z I1024 13:05:00.148604       1 base_controller.go:111] Starting #1 worker of RemoveStaleConditionsController controller ...
2024-10-24T13:05:00.148660045Z I1024 13:05:00.148651       1 certrotationcontroller.go:802] Finished waiting for CertRotation
2024-10-24T13:05:00.148739464Z I1024 13:05:00.148686       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148739464Z I1024 13:05:00.148732       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148812765Z I1024 13:05:00.148772       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148812765Z I1024 13:05:00.148785       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148812765Z I1024 13:05:00.148799       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148812765Z I1024 13:05:00.148802       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148833945Z I1024 13:05:00.148811       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148833945Z I1024 13:05:00.148824       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148833945Z I1024 13:05:00.148786       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148844775Z I1024 13:05:00.148832       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148844775Z I1024 13:05:00.148824       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.148855365Z I1024 13:05:00.148843       1 base_controller.go:68] Waiting for caches to sync for CertRotationController
2024-10-24T13:05:00.150105615Z I1024 13:05:00.150067       1 base_controller.go:74] Caches are synced for CertRotationTimeUpgradeableController 
2024-10-24T13:05:00.150105615Z I1024 13:05:00.150086       1 base_controller.go:111] Starting #1 worker of CertRotationTimeUpgradeableController controller ...
2024-10-24T13:05:00.151289385Z I1024 13:05:00.151219       1 base_controller.go:74] Caches are synced for StartupMonitorPodCondition 
2024-10-24T13:05:00.151289385Z I1024 13:05:00.151255       1 base_controller.go:111] Starting #1 worker of StartupMonitorPodCondition controller ...
2024-10-24T13:05:00.152309494Z I1024 13:05:00.152233       1 base_controller.go:74] Caches are synced for LoggingSyncer 
2024-10-24T13:05:00.152309494Z I1024 13:05:00.152250       1 base_controller.go:111] Starting #1 worker of LoggingSyncer controller ...
2024-10-24T13:05:00.152309494Z I1024 13:05:00.152277       1 base_controller.go:74] Caches are synced for StaticPodStateFallback 
2024-10-24T13:05:00.152309494Z I1024 13:05:00.152291       1 base_controller.go:111] Starting #1 worker of StaticPodStateFallback controller ...
2024-10-24T13:05:00.152492775Z I1024 13:05:00.152473       1 base_controller.go:74] Caches are synced for UnsupportedConfigOverridesController 
2024-10-24T13:05:00.152539765Z I1024 13:05:00.152526       1 base_controller.go:111] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2024-10-24T13:05:00.154869165Z I1024 13:05:00.154804       1 base_controller.go:74] Caches are synced for kube-apiserver-StaticPodState 
2024-10-24T13:05:00.154869165Z I1024 13:05:00.154829       1 base_controller.go:111] Starting #1 worker of kube-apiserver-StaticPodState controller ...
2024-10-24T13:05:00.225306895Z I1024 13:05:00.225240       1 reflector.go:368] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.247416015Z I1024 13:05:00.247352       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.249539125Z I1024 13:05:00.249499       1 base_controller.go:74] Caches are synced for StatusSyncer_kube-apiserver 
2024-10-24T13:05:00.249539125Z I1024 13:05:00.249515       1 base_controller.go:111] Starting #1 worker of StatusSyncer_kube-apiserver controller ...
2024-10-24T13:05:00.373265385Z I1024 13:05:00.373198       1 reflector.go:368] Caches populated for *v1.CustomResourceDefinition from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.412596785Z I1024 13:05:00.412542       1 reflector.go:368] Caches populated for *v1.Image from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.440475385Z I1024 13:05:00.440432       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.450795045Z I1024 13:05:00.449841       1 base_controller.go:74] Caches are synced for ConnectivityCheckController 
2024-10-24T13:05:00.450795045Z I1024 13:05:00.449865       1 base_controller.go:111] Starting #1 worker of ConnectivityCheckController controller ...
2024-10-24T13:05:00.450795045Z I1024 13:05:00.450224       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:00.612991285Z I1024 13:05:00.612726       1 reflector.go:368] Caches populated for *v1.Authentication from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.641657925Z I1024 13:05:00.641592       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.646907045Z I1024 13:05:00.646866       1 base_controller.go:74] Caches are synced for ServiceAccountIssuerController 
2024-10-24T13:05:00.646907045Z I1024 13:05:00.646885       1 base_controller.go:111] Starting #1 worker of ServiceAccountIssuerController controller ...
2024-10-24T13:05:00.649365925Z I1024 13:05:00.649304       1 base_controller.go:74] Caches are synced for webhookSupportabilityController 
2024-10-24T13:05:00.649365925Z I1024 13:05:00.649325       1 base_controller.go:111] Starting #1 worker of webhookSupportabilityController controller ...
2024-10-24T13:05:00.812605085Z I1024 13:05:00.812346       1 reflector.go:368] Caches populated for *v1.APIServer from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.841011466Z I1024 13:05:00.840969       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:00.850012925Z I1024 13:05:00.849946       1 base_controller.go:74] Caches are synced for KubeletVersionSkewController 
2024-10-24T13:05:00.850012925Z I1024 13:05:00.849979       1 base_controller.go:111] Starting #1 worker of KubeletVersionSkewController controller ...
2024-10-24T13:05:00.852099275Z I1024 13:05:00.852050       1 base_controller.go:74] Caches are synced for kube-apiserver-Node 
2024-10-24T13:05:00.852099275Z I1024 13:05:00.852066       1 base_controller.go:111] Starting #1 worker of kube-apiserver-Node controller ...
2024-10-24T13:05:00.854279895Z I1024 13:05:00.854248       1 base_controller.go:74] Caches are synced for GuardController 
2024-10-24T13:05:00.854407585Z I1024 13:05:00.854336       1 base_controller.go:111] Starting #1 worker of GuardController controller ...
2024-10-24T13:05:00.854812335Z E1024 13:05:00.854787       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:00.854812335Z E1024 13:05:00.854804       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:00.854838116Z E1024 13:05:00.854810       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:00.886238475Z E1024 13:05:00.886187       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:00.892672175Z E1024 13:05:00.892645       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:00.892672175Z E1024 13:05:00.892667       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:00.892698366Z E1024 13:05:00.892673       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:00.893895835Z E1024 13:05:00.893850       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:00.905383926Z E1024 13:05:00.905339       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:00.905383926Z E1024 13:05:00.905354       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:00.905383926Z E1024 13:05:00.905359       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:00.906511265Z E1024 13:05:00.906468       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:00.927959665Z E1024 13:05:00.927921       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:00.927959665Z E1024 13:05:00.927938       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:00.927959665Z E1024 13:05:00.927941       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:00.929200375Z E1024 13:05:00.929167       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:00.971297386Z E1024 13:05:00.971262       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:00.971297386Z E1024 13:05:00.971286       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:00.971325015Z E1024 13:05:00.971298       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:00.972695705Z E1024 13:05:00.972661       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:01.042832156Z I1024 13:05:01.042713       1 reflector.go:368] Caches populated for *v1.Event from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:01.049431305Z I1024 13:05:01.049371       1 base_controller.go:74] Caches are synced for EventWatchController 
2024-10-24T13:05:01.049431305Z I1024 13:05:01.049400       1 base_controller.go:111] Starting #1 worker of EventWatchController controller ...
2024-10-24T13:05:01.055281216Z E1024 13:05:01.055236       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:01.055281216Z E1024 13:05:01.055261       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:01.055281216Z E1024 13:05:01.055265       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:01.056646996Z E1024 13:05:01.056608       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:01.218519296Z E1024 13:05:01.218450       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:01.218519296Z E1024 13:05:01.218476       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:01.218519296Z E1024 13:05:01.218480       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:01.219865176Z E1024 13:05:01.219798       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:01.238299936Z I1024 13:05:01.238243       1 request.go:700] Waited for 1.189615972s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts?limit=500&resourceVersion=0
2024-10-24T13:05:01.241133866Z I1024 13:05:01.241094       1 reflector.go:368] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:01.251990696Z I1024 13:05:01.251936       1 base_controller.go:74] Caches are synced for BackingResourceController-StaticResources 
2024-10-24T13:05:01.251990696Z I1024 13:05:01.251965       1 base_controller.go:111] Starting #1 worker of BackingResourceController-StaticResources controller ...
2024-10-24T13:05:01.441083407Z I1024 13:05:01.441033       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:01.542237846Z E1024 13:05:01.542185       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:01.542237846Z E1024 13:05:01.542210       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:01.542237846Z E1024 13:05:01.542219       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:01.543636966Z E1024 13:05:01.543589       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:01.641021307Z I1024 13:05:01.640960       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:01.842145687Z I1024 13:05:01.842080       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:02.041044077Z I1024 13:05:02.040969       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:02.049814167Z I1024 13:05:02.049720       1 base_controller.go:74] Caches are synced for EncryptionConditionController 
2024-10-24T13:05:02.049814167Z I1024 13:05:02.049745       1 base_controller.go:111] Starting #1 worker of EncryptionConditionController controller ...
2024-10-24T13:05:02.050782187Z I1024 13:05:02.050722       1 base_controller.go:74] Caches are synced for EncryptionKeyController 
2024-10-24T13:05:02.050782187Z I1024 13:05:02.050759       1 base_controller.go:111] Starting #1 worker of EncryptionKeyController controller ...
2024-10-24T13:05:02.051423347Z I1024 13:05:02.051394       1 base_controller.go:74] Caches are synced for EncryptionMigrationController 
2024-10-24T13:05:02.051423347Z I1024 13:05:02.051405       1 base_controller.go:111] Starting #1 worker of EncryptionMigrationController controller ...
2024-10-24T13:05:02.051423347Z I1024 13:05:02.051418       1 base_controller.go:74] Caches are synced for EncryptionPruneController 
2024-10-24T13:05:02.051448107Z I1024 13:05:02.051421       1 base_controller.go:111] Starting #1 worker of EncryptionPruneController controller ...
2024-10-24T13:05:02.051448107Z I1024 13:05:02.051426       1 base_controller.go:74] Caches are synced for EncryptionStateController 
2024-10-24T13:05:02.051448107Z I1024 13:05:02.051436       1 base_controller.go:111] Starting #1 worker of EncryptionStateController controller ...
2024-10-24T13:05:02.186215577Z E1024 13:05:02.186143       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:02.186215577Z E1024 13:05:02.186172       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:02.186215577Z E1024 13:05:02.186180       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:02.187552197Z E1024 13:05:02.187508       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:02.241876947Z I1024 13:05:02.241813       1 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:02.438376457Z I1024 13:05:02.438312       1 request.go:700] Waited for 2.384598974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/configmaps?limit=500&resourceVersion=0
2024-10-24T13:05:02.441478298Z I1024 13:05:02.441412       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:02.640740948Z I1024 13:05:02.640680       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:02.649938708Z I1024 13:05:02.649886       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649938708Z I1024 13:05:02.649905       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649938708Z I1024 13:05:02.649909       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649938708Z I1024 13:05:02.649923       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649938708Z I1024 13:05:02.649931       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649911       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649939       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649941       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649946       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649950       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649959       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.649977407Z I1024 13:05:02.649971       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.649994068Z I1024 13:05:02.649984       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.650031538Z I1024 13:05:02.650006       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.650031538Z I1024 13:05:02.650017       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.650031538Z I1024 13:05:02.649913       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.650049828Z I1024 13:05:02.650032       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.650049828Z I1024 13:05:02.650037       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.650049828Z I1024 13:05:02.649933       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.650118668Z I1024 13:05:02.649879       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.650189708Z I1024 13:05:02.650158       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.651360718Z I1024 13:05:02.649985       1 base_controller.go:74] Caches are synced for CertRotationController 
2024-10-24T13:05:02.651360718Z I1024 13:05:02.649931       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.651385228Z I1024 13:05:02.651349       1 base_controller.go:111] Starting #1 worker of CertRotationController controller ...
2024-10-24T13:05:02.840392308Z I1024 13:05:02.840350       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:03.048612038Z I1024 13:05:03.046867       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:05:03.048612038Z I1024 13:05:03.047592       1 base_controller.go:74] Caches are synced for kube-apiserver 
2024-10-24T13:05:03.048612038Z I1024 13:05:03.047606       1 base_controller.go:111] Starting #1 worker of kube-apiserver controller ...
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049906       1 base_controller.go:74] Caches are synced for KubeAPIServerStaticResources-StaticResources 
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049926       1 base_controller.go:111] Starting #1 worker of KubeAPIServerStaticResources-StaticResources controller ...
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049948       1 base_controller.go:74] Caches are synced for TargetConfigController 
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049954       1 base_controller.go:111] Starting #1 worker of TargetConfigController controller ...
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049965       1 base_controller.go:74] Caches are synced for NodeKubeconfigController 
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049969       1 base_controller.go:111] Starting #1 worker of NodeKubeconfigController controller ...
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049987       1 base_controller.go:74] Caches are synced for ConfigObserver 
2024-10-24T13:05:03.051435468Z I1024 13:05:03.049991       1 base_controller.go:111] Starting #1 worker of ConfigObserver controller ...
2024-10-24T13:05:03.053474508Z I1024 13:05:03.053449       1 base_controller.go:74] Caches are synced for WorkerLatencyProfile 
2024-10-24T13:05:03.053534708Z I1024 13:05:03.053520       1 base_controller.go:111] Starting #1 worker of WorkerLatencyProfile controller ...
2024-10-24T13:05:03.053783979Z I1024 13:05:03.053733       1 base_controller.go:74] Caches are synced for auditPolicyController 
2024-10-24T13:05:03.053783979Z I1024 13:05:03.053767       1 base_controller.go:111] Starting #1 worker of auditPolicyController controller ...
2024-10-24T13:05:03.053808748Z I1024 13:05:03.053783       1 base_controller.go:74] Caches are synced for kube-apiserver-Installer 
2024-10-24T13:05:03.053808748Z I1024 13:05:03.053797       1 base_controller.go:111] Starting #1 worker of kube-apiserver-Installer controller ...
2024-10-24T13:05:03.053820088Z I1024 13:05:03.053811       1 base_controller.go:74] Caches are synced for BoundSATokenSignerController 
2024-10-24T13:05:03.053830108Z I1024 13:05:03.053817       1 base_controller.go:111] Starting #1 worker of BoundSATokenSignerController controller ...
2024-10-24T13:05:03.053839888Z I1024 13:05:03.053830       1 base_controller.go:74] Caches are synced for MissingStaticPodController 
2024-10-24T13:05:03.053839888Z I1024 13:05:03.053836       1 base_controller.go:111] Starting #1 worker of MissingStaticPodController controller ...
2024-10-24T13:05:03.056600118Z E1024 13:05:03.056554       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.056872928Z I1024 13:05:03.056818       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "auditPolicyController" resync interval is set to 10s which might lead to client request throttling
2024-10-24T13:05:03.056964188Z I1024 13:05:03.056837       1 base_controller.go:74] Caches are synced for RevisionController 
2024-10-24T13:05:03.057055728Z I1024 13:05:03.057011       1 base_controller.go:111] Starting #1 worker of RevisionController controller ...
2024-10-24T13:05:03.057055728Z I1024 13:05:03.056854       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.057770998Z I1024 13:05:03.057681       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 1 triggered by "configmap \"kube-apiserver-pod-0\" not found"
2024-10-24T13:05:03.057770998Z I1024 13:05:03.057712       1 base_controller.go:74] Caches are synced for PruneController 
2024-10-24T13:05:03.057770998Z I1024 13:05:03.057721       1 base_controller.go:111] Starting #1 worker of PruneController controller ...
2024-10-24T13:05:03.058217908Z I1024 13:05:03.058183       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveWebhookTokenAuthenticator' authentication-token webhook configuration status changed from false to true
2024-10-24T13:05:03.058630788Z I1024 13:05:03.058570       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:05:03.058630788Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:05:03.058630788Z   	"apiServerArguments": map[string]any{
2024-10-24T13:05:03.058630788Z   		"api-audiences": []any{string("https://kubernetes.default.svc")},
2024-10-24T13:05:03.058630788Z + 		"authentication-token-webhook-config-file": []any{
2024-10-24T13:05:03.058630788Z + 			string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticator/kubeConfig"),
2024-10-24T13:05:03.058630788Z + 		},
2024-10-24T13:05:03.058630788Z + 		"authentication-token-webhook-version": []any{string("v1")},
2024-10-24T13:05:03.058630788Z   		"etcd-servers":                         []any{string("https://10.0.0.5:2379"), string("https://10.0.0.6:2379"), string("https://localhost:2379")},
2024-10-24T13:05:03.058630788Z   		"feature-gates":                        []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:05:03.058630788Z   		... // 4 identical entries
2024-10-24T13:05:03.058630788Z   	},
2024-10-24T13:05:03.058630788Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:05:03.058630788Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:05:03.058630788Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:05:03.058630788Z   }
2024-10-24T13:05:03.063532118Z I1024 13:05:03.063481       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.064834118Z E1024 13:05:03.064787       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.076712558Z I1024 13:05:03.076632       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.078115868Z E1024 13:05:03.078065       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.091364668Z I1024 13:05:03.091317       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.094129479Z I1024 13:05:03.094080       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:03.098527428Z E1024 13:05:03.098482       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.102870688Z I1024 13:05:03.102817       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.104961538Z E1024 13:05:03.104922       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.143164748Z I1024 13:05:03.143086       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.144695688Z E1024 13:05:03.144648       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.307092149Z I1024 13:05:03.307029       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.309026429Z E1024 13:05:03.308989       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.470057369Z E1024 13:05:03.469994       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:03.470057369Z E1024 13:05:03.470021       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:03.470057369Z E1024 13:05:03.470029       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:03.471729799Z E1024 13:05:03.471672       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:03.631690569Z I1024 13:05:03.631590       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:03.633722799Z E1024 13:05:03.633636       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:03.641797169Z I1024 13:05:03.641727       1 request.go:700] Waited for 2.591031493s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2024-10-24T13:05:04.276124930Z I1024 13:05:04.276038       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:04.277678380Z E1024 13:05:04.277597       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:04.558929320Z I1024 13:05:04.558880       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:04.667125321Z E1024 13:05:04.667024       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:04.667218571Z E1024 13:05:04.667199       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:04.667259340Z E1024 13:05:04.667245       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:04.669742370Z E1024 13:05:04.669575       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:04.672597271Z E1024 13:05:04.672391       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:04.673380971Z E1024 13:05:04.673361       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:04.673433421Z E1024 13:05:04.673419       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:04.675621330Z E1024 13:05:04.675486       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:04.751775461Z E1024 13:05:04.751722       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:04.751817711Z E1024 13:05:04.751779       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:04.751817711Z E1024 13:05:04.751787       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:04.753898751Z E1024 13:05:04.753603       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:04.837874601Z I1024 13:05:04.837818       1 request.go:700] Waited for 1.779698553s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets
2024-10-24T13:05:04.847515321Z I1024 13:05:04.847442       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:04.850600621Z I1024 13:05:04.850542       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:04.851384401Z E1024 13:05:04.851354       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:05.560517982Z I1024 13:05:05.560404       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:05.562007362Z E1024 13:05:05.561915       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:05.810893602Z I1024 13:05:05.810679       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:05.813423832Z I1024 13:05:05.813389       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:05.838217572Z I1024 13:05:05.838172       1 request.go:700] Waited for 1.587798982s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:06.034297392Z E1024 13:05:06.034109       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:06.034297392Z E1024 13:05:06.034266       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:06.034297392Z E1024 13:05:06.034274       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:06.036044523Z E1024 13:05:06.036009       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:06.246919133Z I1024 13:05:06.246860       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:06.247699663Z I1024 13:05:06.247640       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:06.249327293Z E1024 13:05:06.249291       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:07.038376404Z I1024 13:05:07.038320       1 request.go:700] Waited for 1.188920112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:07.247561854Z I1024 13:05:07.247483       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:07.247959484Z I1024 13:05:07.247908       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:07.249602124Z E1024 13:05:07.249556       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:08.238280565Z I1024 13:05:08.238213       1 request.go:700] Waited for 1.187612941s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:08.448026865Z I1024 13:05:08.447969       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:08.449197755Z E1024 13:05:08.449097       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:08.449364575Z I1024 13:05:08.449321       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:09.238372886Z I1024 13:05:09.238319       1 request.go:700] Waited for 1.196101801s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:05:09.270349236Z E1024 13:05:09.270308       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:09.270468716Z E1024 13:05:09.270452       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:09.270514496Z E1024 13:05:09.270502       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:09.272736816Z E1024 13:05:09.272711       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:09.447264486Z E1024 13:05:09.447011       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:09.447264486Z E1024 13:05:09.447037       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:09.447264486Z E1024 13:05:09.447042       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:09.449099526Z E1024 13:05:09.449041       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:10.648205058Z I1024 13:05:10.647931       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:10.651971768Z I1024 13:05:10.651941       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:10.653527698Z E1024 13:05:10.653483       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:10.684770998Z I1024 13:05:10.684707       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:10.686019538Z E1024 13:05:10.685991       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:11.250625789Z I1024 13:05:11.250572       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:11.250913489Z I1024 13:05:11.250868       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:11.251496289Z E1024 13:05:11.251426       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:11.854223429Z I1024 13:05:11.854146       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:11.901718089Z I1024 13:05:11.901674       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:12.049686650Z I1024 13:05:12.048044       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:12.050902680Z E1024 13:05:12.050865       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:12.051262719Z I1024 13:05:12.051224       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:12.618675960Z I1024 13:05:12.618262       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:12.846436770Z I1024 13:05:12.846349       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 1 triggered by "configmap \"kube-apiserver-pod-0\" not found"
2024-10-24T13:05:12.846542221Z I1024 13:05:12.846496       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0
2024-10-24T13:05:12.846613330Z W1024 13:05:12.846586       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:12.846613330Z W1024 13:05:12.846606       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:12.847740090Z E1024 13:05:12.847691       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:12.876166540Z W1024 13:05:12.876126       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:12.876166540Z W1024 13:05:12.876155       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:12.877696780Z I1024 13:05:12.877668       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:12.877834560Z I1024 13:05:12.877808       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:03:31Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:05:12.878301540Z I1024 13:05:12.878250       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:12.892068791Z I1024 13:05:12.892015       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nNodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded: configmap \"kube-apiserver-pod\" not found\nInstallerControllerDegraded: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]"
2024-10-24T13:05:12.923681240Z E1024 13:05:12.923106       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:12.923681240Z E1024 13:05:12.923136       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:12.923681240Z E1024 13:05:12.923148       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:12.925730891Z E1024 13:05:12.925628       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:13.247958501Z I1024 13:05:13.247900       1 core.go:352] ConfigMap "openshift-kube-apiserver/config" changes: {"apiVersion":"v1","data":{"config.yaml":"{\"admission\":{\"pluginConfig\":{\"PodSecurity\":{\"configuration\":{\"apiVersion\":\"pod-security.admission.config.k8s.io/v1\",\"defaults\":{\"audit\":\"restricted\",\"audit-version\":\"latest\",\"enforce\":\"restricted\",\"enforce-version\":\"latest\",\"warn\":\"restricted\",\"warn-version\":\"latest\"},\"exemptions\":{\"usernames\":[\"system:serviceaccount:openshift-infra:build-controller\"]},\"kind\":\"PodSecurityConfiguration\"}},\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"200\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/policy.yaml\"],\"authentication-token-webhook-config-file\":[\"/etc/kubernetes/static-pod-resources/secrets/webhook-authenticator/kubeConfig\"],\"authentication-token-webhook-version\":[\"v1\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"ValidatingAdmissionPolicy\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\",\"storage.openshift.io/CSIInlineVolumeSecurity\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-healthcheck-timeout\":[\"9s\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-readycheck-timeout\":[\"9s\"],\"etcd-servers\":[\"https://10.0.0.5:2379\",\"https://10.0.0.6:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"AWSEFSDriverVolumeMetrics=true\",\"AdminNetworkPolicy=true\",\"AlibabaPlatform=true\",\"AzureWorkloadIdentity=true\",\"BareMetalLoadBalancer=true\",\"BuildCSIVolumes=true\",\"ChunkSizeMiB=true\",\"CloudDualStackNodeIPs=true\",\"DisableKubeletCloudCredentialProviders=true\",\"GCPLabelsTags=true\",\"HardwareSpeed=true\",\"IngressControllerLBSubnetsAWS=true\",\"KMSv1=true\",\"ManagedBootImages=true\",\"MetricsServer=true\",\"MultiArchInstallAWS=true\",\"MultiArchInstallGCP=true\",\"NetworkDiagnosticsConfig=true\",\"NetworkLiveMigration=true\",\"NodeDisruptionPolicy=true\",\"OpenShiftPodSecurityAdmission=true\",\"PrivateHostedZoneAWS=true\",\"SetEIPForNLBIngressController=true\",\"VSphereControlPlaneMachineSet=true\",\"VSphereDriverConfiguration=true\",\"VSphereStaticIPs=true\",\"ValidatingAdmissionPolicy=true\",\"AWSClusterHostedDNS=false\",\"AdditionalRoutingCapabilities=false\",\"AutomatedEtcdBackup=false\",\"BootcNodeManagement=false\",\"CSIDriverSharedResource=false\",\"ClusterAPIInstall=false\",\"ClusterAPIInstallIBMCloud=false\",\"ClusterMonitoringConfig=false\",\"DNSNameResolver=false\",\"DynamicResourceAllocation=false\",\"EtcdBackendQuota=false\",\"EventedPLEG=false\",\"Example=false\",\"ExternalOIDC=false\",\"GCPClusterHostedDNS=false\",\"GatewayAPI=false\",\"ImageStreamImportMode=false\",\"IngressControllerDynamicConfigurationManager=false\",\"InsightsConfig=false\",\"InsightsConfigAPI=false\",\"InsightsOnDemandDataGather=false\",\"InsightsRuntimeExtractor=false\",\"MachineAPIMigration=false\",\"MachineAPIOperatorDisableMachineHealthCheckController=false\",\"MachineAPIProviderOpenStack=false\",\"MachineConfigNodes=false\",\"ManagedBootImagesAWS=false\",\"MaxUnavailableStatefulSet=false\",\"MetricsCollectionProfiles=false\",\"MixedCPUsAllocation=false\",\"MultiArchInstallAzure=false\",\"NetworkSegmentation=false\",\"NewOLM=false\",\"NodeSwap=false\",\"OVNObservability=false\",\"OnClusterBuild=false\",\"PersistentIPsForVirtualization=false\",\"PinnedImages=false\",\"PlatformOperators=false\",\"ProcMountType=false\",\"RouteAdvertisements=false\",\"RouteExternalCertificate=false\",\"ServiceAccountTokenNodeBinding=false\",\"SignatureStores=false\",\"SigstoreImageVerification=false\",\"TranslateStreamCloseWebsocketRequests=false\",\"UpgradeStatus=false\",\"UserNamespacesPodSecurityStandards=false\",\"UserNamespacesSupport=false\",\"VSphereMultiNetworks=false\",\"VSphereMultiVCenters=false\",\"VolumeGroupSnapshot=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.key\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"admissionregistration.k8s.io/v1beta1=true\"],\"send-retry-after-while-not-ready-once\":[\"false\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"shutdown-send-retry-after\":[\"true\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"strict-transport-security-directives\":[\"max-age=31536000,includeSubDomains,preload\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}}"},"kind":"ConfigMap","metadata":{"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null}}
2024-10-24T13:05:13.248149491Z I1024 13:05:13.248110       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:13.248955121Z I1024 13:05:13.248911       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/config -n openshift-kube-apiserver:
2024-10-24T13:05:13.248955121Z cause by changes in data.config.yaml
2024-10-24T13:05:13.250176591Z I1024 13:05:13.250129       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 2 triggered by "required configmap/config has changed"
2024-10-24T13:05:14.037657392Z I1024 13:05:14.037602       1 request.go:700] Waited for 1.159138242s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:05:14.645685503Z I1024 13:05:14.645629       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:15.237996363Z I1024 13:05:15.237897       1 request.go:700] Waited for 1.593134311s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:05:16.046733474Z I1024 13:05:16.046681       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:16.047010894Z I1024 13:05:16.046945       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:16.237982365Z I1024 13:05:16.237919       1 request.go:700] Waited for 1.395401102s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2024-10-24T13:05:17.438343446Z I1024 13:05:17.438152       1 request.go:700] Waited for 1.391633112s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:05:17.449354826Z I1024 13:05:17.449284       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:17.450054006Z I1024 13:05:17.450011       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:18.242257837Z I1024 13:05:18.242197       1 installer_controller.go:537] node ci-op-2fcpj5j6-f6035-2lklf-master-0 static pod not found and needs new revision 1
2024-10-24T13:05:18.242303877Z I1024 13:05:18.242275       1 installer_controller.go:545] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:05:18.242303877Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:05:18.242303877Z  CurrentRevision: (int32) 0,
2024-10-24T13:05:18.242303877Z  TargetRevision: (int32) 1,
2024-10-24T13:05:18.242303877Z  LastFailedRevision: (int32) 0,
2024-10-24T13:05:18.242303877Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:05:18.242303877Z  LastFailedReason: (string) "",
2024-10-24T13:05:18.242303877Z  LastFailedCount: (int) 0,
2024-10-24T13:05:18.242303877Z  LastFallbackCount: (int) 0,
2024-10-24T13:05:18.242303877Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:05:18.242303877Z }
2024-10-24T13:05:18.243831167Z W1024 13:05:18.243786       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:18.243831167Z W1024 13:05:18.243805       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:18.243831167Z W1024 13:05:18.243812       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:18.243831167Z W1024 13:05:18.243818       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:18.243831167Z W1024 13:05:18.243823       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:05:18.269378207Z I1024 13:05:18.269270       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ci-op-2fcpj5j6-f6035-2lklf-master-0" from revision 0 to 1 because node ci-op-2fcpj5j6-f6035-2lklf-master-0 static pod not found
2024-10-24T13:05:18.272341637Z I1024 13:05:18.272275       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 1","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 1","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:05:18.272587347Z I1024 13:05:18.272535       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:18.272836077Z I1024 13:05:18.272804       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:18.285868597Z I1024 13:05:18.285064       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: bound-sa-token-signing-certs-0,config-0,etcd-serving-ca-0,kube-apiserver-audit-policies-0,kube-apiserver-cert-syncer-kubeconfig-0,kube-apiserver-pod-0,kubelet-serving-ca-0,sa-token-signing-certs-0, secrets: etcd-client-0,localhost-recovery-client-token-0,localhost-recovery-serving-certkey-0]" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nNodeControllerDegraded: All master nodes are ready",Progressing changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 1"),Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 1"
2024-10-24T13:05:18.300900467Z I1024 13:05:18.300860       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:18.301685487Z I1024 13:05:18.301652       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:18.637507268Z I1024 13:05:18.637450       1 request.go:700] Waited for 1.188161512s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:05:18.651397458Z I1024 13:05:18.651361       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:18.651870688Z I1024 13:05:18.651792       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:19.637893829Z I1024 13:05:19.637842       1 request.go:700] Waited for 1.365292003s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:19.833729960Z E1024 13:05:19.833661       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:19.833878690Z E1024 13:05:19.833836       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:19.833925790Z E1024 13:05:19.833911       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:19.839813030Z E1024 13:05:19.839785       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:20.250730640Z I1024 13:05:20.250611       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:20.250871630Z I1024 13:05:20.250803       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:20.638025871Z I1024 13:05:20.637973       1 request.go:700] Waited for 1.539137722s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:05:21.638850002Z I1024 13:05:21.638317       1 request.go:700] Waited for 1.585091052s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:21.851263672Z I1024 13:05:21.851222       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:21.853563012Z I1024 13:05:21.853508       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:22.838429053Z I1024 13:05:22.838371       1 request.go:700] Waited for 1.595289322s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:23.251190944Z I1024 13:05:23.251132       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:23.251291964Z I1024 13:05:23.251193       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:24.038413854Z I1024 13:05:24.037724       1 request.go:700] Waited for 1.366154201s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:24.043059984Z I1024 13:05:24.043009       1 installer_controller.go:537] node ci-op-2fcpj5j6-f6035-2lklf-master-0 static pod not found and needs new revision 1
2024-10-24T13:05:24.043174644Z I1024 13:05:24.043128       1 installer_controller.go:545] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:05:24.043174644Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:05:24.043174644Z  CurrentRevision: (int32) 0,
2024-10-24T13:05:24.043174644Z  TargetRevision: (int32) 1,
2024-10-24T13:05:24.043174644Z  LastFailedRevision: (int32) 0,
2024-10-24T13:05:24.043174644Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:05:24.043174644Z  LastFailedReason: (string) "",
2024-10-24T13:05:24.043174644Z  LastFailedCount: (int) 0,
2024-10-24T13:05:24.043174644Z  LastFallbackCount: (int) 0,
2024-10-24T13:05:24.043174644Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:05:24.043174644Z }
2024-10-24T13:05:24.046816384Z I1024 13:05:24.046769       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ci-op-2fcpj5j6-f6035-2lklf-master-0" from revision 0 to 1 because node ci-op-2fcpj5j6-f6035-2lklf-master-0 static pod not found
2024-10-24T13:05:24.167836945Z I1024 13:05:24.165737       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:24.651416345Z I1024 13:05:24.651243       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:24.651416345Z I1024 13:05:24.651326       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:25.038660196Z I1024 13:05:25.038114       1 request.go:700] Waited for 1.501589542s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:05:26.237880447Z I1024 13:05:26.237807       1 request.go:700] Waited for 1.589760882s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:05:26.246622717Z I1024 13:05:26.246566       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:26.246666937Z I1024 13:05:26.246608       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:27.437998599Z I1024 13:05:27.437926       1 request.go:700] Waited for 1.384357721s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:27.645035539Z I1024 13:05:27.644966       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:27.645453919Z I1024 13:05:27.645322       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:28.638680000Z I1024 13:05:28.638312       1 request.go:700] Waited for 1.183814532s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:28.847262990Z I1024 13:05:28.847154       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:29.624683751Z E1024 13:05:29.624219       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:29.624774481Z E1024 13:05:29.624743       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:29.624822361Z E1024 13:05:29.624807       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:29.628387421Z E1024 13:05:29.628360       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:29.838658392Z I1024 13:05:29.838283       1 request.go:700] Waited for 1.180273791s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:05:30.648149812Z E1024 13:05:30.648102       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:30.648149812Z E1024 13:05:30.648129       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:30.648149812Z E1024 13:05:30.648144       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:30.650691582Z W1024 13:05:30.650437       1 degraded_webhook.go:147] failed to connect to webhook "multus-validating-config.k8s.io" via service "multus-admission-controller.openshift-multus.svc:443": dial tcp: lookup multus-admission-controller.openshift-multus.svc: i/o timeout
2024-10-24T13:05:30.651867152Z E1024 13:05:30.651830       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:30.655680122Z I1024 13:05:30.655445       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-1-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:30.658963392Z E1024 13:05:30.658935       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:30.659029412Z E1024 13:05:30.659015       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:30.659062292Z E1024 13:05:30.659051       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:30.661930852Z E1024 13:05:30.661845       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:30.671030943Z E1024 13:05:30.671004       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:30.671124382Z E1024 13:05:30.671073       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:30.671124382Z E1024 13:05:30.671088       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:30.673112252Z E1024 13:05:30.673068       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:30.681695362Z E1024 13:05:30.681479       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:30.681695362Z E1024 13:05:30.681504       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:30.681695362Z E1024 13:05:30.681510       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:30.684318702Z E1024 13:05:30.684162       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:31.047331873Z I1024 13:05:31.047269       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:31.167882673Z E1024 13:05:31.167838       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:31.168006333Z E1024 13:05:31.167988       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:31.168041733Z E1024 13:05:31.168031       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:31.170428453Z E1024 13:05:31.170397       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:31.642498713Z I1024 13:05:31.642442       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 1, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:05:31.837773064Z I1024 13:05:31.837701       1 request.go:700] Waited for 1.181005961s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2024-10-24T13:05:32.403832714Z E1024 13:05:32.403789       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:32.403949274Z E1024 13:05:32.403933       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:32.404019094Z E1024 13:05:32.404006       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:32.406829035Z E1024 13:05:32.406801       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:32.448991274Z I1024 13:05:32.448927       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:32.838109165Z I1024 13:05:32.838057       1 request.go:700] Waited for 1.589289382s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:05:34.037855696Z I1024 13:05:34.037392       1 request.go:700] Waited for 1.588674542s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets
2024-10-24T13:05:34.143235167Z I1024 13:05:34.143179       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:35.238294108Z I1024 13:05:35.238245       1 request.go:700] Waited for 1.790665662s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2024-10-24T13:05:35.687964438Z E1024 13:05:35.687613       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:35.687964438Z E1024 13:05:35.687643       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:35.687964438Z E1024 13:05:35.687655       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:35.691645078Z E1024 13:05:35.691589       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:35.848378078Z W1024 13:05:35.848328       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:35.848485239Z W1024 13:05:35.848466       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:35.848883558Z I1024 13:05:35.848845       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 2 triggered by "required configmap/config has changed"
2024-10-24T13:05:35.849243219Z I1024 13:05:35.849207       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:35.879693768Z W1024 13:05:35.879643       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:35.879693768Z W1024 13:05:35.879670       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:35.886628379Z I1024 13:05:35.886580       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 1","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 1","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:05:35.887125879Z I1024 13:05:35.887089       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:35.887440309Z I1024 13:05:35.887352       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:35.901420299Z I1024 13:05:35.898865       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded changed from False to True ("GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]")
2024-10-24T13:05:36.438272979Z I1024 13:05:36.438197       1 request.go:700] Waited for 1.594777412s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:37.637854903Z I1024 13:05:37.637802       1 request.go:700] Waited for 1.394767854s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2024-10-24T13:05:38.842980853Z I1024 13:05:38.842769       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 1, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:05:39.163320104Z I1024 13:05:39.162610       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:39.766888439Z E1024 13:05:39.766136       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:39.766888439Z E1024 13:05:39.766165       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:39.766888439Z E1024 13:05:39.766175       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:39.768800419Z E1024 13:05:39.768590       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:40.519421705Z E1024 13:05:40.516976       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:40.519421705Z E1024 13:05:40.517002       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:40.519421705Z E1024 13:05:40.517018       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:40.519421705Z E1024 13:05:40.519252       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:40.682135236Z E1024 13:05:40.682048       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:40.682135236Z E1024 13:05:40.682075       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:40.682135236Z E1024 13:05:40.682081       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:40.684163036Z E1024 13:05:40.684122       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:42.851760679Z E1024 13:05:42.851461       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:42.851760679Z E1024 13:05:42.851730       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:42.851760679Z E1024 13:05:42.851738       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:42.853445579Z I1024 13:05:42.853400       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:05:42.853445579Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:05:42.853445579Z  CurrentRevision: (int32) 0,
2024-10-24T13:05:42.853445579Z  TargetRevision: (int32) 2,
2024-10-24T13:05:42.853445579Z  LastFailedRevision: (int32) 0,
2024-10-24T13:05:42.853445579Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:05:42.853445579Z  LastFailedReason: (string) "",
2024-10-24T13:05:42.853445579Z  LastFailedCount: (int) 0,
2024-10-24T13:05:42.853445579Z  LastFallbackCount: (int) 0,
2024-10-24T13:05:42.853445579Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:05:42.853445579Z }
2024-10-24T13:05:42.853445579Z  because new revision pending
2024-10-24T13:05:42.855718529Z W1024 13:05:42.855684       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:42.855718529Z W1024 13:05:42.855705       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:42.855718529Z W1024 13:05:42.855712       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:05:42.855799979Z W1024 13:05:42.855718       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:05:42.855799979Z W1024 13:05:42.855723       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:05:42.857194609Z E1024 13:05:42.857170       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:42.892148177Z I1024 13:05:42.892100       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:42.894560997Z I1024 13:05:42.894508       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:05:42.897831767Z I1024 13:05:42.897794       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:42.908944666Z I1024 13:05:42.907930       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 1" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 1" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2"
2024-10-24T13:05:42.939845644Z I1024 13:05:42.939799       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:05:42.941199884Z I1024 13:05:42.941162       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:43.726067898Z E1024 13:05:43.726010       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:43.726067898Z E1024 13:05:43.726047       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:43.726067898Z E1024 13:05:43.726053       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:43.728288188Z E1024 13:05:43.728213       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:44.038324130Z I1024 13:05:44.038276       1 request.go:700] Waited for 1.152399683s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:45.238226630Z I1024 13:05:45.237719       1 request.go:700] Waited for 1.394491898s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:46.238162812Z I1024 13:05:46.238096       1 request.go:700] Waited for 1.395670159s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:05:46.998551958Z E1024 13:05:46.998499       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:46.998551958Z E1024 13:05:46.998527       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:46.998551958Z E1024 13:05:46.998533       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:47.002223568Z E1024 13:05:47.002171       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:48.242174095Z I1024 13:05:48.242114       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:05:48.242174095Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:05:48.242174095Z  CurrentRevision: (int32) 0,
2024-10-24T13:05:48.242174095Z  TargetRevision: (int32) 2,
2024-10-24T13:05:48.242174095Z  LastFailedRevision: (int32) 0,
2024-10-24T13:05:48.242174095Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:05:48.242174095Z  LastFailedReason: (string) "",
2024-10-24T13:05:48.242174095Z  LastFailedCount: (int) 0,
2024-10-24T13:05:48.242174095Z  LastFallbackCount: (int) 0,
2024-10-24T13:05:48.242174095Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:05:48.242174095Z }
2024-10-24T13:05:48.242174095Z  because new revision pending
2024-10-24T13:05:50.455065716Z I1024 13:05:50.454960       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-2-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it was missing
2024-10-24T13:05:50.456009836Z E1024 13:05:50.455977       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:50.456073736Z E1024 13:05:50.456060       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:50.456131186Z E1024 13:05:50.456117       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:50.458003576Z E1024 13:05:50.457944       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:50.467398675Z E1024 13:05:50.467160       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:50.467398675Z E1024 13:05:50.467386       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:50.467398675Z E1024 13:05:50.467392       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:50.469327805Z E1024 13:05:50.469295       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:50.482826745Z E1024 13:05:50.482805       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:50.482881755Z E1024 13:05:50.482866       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:50.482928675Z E1024 13:05:50.482904       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:50.484800695Z E1024 13:05:50.484744       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:50.738184010Z E1024 13:05:50.738127       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:50.738265860Z E1024 13:05:50.738252       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:50.738296530Z E1024 13:05:50.738286       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:50.740368710Z E1024 13:05:50.740257       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:50.844474414Z I1024 13:05:50.844227       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:05:50.997824735Z E1024 13:05:50.997582       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:50.997824735Z E1024 13:05:50.997626       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:50.997824735Z E1024 13:05:50.997634       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:50.999981115Z E1024 13:05:50.999916       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:51.554054262Z E1024 13:05:51.553997       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:05:51.554054262Z E1024 13:05:51.554028       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:05:51.554054262Z E1024 13:05:51.554034       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:05:51.555993062Z E1024 13:05:51.555944       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:05:54.161076391Z I1024 13:05:54.161028       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:05:54.242043086Z I1024 13:05:54.241974       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:05:55.443225146Z I1024 13:05:55.442982       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:06:01.652601624Z W1024 13:06:01.652244       1 degraded_webhook.go:147] failed to connect to webhook "multus-validating-config.k8s.io" via service "multus-admission-controller.openshift-multus.svc:443": dial tcp: lookup multus-admission-controller.openshift-multus.svc: i/o timeout
2024-10-24T13:06:02.401081481Z E1024 13:06:02.400971       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:02.401081481Z E1024 13:06:02.401059       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:02.401081481Z E1024 13:06:02.401066       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:02.403562591Z E1024 13:06:02.403523       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:02.425117939Z I1024 13:06:02.425071       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:06:02.656568236Z E1024 13:06:02.656496       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:02.656568236Z E1024 13:06:02.656531       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:02.656568236Z E1024 13:06:02.656537       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:02.658139046Z E1024 13:06:02.658083       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:02.663821316Z E1024 13:06:02.663780       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:02.663905575Z E1024 13:06:02.663890       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:02.663983165Z E1024 13:06:02.663933       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:02.665430245Z E1024 13:06:02.665399       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:04.200459006Z I1024 13:06:04.200393       1 request.go:700] Waited for 1.145512683s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:06:06.406402547Z I1024 13:06:06.406170       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:06:07.951810427Z I1024 13:06:07.951359       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:07.952294917Z I1024 13:06:07.952269       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:08.607129488Z I1024 13:06:08.607070       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:06:10.502013448Z E1024 13:06:10.501977       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:10.502092738Z E1024 13:06:10.502080       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:10.502134278Z E1024 13:06:10.502123       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:10.504457878Z E1024 13:06:10.504411       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:11.029220937Z E1024 13:06:11.028408       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:11.029220937Z E1024 13:06:11.028439       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:11.029220937Z E1024 13:06:11.028451       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:11.030617287Z E1024 13:06:11.030587       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:12.005285070Z I1024 13:06:12.005198       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:12.050125528Z I1024 13:06:12.050063       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:14.634895536Z E1024 13:06:14.633803       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:14.635051697Z E1024 13:06:14.635032       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:14.637784846Z E1024 13:06:14.635215       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:14.640586416Z E1024 13:06:14.640523       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:21.255132550Z E1024 13:06:21.253297       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:21.255132550Z E1024 13:06:21.253776       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:21.255132550Z E1024 13:06:21.253786       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:21.255958010Z E1024 13:06:21.255917       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:38.368478212Z I1024 13:06:38.367579       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:06:38.374606242Z I1024 13:06:38.374526       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:06:38.374606242Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:06:38.374606242Z   	"apiServerArguments": map[string]any{
2024-10-24T13:06:38.374606242Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:06:38.374606242Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:06:38.374606242Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:06:38.374606242Z   		"etcd-servers": []any{
2024-10-24T13:06:38.374606242Z + 			string("https://10.0.0.3:2379"),
2024-10-24T13:06:38.374606242Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:06:38.374606242Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:06:38.374606242Z   			string("https://localhost:2379"),
2024-10-24T13:06:38.374606242Z   		},
2024-10-24T13:06:38.374606242Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:06:38.374606242Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:06:38.374606242Z   		... // 3 identical entries
2024-10-24T13:06:38.374606242Z   	},
2024-10-24T13:06:38.374606242Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:06:38.374606242Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:06:38.374606242Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:06:38.374606242Z   }
2024-10-24T13:06:38.412414759Z I1024 13:06:38.412351       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:38.414023219Z I1024 13:06:38.413975       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:38.819281496Z I1024 13:06:38.819220       1 core.go:352] ConfigMap "openshift-kube-apiserver/config" changes: {"apiVersion":"v1","data":{"config.yaml":"{\"admission\":{\"pluginConfig\":{\"PodSecurity\":{\"configuration\":{\"apiVersion\":\"pod-security.admission.config.k8s.io/v1\",\"defaults\":{\"audit\":\"restricted\",\"audit-version\":\"latest\",\"enforce\":\"restricted\",\"enforce-version\":\"latest\",\"warn\":\"restricted\",\"warn-version\":\"latest\"},\"exemptions\":{\"usernames\":[\"system:serviceaccount:openshift-infra:build-controller\"]},\"kind\":\"PodSecurityConfiguration\"}},\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"200\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/policy.yaml\"],\"authentication-token-webhook-config-file\":[\"/etc/kubernetes/static-pod-resources/secrets/webhook-authenticator/kubeConfig\"],\"authentication-token-webhook-version\":[\"v1\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"ValidatingAdmissionPolicy\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\",\"storage.openshift.io/CSIInlineVolumeSecurity\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-healthcheck-timeout\":[\"9s\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-readycheck-timeout\":[\"9s\"],\"etcd-servers\":[\"https://10.0.0.3:2379\",\"https://10.0.0.5:2379\",\"https://10.0.0.6:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"AWSEFSDriverVolumeMetrics=true\",\"AdminNetworkPolicy=true\",\"AlibabaPlatform=true\",\"AzureWorkloadIdentity=true\",\"BareMetalLoadBalancer=true\",\"BuildCSIVolumes=true\",\"ChunkSizeMiB=true\",\"CloudDualStackNodeIPs=true\",\"DisableKubeletCloudCredentialProviders=true\",\"GCPLabelsTags=true\",\"HardwareSpeed=true\",\"IngressControllerLBSubnetsAWS=true\",\"KMSv1=true\",\"ManagedBootImages=true\",\"MetricsServer=true\",\"MultiArchInstallAWS=true\",\"MultiArchInstallGCP=true\",\"NetworkDiagnosticsConfig=true\",\"NetworkLiveMigration=true\",\"NodeDisruptionPolicy=true\",\"OpenShiftPodSecurityAdmission=true\",\"PrivateHostedZoneAWS=true\",\"SetEIPForNLBIngressController=true\",\"VSphereControlPlaneMachineSet=true\",\"VSphereDriverConfiguration=true\",\"VSphereStaticIPs=true\",\"ValidatingAdmissionPolicy=true\",\"AWSClusterHostedDNS=false\",\"AdditionalRoutingCapabilities=false\",\"AutomatedEtcdBackup=false\",\"BootcNodeManagement=false\",\"CSIDriverSharedResource=false\",\"ClusterAPIInstall=false\",\"ClusterAPIInstallIBMCloud=false\",\"ClusterMonitoringConfig=false\",\"DNSNameResolver=false\",\"DynamicResourceAllocation=false\",\"EtcdBackendQuota=false\",\"EventedPLEG=false\",\"Example=false\",\"ExternalOIDC=false\",\"GCPClusterHostedDNS=false\",\"GatewayAPI=false\",\"ImageStreamImportMode=false\",\"IngressControllerDynamicConfigurationManager=false\",\"InsightsConfig=false\",\"InsightsConfigAPI=false\",\"InsightsOnDemandDataGather=false\",\"InsightsRuntimeExtractor=false\",\"MachineAPIMigration=false\",\"MachineAPIOperatorDisableMachineHealthCheckController=false\",\"MachineAPIProviderOpenStack=false\",\"MachineConfigNodes=false\",\"ManagedBootImagesAWS=false\",\"MaxUnavailableStatefulSet=false\",\"MetricsCollectionProfiles=false\",\"MixedCPUsAllocation=false\",\"MultiArchInstallAzure=false\",\"NetworkSegmentation=false\",\"NewOLM=false\",\"NodeSwap=false\",\"OVNObservability=false\",\"OnClusterBuild=false\",\"PersistentIPsForVirtualization=false\",\"PinnedImages=false\",\"PlatformOperators=false\",\"ProcMountType=false\",\"RouteAdvertisements=false\",\"RouteExternalCertificate=false\",\"ServiceAccountTokenNodeBinding=false\",\"SignatureStores=false\",\"SigstoreImageVerification=false\",\"TranslateStreamCloseWebsocketRequests=false\",\"UpgradeStatus=false\",\"UserNamespacesPodSecurityStandards=false\",\"UserNamespacesSupport=false\",\"VSphereMultiNetworks=false\",\"VSphereMultiVCenters=false\",\"VolumeGroupSnapshot=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.key\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"admissionregistration.k8s.io/v1beta1=true\"],\"send-retry-after-while-not-ready-once\":[\"false\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"shutdown-send-retry-after\":[\"true\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"strict-transport-security-directives\":[\"max-age=31536000,includeSubDomains,preload\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}}"},"kind":"ConfigMap","metadata":{"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null}}
2024-10-24T13:06:38.822288446Z I1024 13:06:38.822238       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/config -n openshift-kube-apiserver:
2024-10-24T13:06:38.822288446Z cause by changes in data.config.yaml
2024-10-24T13:06:38.822313636Z I1024 13:06:38.822280       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:38.823030915Z I1024 13:06:38.822982       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 3 triggered by "required configmap/config has changed"
2024-10-24T13:06:40.011029836Z I1024 13:06:40.010733       1 request.go:700] Waited for 1.18778887s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:06:40.023548285Z I1024 13:06:40.023489       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:40.821086639Z I1024 13:06:40.820919       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:40.821267029Z I1024 13:06:40.821244       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:40.969690390Z E1024 13:06:40.966712       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:40.969690390Z E1024 13:06:40.966739       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:40.969690390Z E1024 13:06:40.966775       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:40.969690390Z E1024 13:06:40.968979       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:41.210401156Z I1024 13:06:41.210336       1 request.go:700] Waited for 1.186284881s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:06:41.417051294Z I1024 13:06:41.417013       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:06:41.500063530Z E1024 13:06:41.500016       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:41.500178080Z E1024 13:06:41.500161       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:41.500216569Z E1024 13:06:41.500204       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:41.502360509Z E1024 13:06:41.502298       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:42.021239639Z I1024 13:06:42.021157       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:42.021703219Z I1024 13:06:42.021661       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:42.210682778Z I1024 13:06:42.210620       1 request.go:700] Waited for 1.187618621s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:06:43.211357467Z I1024 13:06:43.210919       1 request.go:700] Waited for 1.189838599s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:06:43.218292667Z I1024 13:06:43.218247       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:43.218570557Z I1024 13:06:43.218545       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:43.655116029Z W1024 13:06:43.654810       1 degraded_webhook.go:147] failed to connect to webhook "vwb.performance.openshift.io" via service "performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc:443": dial tcp: lookup performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc: i/o timeout
2024-10-24T13:06:44.410327884Z I1024 13:06:44.410261       1 request.go:700] Waited for 1.192195896s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:06:44.417220364Z I1024 13:06:44.417148       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:44.417896635Z I1024 13:06:44.417862       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:45.286903397Z E1024 13:06:45.286859       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:45.287001617Z E1024 13:06:45.286982       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:45.287044998Z E1024 13:06:45.287032       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:45.289226727Z E1024 13:06:45.289200       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:45.410978575Z I1024 13:06:45.410929       1 request.go:700] Waited for 1.196283147s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/localhost-recovery-client-token
2024-10-24T13:06:45.620181031Z I1024 13:06:45.620133       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:45.620477161Z I1024 13:06:45.620430       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:46.609394882Z E1024 13:06:46.607329       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:46.609394882Z E1024 13:06:46.607358       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:46.609394882Z E1024 13:06:46.607371       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:46.611372982Z I1024 13:06:46.611328       1 request.go:700] Waited for 1.190489667s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:06:46.613060282Z E1024 13:06:46.612994       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:46.818183388Z I1024 13:06:46.818122       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:46.818880288Z I1024 13:06:46.818831       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:47.414725846Z I1024 13:06:47.414317       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:06:47.810274149Z I1024 13:06:47.810219       1 request.go:700] Waited for 1.190745017s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-pod
2024-10-24T13:06:48.017270195Z I1024 13:06:48.017193       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:48.017617455Z I1024 13:06:48.017561       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:48.810956710Z I1024 13:06:48.810904       1 request.go:700] Waited for 1.195810476s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:06:48.990700546Z E1024 13:06:48.990658       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:48.990847986Z E1024 13:06:48.990831       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:48.990900026Z E1024 13:06:48.990884       1 guard_controller.go:300] Missing PodIP in operand kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0 on node ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:49.016552685Z E1024 13:06:49.016487       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing PodIP in operand kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0 on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:49.019525186Z I1024 13:06:49.019478       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing PodIP in operand kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0 on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:06:49.019894576Z I1024 13:06:49.019867       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:49.021598105Z I1024 13:06:49.021566       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:49.034078245Z I1024 13:06:49.034010       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]" to "GuardControllerDegraded: [Missing PodIP in operand kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0 on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:49.220621342Z I1024 13:06:49.220571       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:49.221339532Z I1024 13:06:49.220717       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:50.010442106Z I1024 13:06:50.010243       1 request.go:700] Waited for 1.154808078s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:06:51.010508637Z I1024 13:06:51.010457       1 request.go:700] Waited for 1.991269992s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:06:51.223329103Z I1024 13:06:51.223286       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:51.223603673Z I1024 13:06:51.223553       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:51.868854900Z I1024 13:06:51.868791       1 tlsconfig.go:181] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2024-10-24 12:49:31 +0000 UTC to 2034-10-22 12:49:31 +0000 UTC (now=2024-10-24 13:06:51.868722991 +0000 UTC))"
2024-10-24T13:06:51.868854900Z I1024 13:06:51.868837       1 tlsconfig.go:181] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2024-10-24 12:49:34 +0000 UTC to 2024-10-25 12:49:34 +0000 UTC (now=2024-10-24 13:06:51.868818251 +0000 UTC))"
2024-10-24T13:06:51.868892921Z I1024 13:06:51.868857       1 tlsconfig.go:181] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2024-10-24 12:49:34 +0000 UTC to 2025-10-24 12:49:34 +0000 UTC (now=2024-10-24 13:06:51.868843991 +0000 UTC))"
2024-10-24T13:06:51.868892921Z I1024 13:06:51.868876       1 tlsconfig.go:181] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2024-10-24 12:49:34 +0000 UTC to 2025-10-24 12:49:34 +0000 UTC (now=2024-10-24 13:06:51.86886332 +0000 UTC))"
2024-10-24T13:06:51.868904071Z I1024 13:06:51.868896       1 tlsconfig.go:181] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2024-10-24 12:49:32 +0000 UTC to 2034-10-22 12:49:32 +0000 UTC (now=2024-10-24 13:06:51.868882021 +0000 UTC))"
2024-10-24T13:06:51.868921901Z I1024 13:06:51.868915       1 tlsconfig.go:181] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1729775012\" [] issuer=\"kubelet-signer\" (2024-10-24 13:03:31 +0000 UTC to 2024-10-25 12:49:34 +0000 UTC (now=2024-10-24 13:06:51.868901891 +0000 UTC))"
2024-10-24T13:06:51.868993101Z I1024 13:06:51.868934       1 tlsconfig.go:181] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1729775011\" [] issuer=\"<self>\" (2024-10-24 13:03:30 +0000 UTC to 2025-10-24 13:03:31 +0000 UTC (now=2024-10-24 13:06:51.868920581 +0000 UTC))"
2024-10-24T13:06:51.868993101Z I1024 13:06:51.868961       1 tlsconfig.go:181] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2024-10-24 12:49:32 +0000 UTC to 2024-10-25 12:49:32 +0000 UTC (now=2024-10-24 13:06:51.868947301 +0000 UTC))"
2024-10-24T13:06:51.869168191Z I1024 13:06:51.869142       1 tlsconfig.go:203] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-apiserver-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-apiserver-operator.svc,metrics.openshift-kube-apiserver-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1729775010\" (2024-10-24 13:03:41 +0000 UTC to 2026-10-24 13:03:42 +0000 UTC (now=2024-10-24 13:06:51.869119521 +0000 UTC))"
2024-10-24T13:06:51.869327011Z I1024 13:06:51.869275       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1729775099\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1729775099\" (2024-10-24 12:04:59 +0000 UTC to 2025-10-24 12:04:59 +0000 UTC (now=2024-10-24 13:06:51.869257631 +0000 UTC))"
2024-10-24T13:06:52.010526108Z I1024 13:06:52.010464       1 request.go:700] Waited for 2.196434878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:53.210376995Z I1024 13:06:53.210319       1 request.go:700] Waited for 1.988185192s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets
2024-10-24T13:06:53.219877195Z I1024 13:06:53.219833       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:54.166525947Z I1024 13:06:54.164039       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:54.193654217Z I1024 13:06:54.193572       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:54.210882816Z I1024 13:06:54.210830       1 request.go:700] Waited for 1.996553602s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:06:54.615455159Z I1024 13:06:54.615394       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-apiserver version "operator" changed from "" to "4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest"
2024-10-24T13:06:54.615455159Z I1024 13:06:54.615431       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-apiserver version "kube-apiserver" changed from "" to "1.31.1"
2024-10-24T13:06:54.615655198Z I1024 13:06:54.615635       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"versions":[{"name":"raw-internal","version":"4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest"},{"name":"operator","version":"4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest"},{"name":"kube-apiserver","version":"1.31.1"}]}}
2024-10-24T13:06:54.630785888Z I1024 13:06:54.628702       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: status.versions changed from [{"raw-internal" "4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest"}] to [{"raw-internal" "4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest"} {"operator" "4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest"} {"kube-apiserver" "1.31.1"}]
2024-10-24T13:06:54.656975178Z W1024 13:06:54.656886       1 degraded_webhook.go:147] failed to connect to webhook "vwb.performance.openshift.io" via service "performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc:443": dial tcp: lookup performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc: i/o timeout
2024-10-24T13:06:55.410862303Z I1024 13:06:55.410783       1 request.go:700] Waited for 1.996055412s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods
2024-10-24T13:06:55.422947173Z E1024 13:06:55.422917       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:06:55.423010593Z E1024 13:06:55.422995       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:55.423244003Z I1024 13:06:55.423204       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:55.453700262Z E1024 13:06:55.453638       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:55.455507762Z E1024 13:06:55.455476       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:06:55.457931442Z I1024 13:06:55.457886       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:06:55.468634882Z I1024 13:06:55.468568       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:06:55.472059502Z I1024 13:06:55.472008       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:06:55.484849662Z I1024 13:06:55.483394       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0 on node ci-op-2fcpj5j6-f6035-2lklf-master-0, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:06:56.410914254Z I1024 13:06:56.410833       1 request.go:700] Waited for 1.993500012s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2024-10-24T13:06:57.221040139Z I1024 13:06:57.220969       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:57.610293941Z I1024 13:06:57.610246       1 request.go:700] Waited for 2.138605729s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:06:58.214805710Z I1024 13:06:58.214734       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because static pod is pending
2024-10-24T13:06:58.610863163Z I1024 13:06:58.610797       1 request.go:700] Waited for 2.193600379s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2024-10-24T13:06:59.419202947Z I1024 13:06:59.419140       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:06:59.610919303Z I1024 13:06:59.610839       1 request.go:700] Waited for 2.195993299s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:06:59.617825673Z I1024 13:06:59.617778       1 core.go:220] Pod "openshift-kube-apiserver/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"registry.build02.ci.openshift.org/ci-op-2fcpj5j6/stable@sha256:adc55330a214af3a57e2fac059ed409c2a7b1dc54d829a3ad1f719ce6c15ffa0","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.0.0.3","path":"readyz","port":6443,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"hostIPs":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2024-10-24T13:07:00.810745291Z I1024 13:07:00.810563       1 request.go:700] Waited for 1.794715835s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2024-10-24T13:07:01.220375382Z I1024 13:07:01.220326       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-3 -n openshift-kube-apiserver because it was missing
2024-10-24T13:07:01.420993378Z E1024 13:07:01.420918       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:01.421128568Z I1024 13:07:01.421046       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it changed
2024-10-24T13:07:01.425979548Z E1024 13:07:01.425945       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:02.010032377Z I1024 13:07:02.009962       1 request.go:700] Waited for 1.754889606s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:07:02.825048672Z I1024 13:07:02.824500       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:02.825365031Z I1024 13:07:02.825307       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 3 triggered by "required configmap/config has changed"
2024-10-24T13:07:02.827520042Z W1024 13:07:02.827406       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:07:02.827520042Z W1024 13:07:02.827425       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:07:02.853364231Z W1024 13:07:02.853300       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:07:02.853364231Z W1024 13:07:02.853332       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:07:02.853885291Z I1024 13:07:02.853853       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:02.857601901Z I1024 13:07:02.857478       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:03.010067928Z I1024 13:07:03.010008       1 request.go:700] Waited for 1.756407236s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:07:04.010115399Z I1024 13:07:04.010068       1 request.go:700] Waited for 1.934490673s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2024-10-24T13:07:05.010864820Z I1024 13:07:05.010527       1 request.go:700] Waited for 1.989879021s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:07:05.075079829Z I1024 13:07:05.075025       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:05.216043366Z E1024 13:07:05.215976       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:05.216043366Z E1024 13:07:05.215998       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:05.217377226Z E1024 13:07:05.217334       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:05.218762076Z E1024 13:07:05.218712       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:06.010848921Z I1024 13:07:06.010789       1 request.go:700] Waited for 1.59674705s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:06.658777908Z E1024 13:07:06.658709       1 degraded_webhook.go:68] vwb.performance.openshift.io: dial tcp: lookup performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc: i/o timeout
2024-10-24T13:07:07.210357998Z I1024 13:07:07.210275       1 request.go:700] Waited for 1.194149007s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:07:07.615235190Z E1024 13:07:07.615160       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:07.616865100Z E1024 13:07:07.616824       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:07.815888657Z I1024 13:07:07.815829       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 2, but has not made progress because static pod is pending
2024-10-24T13:07:09.110000501Z I1024 13:07:09.109933       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:09.157830751Z I1024 13:07:09.157055       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:10.410631556Z I1024 13:07:10.410235       1 request.go:700] Waited for 1.089868129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:07:11.216095671Z E1024 13:07:11.216022       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:11.216095671Z E1024 13:07:11.216062       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:11.218208671Z E1024 13:07:11.218170       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:11.631044513Z I1024 13:07:11.630974       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:07:11.631044513Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:07:11.631044513Z  CurrentRevision: (int32) 0,
2024-10-24T13:07:11.631044513Z  TargetRevision: (int32) 3,
2024-10-24T13:07:11.631044513Z  LastFailedRevision: (int32) 0,
2024-10-24T13:07:11.631044513Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:07:11.631044513Z  LastFailedReason: (string) "",
2024-10-24T13:07:11.631044513Z  LastFailedCount: (int) 0,
2024-10-24T13:07:11.631044513Z  LastFallbackCount: (int) 0,
2024-10-24T13:07:11.631044513Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:07:11.631044513Z }
2024-10-24T13:07:11.631044513Z  because new revision pending
2024-10-24T13:07:11.632623653Z W1024 13:07:11.632580       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:07:11.632623653Z W1024 13:07:11.632601       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:07:11.632623653Z W1024 13:07:11.632605       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:07:11.632623653Z W1024 13:07:11.632609       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:07:11.632623653Z W1024 13:07:11.632612       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:07:11.663463262Z I1024 13:07:11.663410       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:11.665208932Z I1024 13:07:11.665178       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:11.666045472Z I1024 13:07:11.666002       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 3","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 3","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:07:11.678891472Z I1024 13:07:11.678816       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 3",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 3"
2024-10-24T13:07:11.692344222Z I1024 13:07:11.692273       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:11.693365902Z I1024 13:07:11.693323       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:12.810899630Z I1024 13:07:12.810828       1 request.go:700] Waited for 1.147507768s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:07:14.010278087Z I1024 13:07:14.010211       1 request.go:700] Waited for 1.389155283s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:07:15.211273894Z I1024 13:07:15.210914       1 request.go:700] Waited for 1.361903425s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:07:16.124273787Z E1024 13:07:16.122429       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:16.659141287Z W1024 13:07:16.659050       1 degraded_webhook.go:147] failed to connect to webhook "volumesnapshotclasses.snapshot.storage.k8s.io" via service "csi-snapshot-webhook.openshift-cluster-storage-operator.svc:443": dial tcp: lookup csi-snapshot-webhook.openshift-cluster-storage-operator.svc: i/o timeout
2024-10-24T13:07:17.615142709Z E1024 13:07:17.615079       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:17.616802849Z E1024 13:07:17.616670       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:24.164274364Z I1024 13:07:24.163767       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:26.521317729Z E1024 13:07:26.520798       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:26.521317729Z E1024 13:07:26.521276       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:26.523274129Z E1024 13:07:26.523243       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:27.661201807Z W1024 13:07:27.660847       1 degraded_webhook.go:147] failed to connect to webhook "volumesnapshotclasses.snapshot.storage.k8s.io" via service "csi-snapshot-webhook.openshift-cluster-storage-operator.svc:443": dial tcp: lookup csi-snapshot-webhook.openshift-cluster-storage-operator.svc: i/o timeout
2024-10-24T13:07:31.985523812Z E1024 13:07:31.985468       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:31.985523812Z E1024 13:07:31.985507       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:31.987916242Z E1024 13:07:31.987870       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:32.436674494Z E1024 13:07:32.436519       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:32.436674494Z E1024 13:07:32.436549       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:32.449336624Z E1024 13:07:32.449282       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:36.757676311Z E1024 13:07:36.757629       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:36.757799822Z E1024 13:07:36.757738       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:36.759989562Z E1024 13:07:36.759962       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:39.170508425Z I1024 13:07:39.170456       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:39.662990986Z E1024 13:07:39.662943       1 degraded_webhook.go:68] volumesnapshotclasses.snapshot.storage.k8s.io: dial tcp: lookup csi-snapshot-webhook.openshift-cluster-storage-operator.svc: i/o timeout
2024-10-24T13:07:39.669760125Z W1024 13:07:39.669708       1 degraded_webhook.go:147] failed to connect to webhook "volumegroupsnapshotclasses.groupsnapshot.storage.k8s.io" via service "csi-snapshot-webhook.openshift-cluster-storage-operator.svc:443": dial tcp: lookup csi-snapshot-webhook.openshift-cluster-storage-operator.svc on 172.30.0.10:53: read udp 10.130.0.17:47744->172.30.0.10:53: read: connection refused
2024-10-24T13:07:40.439036791Z I1024 13:07:40.438972       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-3-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it was missing
2024-10-24T13:07:40.447074850Z I1024 13:07:40.447032       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:07:40.605862567Z E1024 13:07:40.605809       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:40.605862567Z E1024 13:07:40.605830       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:40.607382108Z E1024 13:07:40.607327       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:40.678054936Z W1024 13:07:40.677996       1 degraded_webhook.go:147] failed to connect to webhook "volumegroupsnapshotclasses.groupsnapshot.storage.k8s.io" via service "csi-snapshot-webhook.openshift-cluster-storage-operator.svc:443": dial tcp: lookup csi-snapshot-webhook.openshift-cluster-storage-operator.svc on 172.30.0.10:53: read udp 10.130.0.17:50374->172.30.0.10:53: read: connection refused
2024-10-24T13:07:41.600246488Z I1024 13:07:41.600145       1 request.go:700] Waited for 1.01393151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:07:42.600583479Z I1024 13:07:42.600517       1 request.go:700] Waited for 1.454663432s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:07:42.686611387Z E1024 13:07:42.686543       1 degraded_webhook.go:68] volumegroupsnapshotclasses.groupsnapshot.storage.k8s.io: dial tcp: lookup csi-snapshot-webhook.openshift-cluster-storage-operator.svc on 172.30.0.10:53: read udp 10.130.0.17:49230->172.30.0.10:53: read: connection refused
2024-10-24T13:07:42.693393337Z W1024 13:07:42.693301       1 degraded_webhook.go:147] failed to connect to webhook "performanceprofiles.performance.openshift.io" via service "performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc:443": dial tcp: lookup performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc on 172.30.0.10:53: read udp 10.130.0.17:54217->172.30.0.10:53: read: connection refused
2024-10-24T13:07:43.405237154Z E1024 13:07:43.405185       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:43.405237154Z E1024 13:07:43.405205       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:43.406732853Z E1024 13:07:43.406633       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:43.701143428Z W1024 13:07:43.701026       1 degraded_webhook.go:147] failed to connect to webhook "performanceprofiles.performance.openshift.io" via service "performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc:443": dial tcp: lookup performance-addon-operator-service.openshift-cluster-node-tuning-operator.svc on 172.30.0.10:53: read udp 10.130.0.17:60982->172.30.0.10:53: read: connection refused
2024-10-24T13:07:43.800483436Z I1024 13:07:43.800425       1 request.go:700] Waited for 1.59039241s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:44.800712667Z I1024 13:07:44.800645       1 request.go:700] Waited for 1.595455079s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:07:45.718324909Z W1024 13:07:45.718247       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp: lookup prometheus-operator-admission-webhook.openshift-monitoring.svc on 172.30.0.10:53: read udp 10.130.0.17:53660->172.30.0.10:53: read: connection refused
2024-10-24T13:07:46.005080013Z E1024 13:07:46.005029       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:46.005190183Z E1024 13:07:46.005152       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:46.007160913Z E1024 13:07:46.007119       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:46.404650435Z I1024 13:07:46.404597       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:07:46.730113900Z W1024 13:07:46.730045       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp: lookup prometheus-operator-admission-webhook.openshift-monitoring.svc on 172.30.0.10:53: read udp 10.130.0.17:59776->172.30.0.10:53: read: connection refused
2024-10-24T13:07:48.405516726Z E1024 13:07:48.405440       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:48.405516726Z E1024 13:07:48.405478       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:48.407816777Z E1024 13:07:48.407730       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:48.769669830Z I1024 13:07:48.769623       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:48.774159721Z I1024 13:07:48.773942       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:48.786280442Z W1024 13:07:48.786226       1 degraded_webhook.go:147] failed to connect to webhook "default.machine.machine.openshift.io" via service "machine-api-operator-webhook.openshift-machine-api.svc:443": dial tcp: lookup machine-api-operator-webhook.openshift-machine-api.svc on 172.30.0.10:53: read udp 10.130.0.17:34097->172.30.0.10:53: read: connection refused
2024-10-24T13:07:49.795631837Z W1024 13:07:49.795572       1 degraded_webhook.go:147] failed to connect to webhook "default.machine.machine.openshift.io" via service "machine-api-operator-webhook.openshift-machine-api.svc:443": dial tcp: lookup machine-api-operator-webhook.openshift-machine-api.svc on 172.30.0.10:53: read udp 10.130.0.17:51200->172.30.0.10:53: read: connection refused
2024-10-24T13:07:49.799694867Z I1024 13:07:49.799645       1 request.go:700] Waited for 1.029222717s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:07:50.405121114Z I1024 13:07:50.405041       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:07:50.800317791Z I1024 13:07:50.800256       1 request.go:700] Waited for 1.194491912s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:07:51.911433615Z W1024 13:07:51.911357       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:07:52.915381579Z W1024 13:07:52.915321       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:07:53.404474175Z E1024 13:07:53.404413       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:07:53.404474175Z E1024 13:07:53.404445       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:07:53.406629665Z E1024 13:07:53.406595       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:07:53.605451374Z I1024 13:07:53.605394       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:07:54.167539257Z I1024 13:07:54.167488       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:54.967882622Z I1024 13:07:54.967579       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:54.980529343Z I1024 13:07:54.980465       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:07:57.409362861Z I1024 13:07:57.409289       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:07:59.600714066Z I1024 13:07:59.600651       1 request.go:700] Waited for 1.147421947s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/sa-token-signing-certs
2024-10-24T13:07:59.608671407Z I1024 13:07:59.608601       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:07:59.612968557Z I1024 13:07:59.612913       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 4 triggered by "required configmap/sa-token-signing-certs has changed"
2024-10-24T13:07:59.613828947Z I1024 13:07:59.613704       1 core.go:352] ConfigMap "openshift-kube-apiserver/sa-token-signing-certs" changes: {"data":{"service-account-002.pub":"-----BEGIN RSA PUBLIC KEY-----\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEApYEdlJOUNNdab1Iuqld4\nPN4sKBBIKCJeIMx34fi7pTN1WcE3YVbptMF9LAyw2QaB/kKXQxUGPcF2k/kBr1wz\nnNVBuMujJqETmQaBj088S6UB6NL+SgYp8Y/zqx6wrAI2gTK/1fYN8o4Ii3iRSlGm\nLafhh2bVHY6+jSfdHhKflT5f9mO+dtRyvaht5dsgXskifFSTsk+sKe4Zf+wBRBAb\nqXEMS35lU6A761MY7GP0Ufz1NkEz2oEODZFzy6Jr+XxInu7MdyDhzfI/bwSrJm7q\nmydVj+8i1KrZWq/WqKtKdZL+TV3eAQ42Ub/J/GWZe2eEIvDq8zo5z+Hox3JD8TDj\nOXhvXGv5YcxOS83DsYEKO6Iz1Po4mtfUIY32pZDELtVGPtmiG2BuWb9dRJPnBnYy\nye3pMxYqIh5vRR8ZKlG8V/C/ER7f4SC2XyIyf+8JVMeaMiITdJ1ZrLYvmrAKGopu\nYeQ8gTGPcMrya9jsiY79UpEIKP6gqtxIy6ss0xeLkn8b7pLD3aGd1HCuhBeaY5pv\ntu57H6W2v8lS57N9H/WRaJ3zEDl1DKIGPstZE77Pr8rMfQnKkQl7Dy8wi3uUZQAQ\nwnrbilgM/OVxzFpfCje2wMzffPgUi5sBzdpb5WkWfOmwiOKOwxCgKmQGaHMFqVVX\nSSxY/EMcbd14U8cLxCFbV7MCAwEAAQ==\n-----END RSA PUBLIC KEY-----\n"},"metadata":{"creationTimestamp":"2024-10-24T12:59:03Z","managedFields":[{"apiVersion":"v1","fieldsType":"FieldsV1","fieldsV1":{"f:data":{".":{},"f:service-account-001.pub":{}}},"manager":"cluster-bootstrap","operation":"Update","time":"2024-10-24T12:59:03Z"},{"apiVersion":"v1","fieldsType":"FieldsV1","fieldsV1":{"f:data":{"f:service-account-002.pub":{}}},"manager":"cluster-kube-controller-manager-operator","operation":"Update","time":"2024-10-24T13:07:58Z"}],"resourceVersion":null,"uid":"6892c918-516b-4409-b453-eafb3ec9f030"}}
2024-10-24T13:07:59.614229727Z I1024 13:07:59.614176       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/sa-token-signing-certs -n openshift-kube-apiserver:
2024-10-24T13:07:59.614229727Z cause by changes in data.service-account-002.pub
2024-10-24T13:08:00.800326779Z I1024 13:08:00.800278       1 request.go:700] Waited for 1.190953641s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:08:01.010389658Z I1024 13:08:01.010326       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:01.800692672Z I1024 13:08:01.800622       1 request.go:700] Waited for 1.643179485s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:02.808504107Z I1024 13:08:02.808428       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:02.808895537Z I1024 13:08:02.808869       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:02.999688035Z I1024 13:08:02.999625       1 request.go:700] Waited for 1.988961717s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:08:04.000445299Z I1024 13:08:04.000375       1 request.go:700] Waited for 1.794924889s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:04.005316899Z E1024 13:08:04.005263       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:04.005316899Z E1024 13:08:04.005295       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:04.007503619Z E1024 13:08:04.007459       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:04.612136736Z I1024 13:08:04.612085       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:04.613868546Z I1024 13:08:04.613746       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:05.200269112Z I1024 13:08:05.200195       1 request.go:700] Waited for 1.59617303s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:06.006186907Z I1024 13:08:06.006111       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:08:06.228604828Z I1024 13:08:06.228526       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:06.236152089Z I1024 13:08:06.236093       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:06.400333594Z I1024 13:08:06.400255       1 request.go:700] Waited for 1.59353311s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:08:07.004943831Z E1024 13:08:07.004891       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:07.004943831Z E1024 13:08:07.004923       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:07.007681241Z E1024 13:08:07.007044       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:07.600037977Z I1024 13:08:07.599928       1 request.go:700] Waited for 1.363602838s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:08:07.608958278Z I1024 13:08:07.608904       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:07.609134368Z I1024 13:08:07.609081       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:07.694324826Z E1024 13:08:07.693675       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:08.600235771Z I1024 13:08:08.600187       1 request.go:700] Waited for 1.194833192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:08.814052301Z I1024 13:08:08.814001       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:08.814450671Z I1024 13:08:08.814429       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:09.800375933Z I1024 13:08:09.800300       1 request.go:700] Waited for 1.394159721s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/localhost-recovery-client-token
2024-10-24T13:08:10.210700882Z I1024 13:08:10.210633       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:10.215181763Z I1024 13:08:10.215140       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:10.404091100Z E1024 13:08:10.404029       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:10.405838240Z E1024 13:08:10.405692       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:10.803806858Z I1024 13:08:10.802857       1 request.go:700] Waited for 1.398489301s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:08:11.614594464Z I1024 13:08:11.614523       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:11.614594464Z I1024 13:08:11.614523       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:12.000387040Z I1024 13:08:12.000322       1 request.go:700] Waited for 1.130990707s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:08:12.605181077Z I1024 13:08:12.605110       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:08:12.808285056Z I1024 13:08:12.808212       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:12.808285056Z I1024 13:08:12.808244       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:13.200435123Z I1024 13:08:13.200375       1 request.go:700] Waited for 1.128891806s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2024-10-24T13:08:14.007149888Z I1024 13:08:14.007069       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:14.007492338Z I1024 13:08:14.007431       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:14.400489685Z I1024 13:08:14.400357       1 request.go:700] Waited for 1.195374252s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:08:15.210512291Z I1024 13:08:15.210398       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:15.600257767Z I1024 13:08:15.600193       1 request.go:700] Waited for 1.154318399s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:08:17.409715147Z I1024 13:08:17.409665       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:18.005292213Z I1024 13:08:18.005224       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:08:18.210152002Z I1024 13:08:18.210070       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:18.985971375Z I1024 13:08:18.985877       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0" (last termination at 2024-10-24 13:06:48 +0000 UTC) at 2024-10-24 13:08:18 +0000 UTC
2024-10-24T13:08:19.210272876Z I1024 13:08:19.210210       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-4 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:19.600673712Z I1024 13:08:19.600609       1 request.go:700] Waited for 1.010849444s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:20.407141629Z I1024 13:08:20.407093       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:20.407543098Z I1024 13:08:20.407513       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 4 triggered by "required configmap/sa-token-signing-certs has changed"
2024-10-24T13:08:20.408530539Z W1024 13:08:20.408486       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:08:20.408530539Z W1024 13:08:20.408506       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:08:20.434353521Z I1024 13:08:20.434319       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:20.445398882Z W1024 13:08:20.445356       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:08:20.445398882Z W1024 13:08:20.445383       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:08:20.453774793Z I1024 13:08:20.447827       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:08:20.800138266Z I1024 13:08:20.800074       1 request.go:700] Waited for 1.194999902s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:20.805015196Z E1024 13:08:20.804980       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:20.805015196Z E1024 13:08:20.804998       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:20.806567106Z E1024 13:08:20.806528       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:21.800394019Z I1024 13:08:21.800317       1 request.go:700] Waited for 1.356932797s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:23.000536252Z I1024 13:08:23.000368       1 request.go:700] Waited for 1.39338208s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:08:23.804517257Z I1024 13:08:23.804438       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:08:24.915165022Z I1024 13:08:24.915117       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:08:26.614514351Z I1024 13:08:26.614447       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:08:26.614514351Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:08:26.614514351Z  CurrentRevision: (int32) 0,
2024-10-24T13:08:26.614514351Z  TargetRevision: (int32) 4,
2024-10-24T13:08:26.614514351Z  LastFailedRevision: (int32) 0,
2024-10-24T13:08:26.614514351Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:08:26.614514351Z  LastFailedReason: (string) "",
2024-10-24T13:08:26.614514351Z  LastFailedCount: (int) 0,
2024-10-24T13:08:26.614514351Z  LastFallbackCount: (int) 0,
2024-10-24T13:08:26.614514351Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:08:26.614514351Z }
2024-10-24T13:08:26.614514351Z  because new revision pending
2024-10-24T13:08:26.614714142Z E1024 13:08:26.614688       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:26.617072371Z W1024 13:08:26.617012       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:08:26.617072371Z W1024 13:08:26.617036       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:08:26.617072371Z W1024 13:08:26.617044       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:08:26.617072371Z W1024 13:08:26.617048       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:08:26.617072371Z W1024 13:08:26.617055       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:08:26.648604965Z I1024 13:08:26.648545       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:08:26.651160045Z I1024 13:08:26.651102       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:26.652027075Z I1024 13:08:26.651841       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:08:26.665876686Z I1024 13:08:26.665136       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 3" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 3" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4"
2024-10-24T13:08:26.682372888Z I1024 13:08:26.682314       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:26.684485678Z I1024 13:08:26.682942       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:08:27.800630172Z I1024 13:08:27.800509       1 request.go:700] Waited for 1.154404178s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:29.000522715Z I1024 13:08:29.000430       1 request.go:700] Waited for 1.795019508s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:29.004784895Z E1024 13:08:29.004712       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:29.006515955Z E1024 13:08:29.006447       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:30.000713029Z I1024 13:08:30.000647       1 request.go:700] Waited for 1.5942348s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:08:30.204644078Z E1024 13:08:30.204600       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:32.005087497Z E1024 13:08:32.005029       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:32.006678287Z E1024 13:08:32.006630       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:32.805108762Z I1024 13:08:32.805028       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:08:32.805108762Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:08:32.805108762Z  CurrentRevision: (int32) 0,
2024-10-24T13:08:32.805108762Z  TargetRevision: (int32) 4,
2024-10-24T13:08:32.805108762Z  LastFailedRevision: (int32) 0,
2024-10-24T13:08:32.805108762Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:08:32.805108762Z  LastFailedReason: (string) "",
2024-10-24T13:08:32.805108762Z  LastFailedCount: (int) 0,
2024-10-24T13:08:32.805108762Z  LastFallbackCount: (int) 0,
2024-10-24T13:08:32.805108762Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:08:32.805108762Z }
2024-10-24T13:08:32.805108762Z  because new revision pending
2024-10-24T13:08:35.614599605Z I1024 13:08:35.614524       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:36.006448252Z I1024 13:08:36.006391       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:08:36.800239907Z I1024 13:08:36.800168       1 request.go:700] Waited for 1.183831131s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:08:37.205399775Z E1024 13:08:37.205328       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:37.205399775Z E1024 13:08:37.205358       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:37.206993695Z E1024 13:08:37.206946       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:38.404630077Z I1024 13:08:38.404558       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:08:40.101906296Z E1024 13:08:40.101844       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:40.102037567Z E1024 13:08:40.102021       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:40.104369917Z E1024 13:08:40.104345       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:40.724178315Z E1024 13:08:40.724125       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:40.730928606Z E1024 13:08:40.730883       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:40.759171758Z E1024 13:08:40.759098       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:40.836856635Z E1024 13:08:40.833816       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:40.836856635Z E1024 13:08:40.833844       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:40.851079817Z E1024 13:08:40.851007       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:50.045472809Z E1024 13:08:50.045400       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:50.045472809Z E1024 13:08:50.045445       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:50.047698359Z E1024 13:08:50.047657       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:50.238297837Z E1024 13:08:50.238247       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:08:50.248016928Z E1024 13:08:50.247945       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:08:50.250162678Z E1024 13:08:50.250113       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:08:53.105925391Z I1024 13:08:53.105874       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:08:53.113184681Z I1024 13:08:53.113145       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:53.119542741Z I1024 13:08:53.119478       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:53.120910361Z I1024 13:08:53.120652       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:53.489783744Z I1024 13:08:53.489570       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:53.489918414Z I1024 13:08:53.489865       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:53.543874766Z I1024 13:08:53.543825       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:08:53.544317696Z I1024 13:08:53.544261       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:08:54.112823675Z I1024 13:08:54.112728       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:54.281265231Z I1024 13:08:54.280531       1 request.go:700] Waited for 1.130830109s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:08:54.484274788Z I1024 13:08:54.484219       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:08:54.686418985Z I1024 13:08:54.686355       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:54.686791706Z I1024 13:08:54.686733       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:55.479724393Z I1024 13:08:55.479670       1 request.go:700] Waited for 1.106164628s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2024-10-24T13:08:55.890399687Z I1024 13:08:55.890287       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:55.890860947Z I1024 13:08:55.890817       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:56.480558897Z I1024 13:08:56.480506       1 request.go:700] Waited for 1.189061071s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:08:57.087876719Z I1024 13:08:57.087805       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:57.088217769Z I1024 13:08:57.088193       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:57.680398789Z I1024 13:08:57.680325       1 request.go:700] Waited for 1.186927511s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:08:58.288363391Z I1024 13:08:58.288273       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:58.288450270Z I1024 13:08:58.288413       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:58.680592304Z I1024 13:08:58.680531       1 request.go:700] Waited for 1.194504952s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:08:59.487271832Z I1024 13:08:59.487213       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:08:59.488848832Z I1024 13:08:59.488692       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:08:59.879973326Z I1024 13:08:59.879904       1 request.go:700] Waited for 1.193681282s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:09:00.348164182Z E1024 13:09:00.348119       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:09:00.485117627Z I1024 13:09:00.485052       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Pending phase
2024-10-24T13:09:00.688530094Z I1024 13:09:00.688442       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:09:00.688858274Z I1024 13:09:00.688808       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:09:00.880723251Z I1024 13:09:00.880658       1 request.go:700] Waited for 1.196119401s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/localhost-recovery-client-token
2024-10-24T13:09:02.080522532Z I1024 13:09:02.080456       1 request.go:700] Waited for 1.729698279s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:09:02.490169526Z I1024 13:09:02.490101       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:09:02.490259296Z I1024 13:09:02.490156       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:09:03.080716796Z I1024 13:09:03.080652       1 request.go:700] Waited for 1.825166692s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:09:03.884889725Z E1024 13:09:03.884825       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:09:03.887165594Z E1024 13:09:03.887121       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:09:04.280026658Z I1024 13:09:04.279945       1 request.go:700] Waited for 1.790845752s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets
2024-10-24T13:09:04.290027559Z I1024 13:09:04.289939       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:09:04.884264439Z W1024 13:09:04.884197       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:04.884264439Z E1024 13:09:04.884254       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:05.082906186Z E1024 13:09:05.082843       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:05.280097682Z I1024 13:09:05.279999       1 request.go:700] Waited for 1.594382225s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:09:05.683337136Z E1024 13:09:05.683278       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:05.683400716Z I1024 13:09:05.683350       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Delete "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:05.684055596Z E1024 13:09:05.684016       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:09:05.687011266Z I1024 13:09:05.686940       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:06.083210940Z W1024 13:09:06.083148       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:06.083259300Z E1024 13:09:06.083206       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:06.280438767Z I1024 13:09:06.280385       1 request.go:700] Waited for 1.190889421s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:09:06.503478874Z E1024 13:09:06.503412       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:06.681546271Z I1024 13:09:06.681479       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:06.682443771Z E1024 13:09:06.682394       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:09:06.683268561Z W1024 13:09:06.683180       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:06.683268561Z W1024 13:09:06.683208       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:06.683268561Z W1024 13:09:06.683216       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:06.684039471Z E1024 13:09:06.683996       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:06.883458298Z E1024 13:09:06.883395       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:06.883499958Z I1024 13:09:06.883444       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:06.886594628Z I1024 13:09:06.886530       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:07.280806622Z I1024 13:09:07.280735       1 request.go:700] Waited for 1.195880451s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:09:07.284028942Z W1024 13:09:07.283968       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:07.284028942Z E1024 13:09:07.284008       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:08.083799840Z E1024 13:09:08.083722       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:08.083861730Z I1024 13:09:08.083783       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:08.086914670Z I1024 13:09:08.086819       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:08.480477664Z I1024 13:09:08.480369       1 request.go:700] Waited for 1.194713811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:09:08.483710264Z W1024 13:09:08.483638       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:08.483710264Z E1024 13:09:08.483677       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:08.683198521Z E1024 13:09:08.683133       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:09.283551142Z E1024 13:09:09.283480       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:09.283551142Z I1024 13:09:09.283518       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:09.286720862Z I1024 13:09:09.286632       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:09.480503689Z I1024 13:09:09.480434       1 request.go:700] Waited for 1.199460563s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:09:09.483362378Z E1024 13:09:09.483287       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:09.682938495Z W1024 13:09:09.682879       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:09.682938495Z E1024 13:09:09.682920       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:10.058032828Z E1024 13:09:10.057976       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:09:10.094437079Z E1024 13:09:10.094378       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:10.482886033Z E1024 13:09:10.482346       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:09:10.482886033Z E1024 13:09:10.482378       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:09:10.486033343Z I1024 13:09:10.485957       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:10.486123723Z E1024 13:09:10.486080       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:10.493084894Z I1024 13:09:10.493014       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:10.680484020Z I1024 13:09:10.680435       1 request.go:700] Waited for 1.192747121s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:09:10.883451427Z E1024 13:09:10.883404       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:11.083490494Z W1024 13:09:11.083429       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:11.083490494Z E1024 13:09:11.083473       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:11.681261274Z I1024 13:09:11.681186       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:11.682693234Z W1024 13:09:11.682659       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:11.682693234Z W1024 13:09:11.682674       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:11.682693234Z W1024 13:09:11.682679       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:11.683475624Z E1024 13:09:11.683428       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:11.736052156Z E1024 13:09:11.736006       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:09:11.880330751Z I1024 13:09:11.880272       1 request.go:700] Waited for 1.397517738s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:09:11.883607741Z W1024 13:09:11.883533       1 base_controller.go:234] Updating status of "GuardController" failed: unable to ApplyStatus for operator using fieldManager "GuardController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=GuardController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:11.883607741Z E1024 13:09:11.883595       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:09:12.083463808Z E1024 13:09:12.083403       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:12.083463808Z I1024 13:09:12.083445       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:12.086677228Z I1024 13:09:12.086632       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:12.483499152Z E1024 13:09:12.483443       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:12.683130499Z W1024 13:09:12.683072       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:12.683130499Z E1024 13:09:12.683118       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:12.883877735Z E1024 13:09:12.883782       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:13.080094552Z I1024 13:09:13.080029       1 request.go:700] Waited for 1.598478094s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:09:13.480907936Z E1024 13:09:13.480857       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:09:13.480907936Z E1024 13:09:13.480881       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:09:13.483423836Z W1024 13:09:13.483372       1 base_controller.go:234] Updating status of "GuardController" failed: unable to ApplyStatus for operator using fieldManager "GuardController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=GuardController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:13.483459336Z E1024 13:09:13.483420       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:09:13.684113073Z E1024 13:09:13.684013       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:13.684113073Z I1024 13:09:13.684076       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:13.687382033Z I1024 13:09:13.687325       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:14.080695666Z I1024 13:09:14.080632       1 request.go:700] Waited for 1.595513944s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:09:14.083809716Z E1024 13:09:14.083729       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:14.282956294Z W1024 13:09:14.282899       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:14.282956294Z E1024 13:09:14.282938       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:14.695042038Z E1024 13:09:14.694996       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:15.083422611Z E1024 13:09:15.083335       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:15.083422611Z I1024 13:09:15.083373       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:15.086452681Z I1024 13:09:15.086391       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:15.280441358Z I1024 13:09:15.280353       1 request.go:700] Waited for 1.399322248s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2024-10-24T13:09:15.483013795Z E1024 13:09:15.482947       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:15.684163982Z W1024 13:09:15.684098       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:15.684163982Z E1024 13:09:15.684142       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:16.280441093Z I1024 13:09:16.280375       1 request.go:700] Waited for 1.398619159s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:09:16.484124270Z E1024 13:09:16.484040       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:16.484124270Z I1024 13:09:16.484102       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:16.487096740Z I1024 13:09:16.487038       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:16.883267044Z E1024 13:09:16.883221       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:17.083470381Z W1024 13:09:17.083403       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:17.083470381Z E1024 13:09:17.083458       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:17.280633817Z I1024 13:09:17.280556       1 request.go:700] Waited for 1.399635418s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:09:17.283584167Z E1024 13:09:17.283529       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:17.680833501Z I1024 13:09:17.680729       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:17.682408982Z W1024 13:09:17.682348       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:17.682408982Z W1024 13:09:17.682366       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:17.682408982Z W1024 13:09:17.682371       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:17.683299652Z E1024 13:09:17.683254       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:17.883309818Z E1024 13:09:17.883251       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:17.883355388Z I1024 13:09:17.883294       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:17.887693498Z I1024 13:09:17.887615       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:18.082349725Z E1024 13:09:18.082286       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:18.280716992Z I1024 13:09:18.280665       1 request.go:700] Waited for 1.395725328s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:09:18.284299482Z E1024 13:09:18.284205       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:18.483311639Z W1024 13:09:18.483256       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:18.483394739Z E1024 13:09:18.483314       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:18.895666593Z E1024 13:09:18.895598       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:19.284030397Z E1024 13:09:19.283963       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:19.284111817Z I1024 13:09:19.284017       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:19.480707193Z I1024 13:09:19.480651       1 request.go:700] Waited for 1.393511269s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:09:19.683047081Z E1024 13:09:19.682989       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:20.059705854Z E1024 13:09:20.059646       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:09:20.105332886Z E1024 13:09:20.105264       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:09:20.106051206Z E1024 13:09:20.105999       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:20.446808718Z I1024 13:09:20.446725       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:20.680678166Z I1024 13:09:20.680613       1 request.go:700] Waited for 1.034214877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:09:20.683865996Z W1024 13:09:20.683819       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:20.683897966Z E1024 13:09:20.683867       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:20.882951753Z E1024 13:09:20.882895       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:21.082506910Z E1024 13:09:21.082455       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:21.683743700Z E1024 13:09:21.683676       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:21.683743700Z I1024 13:09:21.683712       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:21.738025642Z E1024 13:09:21.737944       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:09:21.879936487Z I1024 13:09:21.879882       1 request.go:700] Waited for 1.399129678s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2024-10-24T13:09:22.083151644Z E1024 13:09:22.083091       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:22.497433599Z E1024 13:09:22.497367       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:22.681032085Z I1024 13:09:22.680959       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:22.682563695Z W1024 13:09:22.682523       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:22.682563695Z W1024 13:09:22.682544       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:22.682563695Z W1024 13:09:22.682549       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:22.683428506Z E1024 13:09:22.683384       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:23.083210049Z E1024 13:09:23.083146       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:24.084074944Z E1024 13:09:24.084001       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:24.283393021Z E1024 13:09:24.283325       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:24.883087622Z E1024 13:09:24.883027       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:25.083708919Z E1024 13:09:25.083642       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:25.502366064Z E1024 13:09:25.502298       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:26.481374598Z I1024 13:09:26.481279       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:26.483040107Z W1024 13:09:26.482994       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:26.483040107Z W1024 13:09:26.483016       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:26.483040107Z W1024 13:09:26.483023       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:26.483939818Z E1024 13:09:26.483897       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:26.883420581Z E1024 13:09:26.883358       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:27.696087750Z E1024 13:09:27.696024       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:29.483210631Z E1024 13:09:29.483142       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:29.681817148Z I1024 13:09:29.681727       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:29.683305218Z W1024 13:09:29.683257       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:29.683305218Z W1024 13:09:29.683275       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:29.683305218Z W1024 13:09:29.683282       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:29.684088139Z E1024 13:09:29.684040       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:29.882787566Z E1024 13:09:29.882725       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:30.061706031Z E1024 13:09:30.061652       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:09:30.083808542Z E1024 13:09:30.083717       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:30.295876279Z E1024 13:09:30.295790       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:31.740264189Z E1024 13:09:31.740198       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:09:31.883523765Z W1024 13:09:31.883448       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:31.883523765Z E1024 13:09:31.883507       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:31.928573266Z I1024 13:09:31.928504       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:32.690018783Z E1024 13:09:32.689945       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:32.883161879Z E1024 13:09:32.883092       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:32.883225909Z I1024 13:09:32.883138       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:33.093780596Z E1024 13:09:33.093714       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:33.281679973Z I1024 13:09:33.281604       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:33.283191043Z W1024 13:09:33.283150       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:33.283191043Z W1024 13:09:33.283167       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:33.283191043Z W1024 13:09:33.283172       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:33.283877473Z E1024 13:09:33.283835       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:33.683856866Z E1024 13:09:33.683791       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:35.082585055Z E1024 13:09:35.082525       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:35.295692912Z E1024 13:09:35.295627       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:36.283888546Z E1024 13:09:36.283830       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:36.481780533Z I1024 13:09:36.481659       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:36.483166033Z W1024 13:09:36.483120       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:36.483166033Z W1024 13:09:36.483142       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:36.483166033Z W1024 13:09:36.483148       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:36.483918743Z E1024 13:09:36.483883       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:37.497174378Z E1024 13:09:37.497111       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:38.681170249Z I1024 13:09:38.681065       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:38.682711829Z W1024 13:09:38.682652       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:38.682711829Z W1024 13:09:38.682676       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:38.682711829Z W1024 13:09:38.682684       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:38.683693639Z E1024 13:09:38.683651       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:39.082420803Z E1024 13:09:39.082351       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:39.697470484Z E1024 13:09:39.697400       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:40.063425437Z E1024 13:09:40.063360       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:09:40.883319315Z E1024 13:09:40.883267       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:41.282023278Z I1024 13:09:41.281950       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:41.283350349Z W1024 13:09:41.283306       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:41.283350349Z W1024 13:09:41.283329       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:41.283350349Z W1024 13:09:41.283333       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:41.284260529Z E1024 13:09:41.284181       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:41.741447334Z E1024 13:09:41.741391       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:09:42.282977612Z E1024 13:09:42.282925       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:42.881310613Z I1024 13:09:42.881239       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:42.882925344Z W1024 13:09:42.882886       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:42.882925344Z W1024 13:09:42.882909       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:42.882925344Z W1024 13:09:42.882913       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:42.883848303Z E1024 13:09:42.883801       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:43.283832026Z E1024 13:09:43.283740       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:44.282836560Z E1024 13:09:44.282764       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:45.281522973Z I1024 13:09:45.281451       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:45.283221494Z W1024 13:09:45.283168       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:45.283221494Z W1024 13:09:45.283189       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:45.283221494Z W1024 13:09:45.283197       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:45.283995813Z E1024 13:09:45.283954       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:46.083453591Z E1024 13:09:46.083376       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:46.107959182Z E1024 13:09:46.107878       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:09:46.108690662Z E1024 13:09:46.108633       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:47.083800425Z E1024 13:09:47.083728       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:47.882679533Z E1024 13:09:47.882617       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:49.082540154Z E1024 13:09:49.082482       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:49.963518954Z E1024 13:09:49.963462       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:09:50.064770557Z E1024 13:09:50.064698       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:09:50.883720816Z E1024 13:09:50.883660       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:51.082148133Z E1024 13:09:51.082098       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:51.743325785Z E1024 13:09:51.743258       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:09:52.369110167Z W1024 13:09:52.369038       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:52.369110167Z E1024 13:09:52.369092       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:53.084385160Z E1024 13:09:53.084316       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:53.367227640Z I1024 13:09:53.367158       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:09:53.370061160Z E1024 13:09:53.370014       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:53.370061160Z I1024 13:09:53.370043       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:55.529028555Z I1024 13:09:55.528955       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:09:55.530342025Z W1024 13:09:55.530302       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:09:55.530342025Z W1024 13:09:55.530324       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:09:55.530342025Z W1024 13:09:55.530331       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:09:55.530989045Z E1024 13:09:55.530949       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:09:57.329487842Z E1024 13:09:57.329427       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.057095827Z E1024 13:10:00.057027       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.064215008Z E1024 13:10:00.064144       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.066056498Z E1024 13:10:00.066017       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:10:00.077062158Z E1024 13:10:00.077007       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.099337718Z E1024 13:10:00.099283       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.141693169Z E1024 13:10:00.141639       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.162914200Z E1024 13:10:00.162849       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.224643901Z E1024 13:10:00.224566       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.387510245Z E1024 13:10:00.387446       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:00.710252122Z E1024 13:10:00.710180       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:01.260859854Z E1024 13:10:01.260794       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:01.272398855Z E1024 13:10:01.272335       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:01.289245295Z E1024 13:10:01.289176       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:01.315014266Z E1024 13:10:01.314955       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:01.658123013Z E1024 13:10:01.658050       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:01.744882636Z E1024 13:10:01.744737       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:10:01.861535859Z E1024 13:10:01.861469       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:02.260513539Z E1024 13:10:02.260457       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:02.660105999Z E1024 13:10:02.660060       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:03.058977148Z E1024 13:10:03.058904       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:03.068914518Z I1024 13:10:03.068847       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:10:03.256931182Z E1024 13:10:03.256868       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:03.460622707Z E1024 13:10:03.460560       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:03.661172011Z W1024 13:10:03.661100       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:10:03.661261902Z E1024 13:10:03.661166       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:04.256141706Z I1024 13:10:04.256077       1 request.go:700] Waited for 1.187194848s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:10:04.260116386Z E1024 13:10:04.260030       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:04.260116386Z I1024 13:10:04.260082       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:10:04.460178681Z E1024 13:10:04.460125       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:05.061324775Z E1024 13:10:05.061212       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:05.273011090Z E1024 13:10:05.272951       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:06.057806239Z E1024 13:10:06.057719       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:06.259628563Z E1024 13:10:06.259574       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:06.461203758Z E1024 13:10:06.461136       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:09.028710369Z E1024 13:10:09.028644       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:10.067338164Z E1024 13:10:10.067278       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:10:10.471536874Z E1024 13:10:10.471468       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:11.180256310Z E1024 13:10:11.180187       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:11.746436223Z E1024 13:10:11.746359       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:10:12.107876152Z E1024 13:10:12.107796       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:10:12.108465842Z E1024 13:10:12.108426       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:10:13.085302385Z E1024 13:10:13.085237       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:14.155335611Z E1024 13:10:14.155271       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:16.017383285Z I1024 13:10:16.017298       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 4 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:10:16.018808815Z W1024 13:10:16.018735       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:10:16.018808815Z W1024 13:10:16.018774       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:10:16.018808815Z W1024 13:10:16.018782       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:10:16.019707905Z E1024 13:10:16.019651       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:17.816564589Z E1024 13:10:17.816500       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:20.069014753Z E1024 13:10:20.068907       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:10:21.423783695Z E1024 13:10:21.423698       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:21.748017203Z E1024 13:10:21.747950       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:10:23.086075954Z E1024 13:10:23.085998       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:23.559616975Z E1024 13:10:23.559561       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:24.402271466Z E1024 13:10:24.402207       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:10:30.070652790Z E1024 13:10:30.070591       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.18016583582a8c09  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:InstallerPodFailed,Message:Failed to create installer pod for revision 4 count 0 on node \"ci-op-2fcpj5j6-f6035-2lklf-master-0\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-4-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,LastTimestamp:2024-10-24 13:09:06.681285641 +0000 UTC m=+247.010133238,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:10:31.749914500Z E1024 13:10:31.749852       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165831cad9fd6  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 5: Delete \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/encryption-config-5\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,LastTimestamp:2024-10-24 13:09:05.683242966 +0000 UTC m=+246.012090573,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:10:33.087402612Z E1024 13:10:33.087346       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:33.335685598Z W1024 13:10:33.335614       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:10:33.335685598Z E1024 13:10:33.335658       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:34.333381332Z I1024 13:10:34.333305       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:10:34.336461212Z E1024 13:10:34.336417       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:10:34.336461212Z I1024 13:10:34.336447       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 5: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:11:03.069909504Z I1024 13:11:03.069832       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:11:03.464095298Z I1024 13:11:03.464009       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:11:04.066084745Z I1024 13:11:04.065998       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:11:04.263258057Z I1024 13:11:04.263138       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-5 -n openshift-kube-apiserver because it was missing
2024-10-24T13:11:04.462111169Z I1024 13:11:04.462001       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:11:04.464073599Z W1024 13:11:04.464025       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:04.464073599Z W1024 13:11:04.464045       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:06.050227646Z I1024 13:11:06.050167       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2024-10-24T13:11:14.310641222Z I1024 13:11:14.310578       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:16.080016101Z I1024 13:11:16.079920       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0" (last termination at 2024-10-24 13:06:48 +0000 UTC) at 2024-10-24 13:09:30 +0000 UTC
2024-10-24T13:11:16.082041701Z I1024 13:11:16.081973       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0" (last termination at 2024-10-24 13:06:48 +0000 UTC) at 2024-10-24 13:11:15 +0000 UTC
2024-10-24T13:11:18.620874348Z I1024 13:11:18.620812       1 reflector.go:368] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:19.113988183Z I1024 13:11:19.113926       1 reflector.go:368] Caches populated for *v1.Image from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:21.799868331Z I1024 13:11:21.799805       1 reflector.go:368] Caches populated for monitoring.coreos.com/v1, Resource=prometheusrules from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:26.102740637Z I1024 13:11:26.102661       1 reflector.go:368] Caches populated for *v1.APIServer from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:26.492105091Z I1024 13:11:26.492042       1 reflector.go:368] Caches populated for *v1alpha1.StorageVersionMigration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:29.570352834Z I1024 13:11:29.570285       1 reflector.go:368] Caches populated for *v1.OAuth from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:32.117178680Z I1024 13:11:32.117115       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:32.756820157Z I1024 13:11:32.756743       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:32.763580467Z I1024 13:11:32.762625       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:32.766349617Z I1024 13:11:32.765584       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:32.778004947Z I1024 13:11:32.769860       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:11:32.778113667Z I1024 13:11:32.777995       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:32.793969847Z I1024 13:11:32.793908       1 revision_controller.go:274] down the branch indicating that our cache was out of date and we're trying to recreate a revision.
2024-10-24T13:11:32.798842067Z W1024 13:11:32.798799       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:32.798928117Z W1024 13:11:32.798910       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:32.831237588Z W1024 13:11:32.831180       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:32.831349047Z W1024 13:11:32.831325       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:32.871885268Z W1024 13:11:32.871808       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:32.871885268Z W1024 13:11:32.871840       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:32.921784648Z W1024 13:11:32.921698       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:32.921784648Z W1024 13:11:32.921728       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:32.991786479Z W1024 13:11:32.991691       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:32.991878929Z W1024 13:11:32.991863       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:33.100896321Z W1024 13:11:33.100821       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:33.100896321Z W1024 13:11:33.100854       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:33.290357152Z W1024 13:11:33.290286       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:33.290357152Z W1024 13:11:33.290334       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:33.643998046Z W1024 13:11:33.643938       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:33.644128826Z W1024 13:11:33.644101       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:34.316780894Z W1024 13:11:34.316709       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:34.316780894Z W1024 13:11:34.316736       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:34.410049504Z I1024 13:11:34.409984       1 reflector.go:368] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:34.798416198Z I1024 13:11:34.798346       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:35.203341253Z I1024 13:11:35.203262       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because waiting for static pod of revision 4, found 3
2024-10-24T13:11:35.221915343Z I1024 13:11:35.221850       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:35.629563997Z W1024 13:11:35.629495       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:35.629563997Z W1024 13:11:35.629520       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:36.485429026Z I1024 13:11:36.485361       1 reflector.go:368] Caches populated for *v1.Infrastructure from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:36.485711056Z I1024 13:11:36.485659       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX
2024-10-24T13:11:36.486017537Z I1024 13:11:36.485938       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:37.800387751Z I1024 13:11:37.800328       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:37.803529141Z I1024 13:11:37.803464       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:37.809872971Z I1024 13:11:37.809810       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:37.809872971Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:37.809872971Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:37.809872971Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:37.809872971Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:37.809872971Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:37.809872971Z   		"etcd-servers": []any{
2024-10-24T13:11:37.809872971Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:37.809872971Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:37.809872971Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:37.809872971Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:37.809872971Z   			string("https://localhost:2379"),
2024-10-24T13:11:37.809872971Z   		},
2024-10-24T13:11:37.809872971Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:37.809872971Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:37.809872971Z   		... // 3 identical entries
2024-10-24T13:11:37.809872971Z   	},
2024-10-24T13:11:37.809872971Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:37.809872971Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:37.809872971Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:37.809872971Z   }
2024-10-24T13:11:37.881542341Z I1024 13:11:37.881479       1 reflector.go:368] Caches populated for *v1.Network from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:37.881734721Z I1024 13:11:37.881672       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2024-10-24T13:11:38.215660255Z W1024 13:11:38.215602       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:38.215660255Z W1024 13:11:38.215629       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:38.224593065Z I1024 13:11:38.224506       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:38.255315576Z E1024 13:11:38.255258       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:38.260841516Z I1024 13:11:38.260665       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:38.262305665Z I1024 13:11:38.262197       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:38.262305665Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:38.262305665Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:38.262305665Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:38.262305665Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:38.262305665Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:38.262305665Z   		"etcd-servers": []any{
2024-10-24T13:11:38.262305665Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:38.262305665Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:38.262305665Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:38.262305665Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:38.262305665Z   			string("https://localhost:2379"),
2024-10-24T13:11:38.262305665Z   		},
2024-10-24T13:11:38.262305665Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:38.262305665Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:38.262305665Z   		... // 3 identical entries
2024-10-24T13:11:38.262305665Z   	},
2024-10-24T13:11:38.262305665Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:38.262305665Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:38.262305665Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:38.262305665Z   }
2024-10-24T13:11:38.665343150Z I1024 13:11:38.665255       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:38.691213250Z E1024 13:11:38.691146       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:38.696192710Z I1024 13:11:38.696104       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:38.696995131Z I1024 13:11:38.696940       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:38.696995131Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:38.696995131Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:38.696995131Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:38.696995131Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:38.696995131Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:38.696995131Z   		"etcd-servers": []any{
2024-10-24T13:11:38.696995131Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:38.696995131Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:38.696995131Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:38.696995131Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:38.696995131Z   			string("https://localhost:2379"),
2024-10-24T13:11:38.696995131Z   		},
2024-10-24T13:11:38.696995131Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:38.696995131Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:38.696995131Z   		... // 3 identical entries
2024-10-24T13:11:38.696995131Z   	},
2024-10-24T13:11:38.696995131Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:38.696995131Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:38.696995131Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:38.696995131Z   }
2024-10-24T13:11:38.801355942Z I1024 13:11:38.801268       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because waiting for static pod of revision 4, found 3
2024-10-24T13:11:39.093724865Z I1024 13:11:39.093630       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:39.124704175Z E1024 13:11:39.124659       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:39.129267335Z I1024 13:11:39.129215       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:39.130570725Z I1024 13:11:39.130515       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:39.130570725Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:39.130570725Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:39.130570725Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:39.130570725Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:39.130570725Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:39.130570725Z   		"etcd-servers": []any{
2024-10-24T13:11:39.130570725Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:39.130570725Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:39.130570725Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:39.130570725Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:39.130570725Z   			string("https://localhost:2379"),
2024-10-24T13:11:39.130570725Z   		},
2024-10-24T13:11:39.130570725Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:39.130570725Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:39.130570725Z   		... // 3 identical entries
2024-10-24T13:11:39.130570725Z   	},
2024-10-24T13:11:39.130570725Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:39.130570725Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:39.130570725Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:39.130570725Z   }
2024-10-24T13:11:39.912936273Z I1024 13:11:39.912858       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:40.056808905Z E1024 13:11:40.056731       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:40.058831975Z I1024 13:11:40.058724       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:40.062782915Z I1024 13:11:40.062705       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:40.062782915Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:40.062782915Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:40.062782915Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:40.062782915Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:40.062782915Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:40.062782915Z   		"etcd-servers": []any{
2024-10-24T13:11:40.062782915Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:40.062782915Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:40.062782915Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:40.062782915Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:40.062782915Z   			string("https://localhost:2379"),
2024-10-24T13:11:40.062782915Z   		},
2024-10-24T13:11:40.062782915Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:40.062782915Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:40.062782915Z   		... // 3 identical entries
2024-10-24T13:11:40.062782915Z   	},
2024-10-24T13:11:40.062782915Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:40.062782915Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:40.062782915Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:40.062782915Z   }
2024-10-24T13:11:40.311362997Z I1024 13:11:40.311291       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:40.400939368Z W1024 13:11:40.400868       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:40.965437034Z I1024 13:11:40.965321       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:41.046057695Z E1024 13:11:41.045987       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:41.047942355Z I1024 13:11:41.047856       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:41.054279745Z I1024 13:11:41.054188       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:41.054279745Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:41.054279745Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:41.054279745Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:41.054279745Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:41.054279745Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:41.054279745Z   		"etcd-servers": []any{
2024-10-24T13:11:41.054279745Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:41.054279745Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:41.054279745Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:41.054279745Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:41.054279745Z   			string("https://localhost:2379"),
2024-10-24T13:11:41.054279745Z   		},
2024-10-24T13:11:41.054279745Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:41.054279745Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:41.054279745Z   		... // 3 identical entries
2024-10-24T13:11:41.054279745Z   	},
2024-10-24T13:11:41.054279745Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:41.054279745Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:41.054279745Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:41.054279745Z   }
2024-10-24T13:11:41.404168879Z W1024 13:11:41.404062       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:41.820123703Z I1024 13:11:41.818706       1 reflector.go:368] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:41.913926774Z I1024 13:11:41.913850       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:42.037915946Z E1024 13:11:42.037854       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:42.043167466Z I1024 13:11:42.043097       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:42.046870206Z I1024 13:11:42.046803       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:42.046870206Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:42.046870206Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:42.046870206Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:42.046870206Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:42.046870206Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:42.046870206Z   		"etcd-servers": []any{
2024-10-24T13:11:42.046870206Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:42.046870206Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:42.046870206Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:42.046870206Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:42.046870206Z   			string("https://localhost:2379"),
2024-10-24T13:11:42.046870206Z   		},
2024-10-24T13:11:42.046870206Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:42.046870206Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:42.046870206Z   		... // 3 identical entries
2024-10-24T13:11:42.046870206Z   	},
2024-10-24T13:11:42.046870206Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:42.046870206Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:42.046870206Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:42.046870206Z   }
2024-10-24T13:11:42.139306716Z I1024 13:11:42.139240       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:42.798796374Z I1024 13:11:42.796830       1 reflector.go:368] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:42.916410625Z I1024 13:11:42.916321       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:43.041149096Z E1024 13:11:43.041081       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:43.043090646Z I1024 13:11:43.043023       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:43.048391006Z I1024 13:11:43.048307       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:43.048391006Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:43.048391006Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:43.048391006Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:43.048391006Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:43.048391006Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:43.048391006Z   		"etcd-servers": []any{
2024-10-24T13:11:43.048391006Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:43.048391006Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:43.048391006Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:43.048391006Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:43.048391006Z   			string("https://localhost:2379"),
2024-10-24T13:11:43.048391006Z   		},
2024-10-24T13:11:43.048391006Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:43.048391006Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:43.048391006Z   		... // 3 identical entries
2024-10-24T13:11:43.048391006Z   	},
2024-10-24T13:11:43.048391006Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:43.048391006Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:43.048391006Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:43.048391006Z   }
2024-10-24T13:11:43.364153979Z W1024 13:11:43.364089       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:43.364153979Z W1024 13:11:43.364114       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:43.478002111Z W1024 13:11:43.477942       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:44.126382097Z I1024 13:11:44.126291       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:44.246294628Z E1024 13:11:44.244102       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:44.267383949Z I1024 13:11:44.267336       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:44.267487329Z I1024 13:11:44.267462       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:44.267487329Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:44.267487329Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:44.267487329Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:44.267487329Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:44.267487329Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:44.267487329Z   		"etcd-servers": []any{
2024-10-24T13:11:44.267487329Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:44.267487329Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:44.267487329Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:44.267487329Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:44.267487329Z   			string("https://localhost:2379"),
2024-10-24T13:11:44.267487329Z   		},
2024-10-24T13:11:44.267487329Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:44.267487329Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:44.267487329Z   		... // 3 identical entries
2024-10-24T13:11:44.267487329Z   	},
2024-10-24T13:11:44.267487329Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:44.267487329Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:44.267487329Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:44.267487329Z   }
2024-10-24T13:11:44.481874831Z W1024 13:11:44.481818       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:44.881234365Z I1024 13:11:44.881164       1 reflector.go:368] Caches populated for *v1.Authentication from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:44.997099846Z I1024 13:11:44.997044       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:45.120459878Z I1024 13:11:45.120383       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:45.241430799Z E1024 13:11:45.241374       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:45.243873349Z I1024 13:11:45.243788       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:45.248042780Z I1024 13:11:45.247980       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:45.248042780Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:45.248042780Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:45.248042780Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:45.248042780Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:45.248042780Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:45.248042780Z   		"etcd-servers": []any{
2024-10-24T13:11:45.248042780Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:45.248042780Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:45.248042780Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:45.248042780Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:45.248042780Z   			string("https://localhost:2379"),
2024-10-24T13:11:45.248042780Z   		},
2024-10-24T13:11:45.248042780Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:45.248042780Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:45.248042780Z   		... // 3 identical entries
2024-10-24T13:11:45.248042780Z   	},
2024-10-24T13:11:45.248042780Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:45.248042780Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:45.248042780Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:45.248042780Z   }
2024-10-24T13:11:45.610220153Z I1024 13:11:45.610146       1 reflector.go:368] Caches populated for *v1.Proxy from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:45.797541835Z I1024 13:11:45.797476       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:46.107675358Z I1024 13:11:46.107575       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:46.197119739Z I1024 13:11:46.196978       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:46.199697029Z I1024 13:11:46.199653       1 reflector.go:368] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:46.239827010Z E1024 13:11:46.239742       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:46.245046880Z I1024 13:11:46.244985       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:46.249332960Z I1024 13:11:46.249271       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:46.249332960Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:46.249332960Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:46.249332960Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:46.249332960Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:46.249332960Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:46.249332960Z   		"etcd-servers": []any{
2024-10-24T13:11:46.249332960Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:46.249332960Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:46.249332960Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:46.249332960Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:46.249332960Z   			string("https://localhost:2379"),
2024-10-24T13:11:46.249332960Z   		},
2024-10-24T13:11:46.249332960Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:46.249332960Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:46.249332960Z   		... // 3 identical entries
2024-10-24T13:11:46.249332960Z   	},
2024-10-24T13:11:46.249332960Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:46.249332960Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:46.249332960Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:46.249332960Z   }
2024-10-24T13:11:47.146228339Z I1024 13:11:47.146119       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:47.197359420Z I1024 13:11:47.197283       1 reflector.go:368] Caches populated for *v1.Pod from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:47.197910120Z E1024 13:11:47.197874       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:11:47.237703940Z E1024 13:11:47.237640       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:47.239438160Z I1024 13:11:47.239368       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:47.240956330Z I1024 13:11:47.240909       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:47.240956330Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:47.240956330Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:47.240956330Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:47.240956330Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:47.240956330Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:47.240956330Z   		"etcd-servers": []any{
2024-10-24T13:11:47.240956330Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:47.240956330Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:47.240956330Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:47.240956330Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:47.240956330Z   			string("https://localhost:2379"),
2024-10-24T13:11:47.240956330Z   		},
2024-10-24T13:11:47.240956330Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:47.240956330Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:47.240956330Z   		... // 3 identical entries
2024-10-24T13:11:47.240956330Z   	},
2024-10-24T13:11:47.240956330Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:47.240956330Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:47.240956330Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:47.240956330Z   }
2024-10-24T13:11:47.600547284Z I1024 13:11:47.600471       1 termination_observer.go:130] Observed termination of API server pod "kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0" at 2024-10-24 13:10:20 +0000 UTC
2024-10-24T13:11:48.117539529Z I1024 13:11:48.117450       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:48.240126011Z E1024 13:11:48.240057       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:48.345192452Z I1024 13:11:48.344722       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:48.355526602Z I1024 13:11:48.354159       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:48.355526602Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:48.355526602Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:48.355526602Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:48.355526602Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:48.355526602Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:48.355526602Z   		"etcd-servers": []any{
2024-10-24T13:11:48.355526602Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:48.355526602Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:48.355526602Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:48.355526602Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:48.355526602Z   			string("https://localhost:2379"),
2024-10-24T13:11:48.355526602Z   		},
2024-10-24T13:11:48.355526602Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:48.355526602Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:48.355526602Z   		... // 3 identical entries
2024-10-24T13:11:48.355526602Z   	},
2024-10-24T13:11:48.355526602Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:48.355526602Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:48.355526602Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:48.355526602Z   }
2024-10-24T13:11:48.394242292Z I1024 13:11:48.394182       1 request.go:700] Waited for 1.195517882s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2024-10-24T13:11:49.000508189Z E1024 13:11:48.999569       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:11:49.001714259Z E1024 13:11:49.001682       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:11:49.121197140Z I1024 13:11:49.121097       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:49.235244561Z E1024 13:11:49.235167       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:49.397949643Z I1024 13:11:49.395833       1 request.go:700] Waited for 1.194356803s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:11:50.002509649Z I1024 13:11:50.002446       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:50.349099093Z I1024 13:11:50.349009       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:50.350607903Z I1024 13:11:50.350566       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:50.350607903Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:50.350607903Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:50.350607903Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:50.350607903Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:50.350607903Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:50.350607903Z   		"etcd-servers": []any{
2024-10-24T13:11:50.350607903Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:50.350607903Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:50.350607903Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:50.350607903Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:50.350607903Z   			string("https://localhost:2379"),
2024-10-24T13:11:50.350607903Z   		},
2024-10-24T13:11:50.350607903Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:50.350607903Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:50.350607903Z   		... // 3 identical entries
2024-10-24T13:11:50.350607903Z   	},
2024-10-24T13:11:50.350607903Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:50.350607903Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:50.350607903Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:50.350607903Z   }
2024-10-24T13:11:50.398725253Z I1024 13:11:50.398655       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:50.594210556Z I1024 13:11:50.594146       1 request.go:700] Waited for 1.395213625s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:11:50.757201967Z I1024 13:11:50.757102       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:50.795797668Z E1024 13:11:50.795225       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:51.200115452Z E1024 13:11:51.200067       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:11:51.200115452Z E1024 13:11:51.200099       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:11:51.201700892Z E1024 13:11:51.201656       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:11:51.343491664Z I1024 13:11:51.342957       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:51.355052114Z I1024 13:11:51.354957       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:51.355052114Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:51.355052114Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:51.355052114Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:51.355052114Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:51.355052114Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:51.355052114Z   		"etcd-servers": []any{
2024-10-24T13:11:51.355052114Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:51.355052114Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:51.355052114Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:51.355052114Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:51.355052114Z   			string("https://localhost:2379"),
2024-10-24T13:11:51.355052114Z   		},
2024-10-24T13:11:51.355052114Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:51.355052114Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:51.355052114Z   		... // 3 identical entries
2024-10-24T13:11:51.355052114Z   	},
2024-10-24T13:11:51.355052114Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:51.355052114Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:51.355052114Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:51.355052114Z   }
2024-10-24T13:11:51.754207728Z I1024 13:11:51.754082       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:51.785797948Z E1024 13:11:51.785188       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:51.819608599Z I1024 13:11:51.819556       1 reflector.go:368] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:52.362981374Z I1024 13:11:52.362850       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:52.371610464Z I1024 13:11:52.371537       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:52.371610464Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:52.371610464Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:52.371610464Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:52.371610464Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:52.371610464Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:52.371610464Z   		"etcd-servers": []any{
2024-10-24T13:11:52.371610464Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:52.371610464Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:52.371610464Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:52.371610464Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:52.371610464Z   			string("https://localhost:2379"),
2024-10-24T13:11:52.371610464Z   		},
2024-10-24T13:11:52.371610464Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:52.371610464Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:52.371610464Z   		... // 3 identical entries
2024-10-24T13:11:52.371610464Z   	},
2024-10-24T13:11:52.371610464Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:52.371610464Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:52.371610464Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:52.371610464Z   }
2024-10-24T13:11:52.788231779Z I1024 13:11:52.788157       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:52.818365469Z E1024 13:11:52.818309       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:53.001614871Z I1024 13:11:53.001557       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because waiting for static pod of revision 4, found 3
2024-10-24T13:11:53.144141633Z I1024 13:11:53.144086       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:53.152334402Z I1024 13:11:53.152276       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:53.152334402Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:53.152334402Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:53.152334402Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:53.152334402Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:53.152334402Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:53.152334402Z   		"etcd-servers": []any{
2024-10-24T13:11:53.152334402Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:53.152334402Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:53.152334402Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:53.152334402Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:53.152334402Z   			string("https://localhost:2379"),
2024-10-24T13:11:53.152334402Z   		},
2024-10-24T13:11:53.152334402Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:53.152334402Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:53.152334402Z   		... // 3 identical entries
2024-10-24T13:11:53.152334402Z   	},
2024-10-24T13:11:53.152334402Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:53.152334402Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:53.152334402Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:53.152334402Z   }
2024-10-24T13:11:53.585446368Z I1024 13:11:53.585367       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:53.617209908Z E1024 13:11:53.617154       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:53.880822221Z W1024 13:11:53.880760       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:53.880822221Z W1024 13:11:53.880791       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:54.405688086Z I1024 13:11:54.405629       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:54.546185327Z I1024 13:11:54.546080       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:54.551200448Z I1024 13:11:54.551118       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:54.551200448Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:54.551200448Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:54.551200448Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:54.551200448Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:54.551200448Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:54.551200448Z   		"etcd-servers": []any{
2024-10-24T13:11:54.551200448Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:54.551200448Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:54.551200448Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:54.551200448Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:54.551200448Z   			string("https://localhost:2379"),
2024-10-24T13:11:54.551200448Z   		},
2024-10-24T13:11:54.551200448Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:54.551200448Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:54.551200448Z   		... // 3 identical entries
2024-10-24T13:11:54.551200448Z   	},
2024-10-24T13:11:54.551200448Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:54.551200448Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:54.551200448Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:54.551200448Z   }
2024-10-24T13:11:54.762499639Z I1024 13:11:54.762421       1 reflector.go:368] Caches populated for *v1.ClusterRole from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:54.918630871Z I1024 13:11:54.918573       1 reflector.go:368] Caches populated for *v1.ValidatingWebhookConfiguration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:54.975168172Z I1024 13:11:54.975098       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:55.007566652Z E1024 13:11:55.007466       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:55.014796272Z W1024 13:11:55.014686       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:55.258244845Z I1024 13:11:55.258200       1 reflector.go:368] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:55.546288898Z I1024 13:11:55.546222       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:55.555667828Z I1024 13:11:55.554791       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:55.555667828Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:55.555667828Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:55.555667828Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:55.555667828Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:55.555667828Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:55.555667828Z   		"etcd-servers": []any{
2024-10-24T13:11:55.555667828Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:55.555667828Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:55.555667828Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:55.555667828Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:55.555667828Z   			string("https://localhost:2379"),
2024-10-24T13:11:55.555667828Z   		},
2024-10-24T13:11:55.555667828Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:55.555667828Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:55.555667828Z   		... // 3 identical entries
2024-10-24T13:11:55.555667828Z   	},
2024-10-24T13:11:55.555667828Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:55.555667828Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:55.555667828Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:55.555667828Z   }
2024-10-24T13:11:55.797092231Z I1024 13:11:55.797014       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:55.961427513Z I1024 13:11:55.961356       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ObservedConfigWriteError' Failed to write observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2024-10-24T13:11:55.991961743Z E1024 13:11:55.991890       1 base_controller.go:271] "Unhandled Error" err="ConfigObserver reconciliation failed: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:11:56.016881993Z I1024 13:11:56.016813       1 reflector.go:368] Caches populated for operator.openshift.io/v1, Resource=kubeapiservers from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:56.017721343Z I1024 13:11:56.017676       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:56.019583143Z I1024 13:11:56.019487       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 6 triggered by "secret \"localhost-recovery-serving-certkey-5\" not found"
2024-10-24T13:11:56.022382183Z I1024 13:11:56.022290       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"ConfigObservationDegraded: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again\nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"ConfigObservation_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:11:56.022809413Z I1024 13:11:56.022766       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.022843203Z W1024 13:11:56.022822       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:56.024367283Z I1024 13:11:56.024328       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObserveStorageUpdated' Updated storage urls to https://10.0.0.3:2379,https://10.0.0.4:2379,https://10.0.0.5:2379,https://10.0.0.6:2379,https://localhost:2379
2024-10-24T13:11:56.024438983Z I1024 13:11:56.024415       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ObservedConfigChanged' Writing updated observed config:   map[string]any{
2024-10-24T13:11:56.024438983Z   	"admission": map[string]any{"pluginConfig": map[string]any{"PodSecurity": map[string]any{"configuration": map[string]any{"defaults": map[string]any{"audit": string("restricted"), "audit-version": string("latest"), "enforce": string("restricted"), "enforce-version": string("latest"), ...}}}, "network.openshift.io/ExternalIPRanger": map[string]any{"configuration": map[string]any{"allowIngressIP": bool(false), "apiVersion": string("network.openshift.io/v1"), "kind": string("ExternalIPRangerAdmissionConfig")}}, "network.openshift.io/RestrictedEndpointsAdmission": map[string]any{"configuration": map[string]any{"apiVersion": string("network.openshift.io/v1"), "kind": string("RestrictedEndpointsAdmissionConfig"), "restrictedCIDRs": []any{string("10.128.0.0/14"), string("172.30.0.0/16")}}}}},
2024-10-24T13:11:56.024438983Z   	"apiServerArguments": map[string]any{
2024-10-24T13:11:56.024438983Z   		"api-audiences":                            []any{string("https://kubernetes.default.svc")},
2024-10-24T13:11:56.024438983Z   		"authentication-token-webhook-config-file": []any{string("/etc/kubernetes/static-pod-resources/secrets/webhook-authenticat"...)},
2024-10-24T13:11:56.024438983Z   		"authentication-token-webhook-version":     []any{string("v1")},
2024-10-24T13:11:56.024438983Z   		"etcd-servers": []any{
2024-10-24T13:11:56.024438983Z   			string("https://10.0.0.3:2379"),
2024-10-24T13:11:56.024438983Z + 			string("https://10.0.0.4:2379"),
2024-10-24T13:11:56.024438983Z   			string("https://10.0.0.5:2379"),
2024-10-24T13:11:56.024438983Z   			string("https://10.0.0.6:2379"),
2024-10-24T13:11:56.024438983Z   			string("https://localhost:2379"),
2024-10-24T13:11:56.024438983Z   		},
2024-10-24T13:11:56.024438983Z   		"feature-gates":  []any{string("AWSEFSDriverVolumeMetrics=true"), string("AdminNetworkPolicy=true"), string("AlibabaPlatform=true"), string("AzureWorkloadIdentity=true"), ...},
2024-10-24T13:11:56.024438983Z   		"runtime-config": []any{string("admissionregistration.k8s.io/v1beta1=true")},
2024-10-24T13:11:56.024438983Z   		... // 3 identical entries
2024-10-24T13:11:56.024438983Z   	},
2024-10-24T13:11:56.024438983Z   	"corsAllowedOrigins": []any{string(`//127\.0\.0\.1(:|$)`), string("//localhost(:|$)")},
2024-10-24T13:11:56.024438983Z   	"servicesSubnet":     string("172.30.0.0/16"),
2024-10-24T13:11:56.024438983Z   	"servingInfo":        map[string]any{"bindAddress": string("0.0.0.0:6443"), "bindNetwork": string("tcp4"), "cipherSuites": []any{string("TLS_AES_128_GCM_SHA256"), string("TLS_AES_256_GCM_SHA384"), string("TLS_CHACHA20_POLY1305_SHA256"), string("TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"), ...}, "minTLSVersion": string("VersionTLS12"), ...},
2024-10-24T13:11:56.024438983Z   }
2024-10-24T13:11:56.033645393Z I1024 13:11:56.033502       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]" to "ConfigObservationDegraded: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again\nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:11:56.052374503Z W1024 13:11:56.052321       1 dynamic_operator_client.go:352] .status.conditions["ConfigObservationDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:56.052374503Z W1024 13:11:56.052342       1 dynamic_operator_client.go:355] .status.conditions["ConfigObservationDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:56.054550063Z I1024 13:11:56.054506       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.056021023Z I1024 13:11:56.055989       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:56.082616174Z I1024 13:11:56.082561       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.085211984Z I1024 13:11:56.085172       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:56.086311354Z I1024 13:11:56.086278       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:11:56.098253874Z I1024 13:11:56.098190       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "ConfigObservationDegraded: error writing updated observed config: Operation cannot be fulfilled on kubeapiservers.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again\nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:11:56.300198616Z I1024 13:11:56.300142       1 reflector.go:368] Caches populated for *v1.CustomResourceDefinition from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:56.300744396Z I1024 13:11:56.300671       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.301083786Z I1024 13:11:56.301053       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.301441416Z I1024 13:11:56.301390       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.301712256Z I1024 13:11:56.301669       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.403328217Z I1024 13:11:56.403264       1 installer_controller.go:525] "ci-op-2fcpj5j6-f6035-2lklf-master-0" is in transition to 4, but has not made progress because waiting for static pod of revision 4, found 3
2024-10-24T13:11:56.405726537Z I1024 13:11:56.405654       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.406998837Z W1024 13:11:56.406976       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:11:56.406998837Z W1024 13:11:56.406989       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:11:56.406998837Z W1024 13:11:56.406994       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:11:56.436199567Z E1024 13:11:56.436143       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.437103117Z I1024 13:11:56.437071       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.437525128Z I1024 13:11:56.437484       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:56.442410717Z I1024 13:11:56.442368       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nInstallerControllerDegraded: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:11:56.442554657Z I1024 13:11:56.442518       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.443468738Z E1024 13:11:56.443396       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.446219368Z I1024 13:11:56.446191       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.449427678Z E1024 13:11:56.449391       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.457894598Z I1024 13:11:56.457862       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.460159178Z E1024 13:11:56.460026       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.460949568Z I1024 13:11:56.460867       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nInstallerControllerDegraded: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5",Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5"
2024-10-24T13:11:56.502055428Z I1024 13:11:56.501974       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.503619248Z E1024 13:11:56.503572       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.585526179Z I1024 13:11:56.585439       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.587173559Z E1024 13:11:56.587142       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.605220789Z I1024 13:11:56.605161       1 reflector.go:368] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:56.605587319Z I1024 13:11:56.605548       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:56.749904801Z I1024 13:11:56.749838       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:56.751729291Z E1024 13:11:56.751678       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.814833412Z I1024 13:11:56.814249       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:56.822981011Z E1024 13:11:56.822917       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:56.829501911Z I1024 13:11:56.823304       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:57.073916584Z I1024 13:11:57.073722       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:57.075255834Z E1024 13:11:57.075203       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:57.194868275Z I1024 13:11:57.194816       1 request.go:700] Waited for 1.171417663s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:11:58.101807155Z W1024 13:11:58.101717       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:58.205431386Z I1024 13:11:58.205007       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:58.205431386Z I1024 13:11:58.205408       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:58.205957406Z I1024 13:11:58.205845       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:11:58.206339676Z E1024 13:11:58.206298       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:58.358073168Z I1024 13:11:58.357998       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:58.360687388Z E1024 13:11:58.360620       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:11:58.394628398Z I1024 13:11:58.394575       1 request.go:700] Waited for 1.387175464s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:11:59.105492626Z W1024 13:11:59.105412       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:11:59.197056027Z I1024 13:11:59.196996       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:11:59.197480697Z I1024 13:11:59.197451       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:11:59.395080949Z I1024 13:11:59.395028       1 request.go:700] Waited for 1.592502967s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:11:59.805191364Z I1024 13:11:59.805092       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:11:59.810292323Z I1024 13:11:59.810258       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:11:59.811159584Z I1024 13:11:59.811116       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:11:59.813495284Z E1024 13:11:59.813425       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:00.594438502Z I1024 13:12:00.594371       1 request.go:700] Waited for 1.118445162s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:12:01.009306216Z I1024 13:12:01.009199       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:01.013862276Z I1024 13:12:01.010585       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:01.013862276Z E1024 13:12:01.013265       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:01.013862276Z I1024 13:12:01.013684       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:01.793945745Z I1024 13:12:01.793859       1 request.go:700] Waited for 1.390460415s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2024-10-24T13:12:02.403895151Z I1024 13:12:02.403837       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:02.404008851Z I1024 13:12:02.403950       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:02.404965501Z I1024 13:12:02.404934       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:02.406474141Z E1024 13:12:02.406396       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:02.597899673Z I1024 13:12:02.597835       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:02.794979926Z I1024 13:12:02.794903       1 request.go:700] Waited for 1.592258737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:12:03.483604552Z I1024 13:12:03.483484       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:03.485393492Z E1024 13:12:03.485343       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:03.994555148Z I1024 13:12:03.994467       1 request.go:700] Waited for 1.590698227s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:12:04.003007878Z I1024 13:12:04.002958       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:04.003115318Z I1024 13:12:04.003084       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:04.004037178Z I1024 13:12:04.004009       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:04.006612998Z E1024 13:12:04.006299       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:04.335597042Z I1024 13:12:04.335530       1 reflector.go:368] Caches populated for *v1.SecurityContextConstraints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:04.801804826Z I1024 13:12:04.801606       1 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:05.082091629Z I1024 13:12:05.082018       1 reflector.go:368] Caches populated for *v1.MutatingWebhookConfiguration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:05.152242380Z W1024 13:12:05.152165       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:12:05.194391071Z I1024 13:12:05.194330       1 request.go:700] Waited for 1.528379017s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:12:05.605299805Z I1024 13:12:05.605250       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:05.605847905Z I1024 13:12:05.605815       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:05.607031925Z I1024 13:12:05.606979       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:05.607564175Z E1024 13:12:05.607507       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:05.808132307Z E1024 13:12:05.807893       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:05.808132307Z E1024 13:12:05.807928       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:05.810054227Z E1024 13:12:05.810010       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:12:06.001051129Z I1024 13:12:06.000954       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:06.003999799Z E1024 13:12:06.003965       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.004305959Z I1024 13:12:06.004275       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.008530029Z E1024 13:12:06.008486       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.008530029Z I1024 13:12:06.008510       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.018241099Z E1024 13:12:06.018156       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.019012929Z I1024 13:12:06.018966       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.025328719Z I1024 13:12:06.025271       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.027020109Z E1024 13:12:06.026982       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.030352479Z I1024 13:12:06.030309       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.032925699Z E1024 13:12:06.032886       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.037007619Z E1024 13:12:06.036970       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.037038409Z I1024 13:12:06.037003       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.040679619Z E1024 13:12:06.040629       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.040679619Z I1024 13:12:06.040655       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.044503589Z E1024 13:12:06.044448       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.044503589Z I1024 13:12:06.044476       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.048210589Z E1024 13:12:06.048173       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.048210589Z I1024 13:12:06.048200       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.051924729Z E1024 13:12:06.051885       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.051924729Z I1024 13:12:06.051912       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.055727759Z E1024 13:12:06.055687       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.055768809Z I1024 13:12:06.055718       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.059508930Z W1024 13:12:06.059468       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:06.059508930Z W1024 13:12:06.059492       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:06.059508930Z W1024 13:12:06.059499       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:06.059823869Z I1024 13:12:06.059741       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: localhost-recovery-serving-certkey-5
2024-10-24T13:12:06.091769430Z E1024 13:12:06.091698       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: missing required resources: secrets: localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.097986810Z I1024 13:12:06.094477       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:06.098063230Z I1024 13:12:06.097696       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:12:06.103309590Z I1024 13:12:06.103282       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nInstallerControllerDegraded: missing required resources: secrets: localhost-recovery-serving-certkey-5","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:12:06.118732650Z I1024 13:12:06.116575       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nInstallerControllerDegraded: missing required resources: secrets: localhost-recovery-client-token-5,localhost-recovery-serving-certkey-5" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nInstallerControllerDegraded: missing required resources: secrets: localhost-recovery-serving-certkey-5"
2024-10-24T13:12:06.154873250Z W1024 13:12:06.154785       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:12:06.394217023Z I1024 13:12:06.394163       1 request.go:700] Waited for 1.969386151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config/configmaps?resourceVersion=16526
2024-10-24T13:12:06.396938723Z I1024 13:12:06.396894       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:07.394717951Z I1024 13:12:07.394646       1 request.go:700] Waited for 1.787866916s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:12:07.404817741Z I1024 13:12:07.404725       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:07.404862261Z I1024 13:12:07.404814       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:08.394830739Z I1024 13:12:08.394741       1 request.go:700] Waited for 1.786998264s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:12:09.202863240Z I1024 13:12:09.202809       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:09.203550900Z I1024 13:12:09.203490       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:09.399702668Z E1024 13:12:09.399635       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:09.399702668Z E1024 13:12:09.399658       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:09.401642248Z E1024 13:12:09.401596       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:12:09.594092316Z I1024 13:12:09.594042       1 request.go:700] Waited for 1.792809968s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:10.205708359Z I1024 13:12:10.205641       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:10.205856988Z I1024 13:12:10.205814       1 core.go:352] ConfigMap "openshift-kube-apiserver/config" changes: {"apiVersion":"v1","data":{"config.yaml":"{\"admission\":{\"pluginConfig\":{\"PodSecurity\":{\"configuration\":{\"apiVersion\":\"pod-security.admission.config.k8s.io/v1\",\"defaults\":{\"audit\":\"restricted\",\"audit-version\":\"latest\",\"enforce\":\"restricted\",\"enforce-version\":\"latest\",\"warn\":\"restricted\",\"warn-version\":\"latest\"},\"exemptions\":{\"usernames\":[\"system:serviceaccount:openshift-infra:build-controller\"]},\"kind\":\"PodSecurityConfiguration\"}},\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"200\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/policy.yaml\"],\"authentication-token-webhook-config-file\":[\"/etc/kubernetes/static-pod-resources/secrets/webhook-authenticator/kubeConfig\"],\"authentication-token-webhook-version\":[\"v1\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"ValidatingAdmissionPolicy\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\",\"storage.openshift.io/CSIInlineVolumeSecurity\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-healthcheck-timeout\":[\"9s\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-readycheck-timeout\":[\"9s\"],\"etcd-servers\":[\"https://10.0.0.3:2379\",\"https://10.0.0.4:2379\",\"https://10.0.0.5:2379\",\"https://10.0.0.6:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"AWSEFSDriverVolumeMetrics=true\",\"AdminNetworkPolicy=true\",\"AlibabaPlatform=true\",\"AzureWorkloadIdentity=true\",\"BareMetalLoadBalancer=true\",\"BuildCSIVolumes=true\",\"ChunkSizeMiB=true\",\"CloudDualStackNodeIPs=true\",\"DisableKubeletCloudCredentialProviders=true\",\"GCPLabelsTags=true\",\"HardwareSpeed=true\",\"IngressControllerLBSubnetsAWS=true\",\"KMSv1=true\",\"ManagedBootImages=true\",\"MetricsServer=true\",\"MultiArchInstallAWS=true\",\"MultiArchInstallGCP=true\",\"NetworkDiagnosticsConfig=true\",\"NetworkLiveMigration=true\",\"NodeDisruptionPolicy=true\",\"OpenShiftPodSecurityAdmission=true\",\"PrivateHostedZoneAWS=true\",\"SetEIPForNLBIngressController=true\",\"VSphereControlPlaneMachineSet=true\",\"VSphereDriverConfiguration=true\",\"VSphereStaticIPs=true\",\"ValidatingAdmissionPolicy=true\",\"AWSClusterHostedDNS=false\",\"AdditionalRoutingCapabilities=false\",\"AutomatedEtcdBackup=false\",\"BootcNodeManagement=false\",\"CSIDriverSharedResource=false\",\"ClusterAPIInstall=false\",\"ClusterAPIInstallIBMCloud=false\",\"ClusterMonitoringConfig=false\",\"DNSNameResolver=false\",\"DynamicResourceAllocation=false\",\"EtcdBackendQuota=false\",\"EventedPLEG=false\",\"Example=false\",\"ExternalOIDC=false\",\"GCPClusterHostedDNS=false\",\"GatewayAPI=false\",\"ImageStreamImportMode=false\",\"IngressControllerDynamicConfigurationManager=false\",\"InsightsConfig=false\",\"InsightsConfigAPI=false\",\"InsightsOnDemandDataGather=false\",\"InsightsRuntimeExtractor=false\",\"MachineAPIMigration=false\",\"MachineAPIOperatorDisableMachineHealthCheckController=false\",\"MachineAPIProviderOpenStack=false\",\"MachineConfigNodes=false\",\"ManagedBootImagesAWS=false\",\"MaxUnavailableStatefulSet=false\",\"MetricsCollectionProfiles=false\",\"MixedCPUsAllocation=false\",\"MultiArchInstallAzure=false\",\"NetworkSegmentation=false\",\"NewOLM=false\",\"NodeSwap=false\",\"OVNObservability=false\",\"OnClusterBuild=false\",\"PersistentIPsForVirtualization=false\",\"PinnedImages=false\",\"PlatformOperators=false\",\"ProcMountType=false\",\"RouteAdvertisements=false\",\"RouteExternalCertificate=false\",\"ServiceAccountTokenNodeBinding=false\",\"SignatureStores=false\",\"SigstoreImageVerification=false\",\"TranslateStreamCloseWebsocketRequests=false\",\"UpgradeStatus=false\",\"UserNamespacesPodSecurityStandards=false\",\"UserNamespacesSupport=false\",\"VSphereMultiNetworks=false\",\"VSphereMultiVCenters=false\",\"VolumeGroupSnapshot=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.key\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"admissionregistration.k8s.io/v1beta1=true\"],\"send-retry-after-while-not-ready-once\":[\"false\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"shutdown-send-retry-after\":[\"true\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"strict-transport-security-directives\":[\"max-age=31536000,includeSubDomains,preload\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}}"},"kind":"ConfigMap","metadata":{"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null}}
2024-10-24T13:12:10.207822899Z I1024 13:12:10.207784       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/config -n openshift-kube-apiserver:
2024-10-24T13:12:10.207822899Z cause by changes in data.config.yaml
2024-10-24T13:12:10.391803786Z I1024 13:12:10.391696       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:10.457963245Z I1024 13:12:10.457897       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:10.594885064Z I1024 13:12:10.594821       1 request.go:700] Waited for 1.593272962s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:12:10.802272732Z I1024 13:12:10.802209       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:10.802706712Z I1024 13:12:10.802646       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:11.793918860Z I1024 13:12:11.793850       1 request.go:700] Waited for 1.315260355s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2024-10-24T13:12:12.206547725Z I1024 13:12:12.206467       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:12.403096933Z I1024 13:12:12.403037       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:12:12.403096933Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:12:12.403096933Z  CurrentRevision: (int32) 0,
2024-10-24T13:12:12.403096933Z  TargetRevision: (int32) 5,
2024-10-24T13:12:12.403096933Z  LastFailedRevision: (int32) 0,
2024-10-24T13:12:12.403096933Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:12:12.403096933Z  LastFailedReason: (string) "",
2024-10-24T13:12:12.403096933Z  LastFailedCount: (int) 0,
2024-10-24T13:12:12.403096933Z  LastFallbackCount: (int) 0,
2024-10-24T13:12:12.403096933Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:12:12.403096933Z }
2024-10-24T13:12:12.403096933Z  because new revision pending
2024-10-24T13:12:12.405020103Z W1024 13:12:12.404979       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:12.405088223Z W1024 13:12:12.405071       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:12.405126923Z W1024 13:12:12.405114       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:12.405157883Z W1024 13:12:12.405146       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:12.405196113Z W1024 13:12:12.405176       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:12.433983873Z I1024 13:12:12.433948       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:12:12.434053863Z I1024 13:12:12.433971       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:12.435531473Z I1024 13:12:12.435487       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:12:12.448699253Z I1024 13:12:12.448636       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nInstallerControllerDegraded: missing required resources: secrets: localhost-recovery-serving-certkey-5" to "GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:12:12.468795292Z I1024 13:12:12.468719       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:12:12.469786902Z I1024 13:12:12.469718       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:12.794912859Z I1024 13:12:12.794836       1 request.go:700] Waited for 1.386995305s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:12:13.227641424Z I1024 13:12:13.227591       1 reflector.go:368] Caches populated for *v1.KubeAPIServer from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:13.994963864Z I1024 13:12:13.994901       1 request.go:700] Waited for 1.557248701s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:12:15.194586661Z I1024 13:12:15.194511       1 request.go:700] Waited for 1.792549939s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:12:15.406406778Z I1024 13:12:15.406334       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:16.194592559Z I1024 13:12:16.194520       1 request.go:700] Waited for 1.593772202s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2024-10-24T13:12:16.402920867Z E1024 13:12:16.402863       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:16.402920867Z E1024 13:12:16.402899       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:16.404830387Z E1024 13:12:16.404722       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:12:17.208422737Z I1024 13:12:17.208343       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:17.394745175Z I1024 13:12:17.394677       1 request.go:700] Waited for 1.794639399s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:18.196954216Z I1024 13:12:18.196872       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:18.197506426Z I1024 13:12:18.197451       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:12:18.394877734Z I1024 13:12:18.394710       1 request.go:700] Waited for 1.591707502s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:18.804454159Z I1024 13:12:18.804372       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-6 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:19.000186607Z I1024 13:12:19.000107       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:12:19.000186607Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:12:19.000186607Z  CurrentRevision: (int32) 0,
2024-10-24T13:12:19.000186607Z  TargetRevision: (int32) 5,
2024-10-24T13:12:19.000186607Z  LastFailedRevision: (int32) 0,
2024-10-24T13:12:19.000186607Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:12:19.000186607Z  LastFailedReason: (string) "",
2024-10-24T13:12:19.000186607Z  LastFailedCount: (int) 0,
2024-10-24T13:12:19.000186607Z  LastFallbackCount: (int) 0,
2024-10-24T13:12:19.000186607Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:12:19.000186607Z }
2024-10-24T13:12:19.000186607Z  because new revision pending
2024-10-24T13:12:19.145224455Z I1024 13:12:19.145149       1 reflector.go:368] Caches populated for *v1.ClusterRoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:12:19.404222842Z E1024 13:12:19.404141       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:19.404222842Z E1024 13:12:19.404171       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:19.406030242Z E1024 13:12:19.405975       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:12:19.594564490Z I1024 13:12:19.594498       1 request.go:700] Waited for 1.536742753s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:12:19.803006598Z I1024 13:12:19.802950       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:19.803508678Z I1024 13:12:19.803451       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 6 triggered by "secret \"localhost-recovery-serving-certkey-5\" not found"
2024-10-24T13:12:19.805406388Z W1024 13:12:19.805377       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:19.805406388Z W1024 13:12:19.805394       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:19.833481337Z W1024 13:12:19.833432       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:19.833481337Z W1024 13:12:19.833461       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:19.835733377Z I1024 13:12:19.835684       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:19.837326317Z I1024 13:12:19.837303       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:12:19.861934877Z I1024 13:12:19.861870       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:20.597795329Z I1024 13:12:20.595363       1 request.go:700] Waited for 1.389173395s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:12:21.401269589Z I1024 13:12:21.401215       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:21.794539155Z I1024 13:12:21.794473       1 request.go:700] Waited for 1.590478732s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:22.994053731Z I1024 13:12:22.993986       1 request.go:700] Waited for 1.593140521s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:12:23.004690071Z I1024 13:12:23.004621       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:23.004690071Z I1024 13:12:23.004675       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:23.994511029Z I1024 13:12:23.994446       1 request.go:700] Waited for 1.393386433s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:12:24.203676187Z E1024 13:12:24.203620       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:12:24.203811517Z E1024 13:12:24.203788       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:12:24.404609055Z I1024 13:12:24.404554       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:12:24.406483365Z I1024 13:12:24.406403       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:12:25.194740316Z I1024 13:12:25.194666       1 request.go:700] Waited for 1.281627966s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2024-10-24T13:12:25.795700499Z I1024 13:12:25.795625       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:25.796483569Z E1024 13:12:25.796407       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:12:25.798015498Z E1024 13:12:25.797989       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:25.798047069Z I1024 13:12:25.798012       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:25.807543448Z I1024 13:12:25.807464       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:25.997660486Z W1024 13:12:25.997599       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:25.997875366Z E1024 13:12:25.997653       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:26.195201184Z I1024 13:12:26.195125       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'PodCreateFailed' Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:26.195277734Z I1024 13:12:26.195169       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 5 count 0 on node "ci-op-2fcpj5j6-f6035-2lklf-master-0": Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:26.196948454Z E1024 13:12:26.196896       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:12:26.197454184Z W1024 13:12:26.197416       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:26.197454184Z W1024 13:12:26.197435       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:26.197454184Z W1024 13:12:26.197440       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:26.198589394Z E1024 13:12:26.198557       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:26.362599522Z W1024 13:12:26.362450       1 reflector.go:561] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=16526": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:26.362643862Z E1024 13:12:26.362607       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=16526\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:26.394031752Z I1024 13:12:26.393996       1 request.go:700] Waited for 1.393075764s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2024-10-24T13:12:26.594649280Z W1024 13:12:26.594560       1 reflector.go:561] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?resourceVersion=16526": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:26.594683170Z E1024 13:12:26.594649       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?resourceVersion=16526\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:26.998057265Z E1024 13:12:26.997998       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:27.075285854Z E1024 13:12:27.075223       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:12:27.195088823Z E1024 13:12:27.195017       1 guard_controller.go:366] Unable to apply pod kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0 changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:27.197348613Z W1024 13:12:27.197262       1 base_controller.go:234] Updating status of "GuardController" failed: unable to ApplyStatus for operator using fieldManager "GuardController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=GuardController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:27.197348613Z E1024 13:12:27.197308       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2, Unable to apply pod kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0 changes: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:27.394200700Z I1024 13:12:27.394135       1 request.go:700] Waited for 1.586727542s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:12:27.397344551Z E1024 13:12:27.397293       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:27.397378420Z I1024 13:12:27.397350       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:27.401314791Z I1024 13:12:27.401278       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:27.598129028Z W1024 13:12:27.598071       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:27.598129028Z E1024 13:12:27.598108       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:27.666924248Z I1024 13:12:27.666840       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0" (last termination at 2024-10-24 13:10:20 +0000 UTC) at 2024-10-24 13:12:27 +0000 UTC
2024-10-24T13:12:27.996974194Z E1024 13:12:27.996906       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:28.208840131Z E1024 13:12:28.208783       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:28.396744939Z E1024 13:12:28.396689       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:28.594309227Z I1024 13:12:28.594245       1 request.go:700] Waited for 1.192939246s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:12:28.597594517Z E1024 13:12:28.597552       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:28.597621427Z I1024 13:12:28.597588       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:28.601552787Z I1024 13:12:28.601502       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:28.797514084Z W1024 13:12:28.797443       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:28.797514084Z E1024 13:12:28.797499       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:29.594508225Z I1024 13:12:29.594442       1 request.go:700] Waited for 1.185950306s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:12:29.598394706Z E1024 13:12:29.598339       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:29.797708593Z E1024 13:12:29.797640       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:29.797708593Z I1024 13:12:29.797680       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:29.802050273Z I1024 13:12:29.801997       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:29.895659892Z E1024 13:12:29.895585       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:12:29.997513191Z W1024 13:12:29.997450       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:29.997513191Z E1024 13:12:29.997493       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:30.794195132Z I1024 13:12:30.794136       1 request.go:700] Waited for 1.173662167s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:12:30.797522732Z E1024 13:12:30.797443       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:30.997704639Z E1024 13:12:30.997635       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:30.997704639Z I1024 13:12:30.997678       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:31.001888529Z I1024 13:12:31.001836       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:31.197722527Z W1024 13:12:31.197665       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:31.197722527Z E1024 13:12:31.197705       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:31.397384515Z W1024 13:12:31.397311       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:31.397384515Z W1024 13:12:31.397340       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:31.397384515Z W1024 13:12:31.397347       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:31.398249345Z E1024 13:12:31.398204       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:31.810128110Z E1024 13:12:31.810046       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:31.994294388Z I1024 13:12:31.994235       1 request.go:700] Waited for 1.154746267s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:12:31.997230998Z E1024 13:12:31.997175       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:32.197735036Z E1024 13:12:32.197669       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:32.197735036Z I1024 13:12:32.197711       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:32.201979945Z I1024 13:12:32.201938       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:32.398461623Z W1024 13:12:32.398383       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:32.398461623Z E1024 13:12:32.398449       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:32.994374956Z I1024 13:12:32.994313       1 request.go:700] Waited for 1.182822576s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2024-10-24T13:12:33.198039174Z E1024 13:12:33.197974       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:33.396825182Z E1024 13:12:33.396767       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:33.396825182Z I1024 13:12:33.396799       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:33.400966712Z I1024 13:12:33.400925       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:33.597518999Z W1024 13:12:33.597454       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:33.597518999Z E1024 13:12:33.597496       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:33.994814894Z I1024 13:12:33.994745       1 request.go:700] Waited for 1.198960345s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2024-10-24T13:12:34.397658070Z E1024 13:12:34.397590       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:34.597676047Z E1024 13:12:34.597598       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:34.597676047Z I1024 13:12:34.597634       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:34.601581527Z I1024 13:12:34.601532       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:34.997655053Z W1024 13:12:34.997593       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:34.997655053Z E1024 13:12:34.997646       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:35.194779610Z I1024 13:12:35.194687       1 request.go:700] Waited for 1.198994986s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2024-10-24T13:12:35.196861080Z E1024 13:12:35.196831       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:35.407731798Z E1024 13:12:35.407663       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:35.599268016Z E1024 13:12:35.599204       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:35.797535483Z E1024 13:12:35.797474       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:35.797535483Z I1024 13:12:35.797517       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:35.882584922Z I1024 13:12:35.882516       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:35.997319361Z W1024 13:12:35.997253       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:35.997319361Z W1024 13:12:35.997279       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:35.997319361Z W1024 13:12:35.997286       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:35.998120821Z E1024 13:12:35.998073       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:36.597934764Z E1024 13:12:36.597858       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:36.794659062Z I1024 13:12:36.794609       1 request.go:700] Waited for 1.154428967s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2024-10-24T13:12:36.797871732Z W1024 13:12:36.797742       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:36.797871732Z E1024 13:12:36.797840       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:36.997919089Z E1024 13:12:36.997849       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:36.997964729Z I1024 13:12:36.997908       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:37.076720629Z E1024 13:12:37.076656       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:12:37.794725790Z I1024 13:12:37.794657       1 request.go:700] Waited for 1.195183426s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:12:37.798062670Z E1024 13:12:37.797996       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:38.414398763Z E1024 13:12:38.414329       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:38.597925851Z E1024 13:12:38.597854       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:38.998001026Z W1024 13:12:38.997938       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:38.998001026Z E1024 13:12:38.997992       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:39.596640759Z W1024 13:12:39.596586       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:39.596640759Z W1024 13:12:39.596607       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:39.596640759Z W1024 13:12:39.596612       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:39.597538469Z E1024 13:12:39.597502       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:39.897927826Z E1024 13:12:39.897860       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:12:40.403808970Z E1024 13:12:40.403723       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:40.611225207Z E1024 13:12:40.611146       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:41.997348752Z W1024 13:12:41.997271       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:41.997348752Z W1024 13:12:41.997294       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:41.997348752Z W1024 13:12:41.997300       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:41.998173111Z E1024 13:12:41.998114       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:42.122596240Z I1024 13:12:42.122527       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:42.197805639Z W1024 13:12:42.197722       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:42.197805639Z E1024 13:12:42.197794       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:42.608176635Z E1024 13:12:42.608117       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:42.998030360Z E1024 13:12:42.997969       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:42.998030360Z I1024 13:12:42.998008       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:43.797654871Z E1024 13:12:43.797605       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:44.596996691Z E1024 13:12:44.596946       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:44.796782099Z E1024 13:12:44.796706       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:45.008329386Z E1024 13:12:45.008270       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:45.196889354Z W1024 13:12:45.196829       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:45.196889354Z W1024 13:12:45.196850       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:45.196889354Z W1024 13:12:45.196857       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:45.197665664Z E1024 13:12:45.197620       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:46.809205376Z E1024 13:12:46.809144       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:47.078410453Z E1024 13:12:47.078345       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:12:47.596913926Z W1024 13:12:47.596832       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:47.596913926Z W1024 13:12:47.596855       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:47.596913926Z W1024 13:12:47.596863       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:47.597551007Z E1024 13:12:47.597523       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:47.997608982Z W1024 13:12:47.997548       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:47.997608982Z E1024 13:12:47.997592       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:48.163540000Z E1024 13:12:48.163491       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:12:48.164077530Z E1024 13:12:48.164042       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:48.597005895Z E1024 13:12:48.596943       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:48.809130503Z E1024 13:12:48.809057       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:49.899423510Z E1024 13:12:49.899354       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:12:50.197149557Z W1024 13:12:50.197095       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:50.197149557Z W1024 13:12:50.197117       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:50.197149557Z W1024 13:12:50.197122       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:50.197820147Z E1024 13:12:50.197793       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:50.611116512Z E1024 13:12:50.611047       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:52.196680944Z E1024 13:12:52.196616       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:52.398304662Z W1024 13:12:52.398239       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:52.398304662Z W1024 13:12:52.398268       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:52.398304662Z W1024 13:12:52.398274       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:52.399331822Z E1024 13:12:52.399284       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:52.610230149Z E1024 13:12:52.610153       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:53.243428672Z I1024 13:12:53.243350       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:12:53.597858578Z E1024 13:12:53.597790       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:54.197046621Z E1024 13:12:54.196980       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:54.197046621Z I1024 13:12:54.197017       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:54.997497932Z E1024 13:12:54.997440       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:55.197438890Z W1024 13:12:55.197381       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:55.197438890Z W1024 13:12:55.197404       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:55.197438890Z W1024 13:12:55.197409       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:55.198268630Z E1024 13:12:55.198223       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:55.409733567Z E1024 13:12:55.409669       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:12:55.996803270Z E1024 13:12:55.996732       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:56.997630129Z W1024 13:12:56.997563       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:56.997630129Z W1024 13:12:56.997589       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:56.997630129Z W1024 13:12:56.997594       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:56.998893929Z E1024 13:12:56.998838       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:57.080039878Z E1024 13:12:57.079980       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:12:57.796600620Z E1024 13:12:57.796534       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:58.797710178Z W1024 13:12:58.797647       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:12:58.797710178Z E1024 13:12:58.797698       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:59.396794591Z W1024 13:12:59.396694       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:12:59.396794591Z W1024 13:12:59.396718       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:12:59.396794591Z W1024 13:12:59.396722       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:12:59.397648591Z E1024 13:12:59.397601       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:12:59.901407906Z E1024 13:12:59.901354       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:12:59.997917254Z E1024 13:12:59.997845       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:00.394811840Z E1024 13:13:00.394743       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:00.995536673Z E1024 13:13:00.995468       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:01.595848076Z E1024 13:13:01.595668       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:01.797306164Z E1024 13:13:01.797249       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:01.998921962Z E1024 13:13:01.998874       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:02.395072507Z E1024 13:13:02.394990       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:02.798306892Z E1024 13:13:02.798258       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:03.071905469Z I1024 13:13:03.071842       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:13:03.195378598Z E1024 13:13:03.195311       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:03.598388593Z E1024 13:13:03.598330       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:03.796699291Z E1024 13:13:03.796637       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:04.194492696Z I1024 13:13:04.194436       1 request.go:700] Waited for 1.122576816s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps
2024-10-24T13:13:04.197847467Z E1024 13:13:04.197743       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:04.197847467Z I1024 13:13:04.197818       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:04.397817684Z E1024 13:13:04.397773       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:04.594536002Z E1024 13:13:04.594479       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:04.797507690Z E1024 13:13:04.797453       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:04.998157817Z E1024 13:13:04.998096       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:05.194720635Z I1024 13:13:05.194646       1 request.go:700] Waited for 1.393451954s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2024-10-24T13:13:05.595572410Z E1024 13:13:05.595499       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:05.995106036Z W1024 13:13:05.994987       1 reflector.go:561] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?resourceVersion=16526": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:05.995169685Z E1024 13:13:05.995097       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?resourceVersion=16526\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:06.198483883Z E1024 13:13:06.198419       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:06.394721171Z I1024 13:13:06.394666       1 request.go:700] Waited for 1.196689586s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-pod
2024-10-24T13:13:06.609526618Z E1024 13:13:06.609454       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:06.995490214Z E1024 13:13:06.995422       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:07.081862313Z E1024 13:13:07.081804       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:13:07.198653682Z E1024 13:13:07.198597       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:07.798088505Z E1024 13:13:07.798028       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:07.998058553Z E1024 13:13:07.997999       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:08.594894616Z E1024 13:13:08.594831       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:08.598952796Z W1024 13:13:08.598880       1 reflector.go:561] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=16526": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:08.599007486Z E1024 13:13:08.598947       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=16526\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:09.199525839Z E1024 13:13:09.199464       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:09.409362866Z E1024 13:13:09.409293       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: [\"assets/kube-apiserver/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrole-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-auth-delegator.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-node-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-node-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-clusterrolebinding-crd-reader.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:kube-apiserver-check-endpoints-crd-reader\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding-kube-system.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:controller:kube-apiserver-check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/check-endpoints-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-apiserver/rolebindings/system:openshift:controller:check-endpoints\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/delegated-incluster-authentication-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/authentication-reader-for-authenticated-users\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-apiserver-recovery\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/localhost-recovery-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml\" (string): Get \"https://172.30.0.1:443/apis/apiextensions.k8s.io/v1/customresourcedefinitions/apirequestcounts.apiserver.openshift.io\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-flowschema-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-flowschema-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/kube-apiserver/storage-version-migration-prioritylevelconfiguration-v1beta3.yaml\" (string): Get \"https://172.30.0.1:443/apis/migration.k8s.io/v1alpha1/storageversionmigrations/flowcontrol-prioritylevel-storage-version-migration-v1beta3\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/api-usage.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/api-usage\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/audit-errors.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/audit-errors\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-requests.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-requests\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-basic.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-basic\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/podsecurity-violations.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/podsecurity\": dial tcp 172.30.0.1:443: connect: connection refused, \"assets/alerts/kube-apiserver-slos-extended.yaml\" (string): Get \"https://172.30.0.1:443/apis/monitoring.coreos.com/v1/namespaces/openshift-kube-apiserver/prometheusrules/kube-apiserver-slos-extended\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"KubeAPIServerStaticResources-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=KubeAPIServerStaticResources-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:09.797513462Z E1024 13:13:09.797459       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:09.903092801Z E1024 13:13:09.903039       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:13:09.997168790Z E1024 13:13:09.997105       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:10.597693403Z E1024 13:13:10.597626       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:10.795183541Z E1024 13:13:10.795115       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:12.197046021Z E1024 13:13:12.197000       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:12.396889266Z W1024 13:13:12.396821       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:13:12.396889266Z W1024 13:13:12.396846       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:13:12.396889266Z W1024 13:13:12.396851       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:13:12.397864835Z E1024 13:13:12.397812       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-Installer&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:12.599036110Z E1024 13:13:12.598969       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:13.596981311Z E1024 13:13:13.596918       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-AuditPolicy&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:13.797136956Z E1024 13:13:13.797068       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:13.995290870Z E1024 13:13:13.995222       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:14.165244847Z E1024 13:13:14.165158       1 leaderelection.go:429] Failed to update lock optimitically: Put "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused, falling back to slow path
2024-10-24T13:13:14.166176627Z E1024 13:13:14.166124       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:14.197154594Z E1024 13:13:14.197086       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:14.686234596Z I1024 13:13:14.686154       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:13:14.798357147Z E1024 13:13:14.798265       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:14.998234742Z E1024 13:13:14.998182       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-RevisionController&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:14.998297442Z I1024 13:13:14.998227       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:15.598492545Z E1024 13:13:15.598422       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:16.397406402Z E1024 13:13:16.397345       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:16.997642675Z E1024 13:13:16.997573       1 base_controller.go:271] "Unhandled Error" err="TargetConfigController reconciliation failed: Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:17.084302669Z E1024 13:13:17.084221       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1b44b9525  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:ConfigMapCreateFailed,Message:Failed to create ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,LastTimestamp:2024-10-24 13:12:25.795450149 +0000 UTC m=+446.124297746,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:13:17.597127598Z E1024 13:13:17.597074       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:18.883828638Z E1024 13:13:18.883745       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:19.117968849Z E1024 13:13:19.117910       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:19.283465856Z W1024 13:13:19.283415       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": Patch "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=NodeKubeconfigController-reportDegraded&force=true": dial tcp 172.30.0.1:443: connect: connection refused
2024-10-24T13:13:19.283520326Z E1024 13:13:19.283467       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:19.904488537Z E1024 13:13:19.904432       1 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/events\": dial tcp 172.30.0.1:443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b1cc1b8304  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:PodCreateFailed,Message:Failed to create Pod/installer-5-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver: Post \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods\": dial tcp 172.30.0.1:443: connect: connection refused,Source:EventSource{Component:kube-apiserver-operator-installer-controller,Host:,},FirstTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,LastTimestamp:2024-10-24 13:12:26.194952964 +0000 UTC m=+446.523800561,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-installer-controller,ReportingInstance:,}"
2024-10-24T13:13:20.724813703Z E1024 13:13:20.724744       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: connect: connection refused, \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-apiserver-installer\": dial tcp 172.30.0.1:443: connect: connection refused, unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=BackingResourceController-StaticResources&force=true\": dial tcp 172.30.0.1:443: connect: connection refused]"
2024-10-24T13:13:21.449488067Z E1024 13:13:21.449421       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-StaticPodState reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-StaticPodState\": Patch \"https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeapiservers/cluster/status?fieldManager=kube-apiserver-StaticPodState&force=true\": dial tcp 172.30.0.1:443: connect: connection refused"
2024-10-24T13:13:49.199351352Z I1024 13:13:49.199272       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:13:52.043156909Z I1024 13:13:52.043091       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:13:52.043156909Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:13:52.043156909Z  CurrentRevision: (int32) 0,
2024-10-24T13:13:52.043156909Z  TargetRevision: (int32) 6,
2024-10-24T13:13:52.043156909Z  LastFailedRevision: (int32) 0,
2024-10-24T13:13:52.043156909Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:13:52.043156909Z  LastFailedReason: (string) "",
2024-10-24T13:13:52.043156909Z  LastFailedCount: (int) 0,
2024-10-24T13:13:52.043156909Z  LastFallbackCount: (int) 0,
2024-10-24T13:13:52.043156909Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:13:52.043156909Z }
2024-10-24T13:13:52.043156909Z  because new revision pending
2024-10-24T13:13:52.046116939Z W1024 13:13:52.046051       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:13:52.046116939Z W1024 13:13:52.046102       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:13:52.046116939Z W1024 13:13:52.046110       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:13:52.046144999Z W1024 13:13:52.046115       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:13:52.046144999Z W1024 13:13:52.046121       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:13:55.729345210Z I1024 13:13:55.729276       1 reflector.go:368] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:13:58.136563721Z I1024 13:13:58.136475       1 reflector.go:368] Caches populated for *v1.CustomResourceDefinition from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:13:58.137099121Z I1024 13:13:58.137042       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:13:58.137426631Z I1024 13:13:58.137361       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:13:58.137663641Z I1024 13:13:58.137613       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:13:58.138076281Z I1024 13:13:58.138001       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:13:58.138242651Z I1024 13:13:58.138159       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:13:58.222639124Z W1024 13:13:58.222557       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:13:59.226674026Z W1024 13:13:59.226548       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:01.310502733Z W1024 13:14:01.310353       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:02.314374234Z W1024 13:14:02.314256       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:03.072811435Z I1024 13:14:03.072716       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:14:03.096844883Z I1024 13:14:03.096724       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:03.266136909Z I1024 13:14:03.266031       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:03.864222513Z I1024 13:14:03.864102       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:03.976620114Z I1024 13:14:03.976551       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:04.025762120Z I1024 13:14:04.025656       1 reflector.go:368] Caches populated for *v1.Proxy from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:04.270188471Z I1024 13:14:04.270091       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:04.466063605Z I1024 13:14:04.465966       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:04.666119680Z I1024 13:14:04.666031       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:04.864278394Z I1024 13:14:04.864175       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:05.068071668Z I1024 13:14:05.067990       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:05.470665687Z I1024 13:14:05.470587       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:05.670813691Z I1024 13:14:05.670740       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:05.865530525Z I1024 13:14:05.865450       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-7 -n openshift-kube-apiserver because it was missing
2024-10-24T13:14:06.067544480Z I1024 13:14:06.067454       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:14:06.070891420Z W1024 13:14:06.070713       1 dynamic_operator_client.go:352] .status.conditions["RevisionControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:14:06.070891420Z W1024 13:14:06.070743       1 dynamic_operator_client.go:355] .status.conditions["RevisionControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:14:06.749625656Z I1024 13:14:06.749524       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:08.098278081Z I1024 13:14:08.098215       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:11.088299306Z I1024 13:14:11.088226       1 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:12.617214676Z W1024 13:14:12.617133       1 reflector.go:484] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: watch of *v1.Event ended with: Internal error occurred: etcdserver: no leader
2024-10-24T13:14:13.788041374Z I1024 13:14:13.787976       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:15.816827145Z I1024 13:14:15.816697       1 reflector.go:368] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:16.253367418Z I1024 13:14:16.253307       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:16.254365779Z I1024 13:14:16.254338       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.254690778Z I1024 13:14:16.254671       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.255082549Z I1024 13:14:16.255061       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.255514798Z I1024 13:14:16.255394       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.256597148Z I1024 13:14:16.256559       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.256985818Z I1024 13:14:16.256963       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.257339048Z I1024 13:14:16.257316       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.257679808Z I1024 13:14:16.257657       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.258086628Z I1024 13:14:16.257998       1 prune_controller.go:277] Nothing to prune
2024-10-24T13:14:16.261427228Z I1024 13:14:16.261361       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 7 triggered by "required configmap/config has changed"
2024-10-24T13:14:17.940127146Z I1024 13:14:17.940062       1 reflector.go:368] Caches populated for *v1.Infrastructure from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:17.940200736Z I1024 13:14:17.940167       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX
2024-10-24T13:14:17.940624796Z I1024 13:14:17.940578       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:14:19.662555662Z I1024 13:14:19.662472       1 reflector.go:368] Caches populated for operator.openshift.io/v1, Resource=kubeapiservers from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:19.663867372Z I1024 13:14:19.663828       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:14:19.665019122Z I1024 13:14:19.664973       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:14:20.701447655Z I1024 13:14:20.701379       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:20.701802245Z I1024 13:14:20.701741       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:14:20.877165966Z I1024 13:14:20.877088       1 reflector.go:368] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:21.614373355Z I1024 13:14:21.614306       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:21.697649781Z W1024 13:14:21.697582       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:22.403886712Z I1024 13:14:22.403810       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:22.701127446Z W1024 13:14:22.701047       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:22.820078400Z I1024 13:14:22.820020       1 reflector.go:368] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:23.596964217Z I1024 13:14:23.596887       1 reflector.go:368] Caches populated for *v1.ClusterRoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:24.800357111Z W1024 13:14:24.800038       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:25.804322117Z W1024 13:14:25.804244       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:25.975030107Z I1024 13:14:25.974967       1 reflector.go:368] Caches populated for *v1.Image from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:26.364532206Z I1024 13:14:26.364477       1 reflector.go:368] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:26.694841977Z E1024 13:14:26.694794       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:26.696705247Z I1024 13:14:26.696651       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:14:26.757142884Z E1024 13:14:26.757075       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:26.885420027Z I1024 13:14:26.885352       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:28.072480022Z I1024 13:14:28.072427       1 reflector.go:368] Caches populated for *v1.ValidatingWebhookConfiguration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:28.086775222Z I1024 13:14:28.086715       1 reflector.go:368] Caches populated for *v1.Authentication from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:28.159615648Z W1024 13:14:28.159545       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:28.185377256Z E1024 13:14:28.185293       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:28.191237516Z W1024 13:14:28.191181       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": rpc error: code = DeadlineExceeded desc = context deadline exceeded
2024-10-24T13:14:28.191237516Z E1024 13:14:28.191229       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": rpc error: code = Unknown desc = malformed header: missing HTTP content-type"
2024-10-24T13:14:29.081085817Z I1024 13:14:29.081024       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:29.163639713Z W1024 13:14:29.163568       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:31.247123999Z I1024 13:14:31.247073       1 reflector.go:368] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:31.886368904Z I1024 13:14:31.886316       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:33.747149432Z E1024 13:14:33.747096       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:33.749184062Z I1024 13:14:33.749137       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:14:33.830914168Z E1024 13:14:33.830871       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:33.872021396Z I1024 13:14:33.871960       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:34.789684525Z I1024 13:14:34.789630       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:35.167199315Z I1024 13:14:35.167155       1 reflector.go:368] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:35.167562785Z I1024 13:14:35.167524       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:14:35.751522373Z I1024 13:14:35.751476       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:36.755546618Z I1024 13:14:36.755488       1 reflector.go:368] Caches populated for *v1.MutatingWebhookConfiguration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:37.241288571Z I1024 13:14:37.241229       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:37.474469079Z I1024 13:14:37.474423       1 reflector.go:368] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:38.232695657Z E1024 13:14:38.232631       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:38.322792892Z W1024 13:14:38.322716       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:38.736356170Z I1024 13:14:38.736281       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:38.736864670Z I1024 13:14:38.736817       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:14:39.218473323Z E1024 13:14:39.218414       1 leaderelection.go:429] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:14:39.326563808Z W1024 13:14:39.326431       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:40.780544918Z E1024 13:14:40.780487       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:40.782713368Z I1024 13:14:40.782672       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:14:40.936898699Z E1024 13:14:40.936821       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:40.997911296Z I1024 13:14:40.997848       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:41.114269700Z I1024 13:14:41.114222       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:41.861502809Z I1024 13:14:41.861196       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:42.200454220Z I1024 13:14:42.200391       1 reflector.go:368] Caches populated for *v1.OAuth from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:47.810143174Z E1024 13:14:47.810073       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:47.812717113Z I1024 13:14:47.812676       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:14:48.046363061Z E1024 13:14:48.046284       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:48.410989301Z E1024 13:14:48.410893       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:48.516027405Z W1024 13:14:48.515780       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:49.519568440Z W1024 13:14:49.519484       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:50.277167759Z E1024 13:14:50.277106       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" event="&Event{ObjectMeta:{kube-apiserver-operator.180165b0529f0563  openshift-kube-apiserver-operator   18912 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:StartingNewRevision,Message:new revision 7 triggered by \"required configmap/config has changed\",Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:12:19 +0000 UTC,LastTimestamp:2024-10-24 13:14:16.261060088 +0000 UTC m=+556.589907705,Count:16,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:14:51.991270405Z I1024 13:14:51.991169       1 reflector.go:368] Caches populated for *v1alpha1.StorageVersionMigration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:57.272163256Z I1024 13:14:57.272097       1 reflector.go:368] Caches populated for *v1.SecurityContextConstraints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:57.298725525Z E1024 13:14:57.298680       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": Timeout: request did not complete within requested timeout - context deadline exceeded"
2024-10-24T13:14:57.298860585Z I1024 13:14:57.298777       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: rpc error: code = DeadlineExceeded desc = context deadline exceeded
2024-10-24T13:14:58.144833419Z I1024 13:14:58.144768       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:14:58.928795576Z E1024 13:14:58.928723       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:58.988466643Z E1024 13:14:58.988398       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:14:58.990516623Z I1024 13:14:58.990481       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:14:59.042334120Z W1024 13:14:59.042262       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:14:59.056439559Z E1024 13:14:59.056396       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:15:00.046052725Z W1024 13:15:00.045927       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:00.315496250Z I1024 13:15:00.315425       1 reflector.go:368] Caches populated for *v1.APIServer from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:03.209270032Z I1024 13:15:03.209198       1 reflector.go:368] Caches populated for *v1.Network from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:03.209405572Z I1024 13:15:03.209358       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2024-10-24T13:15:06.023800118Z E1024 13:15:06.023704       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:15:06.025701498Z I1024 13:15:06.025632       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:06.114249183Z E1024 13:15:06.114194       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:15:09.077760691Z E1024 13:15:09.077686       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:15:09.171340536Z W1024 13:15:09.171271       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:10.174605991Z W1024 13:15:10.174535       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:13.040656545Z E1024 13:15:13.040601       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:15:13.042449485Z I1024 13:15:13.042401       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:13.147385939Z E1024 13:15:13.147345       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:15:17.945596807Z E1024 13:15:17.945551       1 base_controller.go:271] "Unhandled Error" err="highCPUUsageAlertController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request"
2024-10-24T13:15:18.304405217Z I1024 13:15:18.304362       1 trace.go:236] Trace[1612222251]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:14:55.036) (total time: 23268ms):
2024-10-24T13:15:18.304405217Z Trace[1612222251]: ---"Objects listed" error:<nil> 23268ms (13:15:18.304)
2024-10-24T13:15:18.304405217Z Trace[1612222251]: [23.268180838s] [23.268180838s] END
2024-10-24T13:15:18.305251457Z I1024 13:15:18.305229       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:18.305536487Z I1024 13:15:18.305075       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:18.305744247Z I1024 13:15:18.305147       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6"
2024-10-24T13:15:18.330178276Z I1024 13:15:18.330120       1 trace.go:236] Trace[1592582367]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:15:01.225) (total time: 17104ms):
2024-10-24T13:15:18.330178276Z Trace[1592582367]: ---"Objects listed" error:<nil> 17104ms (13:15:18.329)
2024-10-24T13:15:18.330178276Z Trace[1592582367]: [17.104809065s] [17.104809065s] END
2024-10-24T13:15:18.330178276Z I1024 13:15:18.330146       1 reflector.go:368] Caches populated for *v1.Event from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:18.486298967Z W1024 13:15:18.484823       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:18.653798628Z E1024 13:15:18.651490       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:18.661836608Z I1024 13:15:18.658809       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:18.704181615Z I1024 13:15:18.702045       1 trace.go:236] Trace[1953781166]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:15:05.983) (total time: 12718ms):
2024-10-24T13:15:18.704181615Z Trace[1953781166]: ---"Objects listed" error:<nil> 12718ms (13:15:18.701)
2024-10-24T13:15:18.704181615Z Trace[1953781166]: [12.718055474s] [12.718055474s] END
2024-10-24T13:15:18.704181615Z I1024 13:15:18.702069       1 reflector.go:368] Caches populated for monitoring.coreos.com/v1, Resource=prometheusrules from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:18.709804935Z I1024 13:15:18.709216       1 trace.go:236] Trace[1512247595]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:15:01.206) (total time: 17503ms):
2024-10-24T13:15:18.709804935Z Trace[1512247595]: ---"Objects listed" error:<nil> 17502ms (13:15:18.708)
2024-10-24T13:15:18.709804935Z Trace[1512247595]: [17.503110464s] [17.503110464s] END
2024-10-24T13:15:18.709804935Z I1024 13:15:18.709253       1 reflector.go:368] Caches populated for *v1.ClusterRole from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:18.987063690Z I1024 13:15:18.986368       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ci-op-2fcpj5j6-f6035-2lklf-master-0 -n openshift-kube-apiserver because it was missing
2024-10-24T13:15:18.994413290Z I1024 13:15:18.994312       1 trace.go:236] Trace[4452090]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:14:58.639) (total time: 20354ms):
2024-10-24T13:15:18.994413290Z Trace[4452090]: ---"Objects listed" error:<nil> 20354ms (13:15:18.994)
2024-10-24T13:15:18.994413290Z Trace[4452090]: [20.354355858s] [20.354355858s] END
2024-10-24T13:15:18.994533039Z I1024 13:15:18.994514       1 reflector.go:368] Caches populated for *v1.KubeAPIServer from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:18.994943919Z E1024 13:15:18.994471       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:18.998650229Z I1024 13:15:18.998605       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:19.000317479Z I1024 13:15:19.000283       1 trace.go:236] Trace[1008440571]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:15:06.377) (total time: 12622ms):
2024-10-24T13:15:19.000317479Z Trace[1008440571]: ---"Objects listed" error:<nil> 12620ms (13:15:18.998)
2024-10-24T13:15:19.000317479Z Trace[1008440571]: [12.62211433s] [12.62211433s] END
2024-10-24T13:15:19.000626699Z I1024 13:15:19.000498       1 reflector.go:368] Caches populated for *v1.Pod from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:19.017039618Z E1024 13:15:19.016978       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type"
2024-10-24T13:15:19.133989122Z I1024 13:15:19.133883       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:15:19.133989122Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:15:19.133989122Z  CurrentRevision: (int32) 0,
2024-10-24T13:15:19.133989122Z  TargetRevision: (int32) 6,
2024-10-24T13:15:19.133989122Z  LastFailedRevision: (int32) 0,
2024-10-24T13:15:19.133989122Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:15:19.133989122Z  LastFailedReason: (string) "",
2024-10-24T13:15:19.133989122Z  LastFailedCount: (int) 0,
2024-10-24T13:15:19.133989122Z  LastFallbackCount: (int) 0,
2024-10-24T13:15:19.133989122Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:15:19.133989122Z }
2024-10-24T13:15:19.133989122Z  because new revision pending
2024-10-24T13:15:19.148784321Z I1024 13:15:19.145738       1 termination_observer.go:130] Observed termination of API server pod "kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-0" at 2024-10-24 13:13:16 +0000 UTC
2024-10-24T13:15:19.149474331Z E1024 13:15:19.149439       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:19.155283081Z I1024 13:15:19.151697       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:19.222363407Z E1024 13:15:19.222307       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:19.225423597Z I1024 13:15:19.225372       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:19.226411487Z I1024 13:15:19.225674       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ci-op-2fcpj5j6-f6035-2lklf-master-1 -n openshift-kube-apiserver because it was missing
2024-10-24T13:15:19.327136961Z E1024 13:15:19.326482       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:19.329654421Z I1024 13:15:19.329617       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:19.429297016Z E1024 13:15:19.429248       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:19.434248825Z I1024 13:15:19.434083       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:19.494132482Z W1024 13:15:19.494056       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:19.543442640Z E1024 13:15:19.543347       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:19.593943947Z I1024 13:15:19.593890       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:19.814727194Z E1024 13:15:19.814671       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:19.816730934Z I1024 13:15:19.816676       1 trace.go:236] Trace[1634732981]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:14:48.637) (total time: 31178ms):
2024-10-24T13:15:19.816730934Z Trace[1634732981]: ---"Objects listed" error:<nil> 31178ms (13:15:19.816)
2024-10-24T13:15:19.816730934Z Trace[1634732981]: [31.178633146s] [31.178633146s] END
2024-10-24T13:15:19.816730934Z I1024 13:15:19.816700       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:20.210464683Z I1024 13:15:20.209941       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:20.310927228Z E1024 13:15:20.310875       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:20.315615237Z I1024 13:15:20.315581       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:20.374400594Z I1024 13:15:20.374349       1 request.go:700] Waited for 1.081257401s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2024-10-24T13:15:20.417837162Z I1024 13:15:20.416609       1 trace.go:236] Trace[2108580578]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:14:54.903) (total time: 25513ms):
2024-10-24T13:15:20.417837162Z Trace[2108580578]: ---"Objects listed" error:<nil> 25513ms (13:15:20.416)
2024-10-24T13:15:20.417837162Z Trace[2108580578]: [25.513367726s] [25.513367726s] END
2024-10-24T13:15:20.417837162Z I1024 13:15:20.416639       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:20.478225298Z E1024 13:15:20.478175       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:20.483987318Z I1024 13:15:20.483561       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:20.542799175Z E1024 13:15:20.542665       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:20.854641248Z E1024 13:15:20.854590       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:20.854774008Z E1024 13:15:20.854740       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:20.858425698Z E1024 13:15:20.858354       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:20.861229657Z E1024 13:15:20.861183       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:20.861229657Z E1024 13:15:20.861204       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:21.375663682Z I1024 13:15:21.375607       1 request.go:700] Waited for 1.432388055s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:21.453472569Z I1024 13:15:21.453410       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:21.816181423Z I1024 13:15:21.816086       1 helpers.go:184] lister was stale at resourceVersion=19083, live get showed resourceVersion=19238
2024-10-24T13:15:21.867858281Z I1024 13:15:21.867715       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ci-op-2fcpj5j6-f6035-2lklf-master-2 -n openshift-kube-apiserver because it was missing
2024-10-24T13:15:22.580899801Z I1024 13:15:22.580848       1 request.go:700] Waited for 1.719280273s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0
2024-10-24T13:15:22.822345441Z E1024 13:15:22.820785       1 base_controller.go:271] "Unhandled Error" err="KubeAPIServerStaticResources-StaticResources reconciliation failed: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)"
2024-10-24T13:15:23.047843380Z I1024 13:15:23.047778       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:23.060295720Z E1024 13:15:23.060238       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:23.199921764Z I1024 13:15:23.199845       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:23.775121200Z I1024 13:15:23.775056       1 request.go:700] Waited for 1.907524818s due to client-side throttling, not priority and fairness, request: DELETE:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/revision-status-1
2024-10-24T13:15:23.787569269Z I1024 13:15:23.787523       1 helpers.go:260] lister was stale at resourceVersion=19083, live get showed resourceVersion=19557
2024-10-24T13:15:24.423587652Z E1024 13:15:24.423539       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:24.975389798Z I1024 13:15:24.975323       1 request.go:700] Waited for 1.8736093s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:15:25.583314462Z I1024 13:15:25.583251       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:25.593281452Z E1024 13:15:25.593234       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:25.595066492Z I1024 13:15:25.595014       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:25.604272991Z E1024 13:15:25.604235       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:26.175137846Z I1024 13:15:26.175079       1 request.go:700] Waited for 1.579701772s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2024-10-24T13:15:26.582827769Z I1024 13:15:26.582726       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:15:26.582827769Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:15:26.582827769Z  CurrentRevision: (int32) 0,
2024-10-24T13:15:26.582827769Z  TargetRevision: (int32) 7,
2024-10-24T13:15:26.582827769Z  LastFailedRevision: (int32) 0,
2024-10-24T13:15:26.582827769Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:15:26.582827769Z  LastFailedReason: (string) "",
2024-10-24T13:15:26.582827769Z  LastFailedCount: (int) 0,
2024-10-24T13:15:26.582827769Z  LastFallbackCount: (int) 0,
2024-10-24T13:15:26.582827769Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:15:26.582827769Z }
2024-10-24T13:15:26.582827769Z  because new revision pending
2024-10-24T13:15:26.584482759Z W1024 13:15:26.584443       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:15:26.584482759Z W1024 13:15:26.584464       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:15:26.584482759Z W1024 13:15:26.584470       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:15:26.584482759Z W1024 13:15:26.584473       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:15:26.584482759Z W1024 13:15:26.584475       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:15:27.175201644Z I1024 13:15:27.175144       1 request.go:700] Waited for 1.3895386s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2024-10-24T13:15:27.386415715Z E1024 13:15:27.386350       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:27.386415715Z E1024 13:15:27.386379       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:27.388055625Z E1024 13:15:27.388014       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:28.374427882Z I1024 13:15:28.374361       1 request.go:700] Waited for 1.38762053s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:29.383832139Z I1024 13:15:29.383779       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:29.394208589Z E1024 13:15:29.394139       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:29.395985538Z I1024 13:15:29.395932       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:29.405300808Z E1024 13:15:29.405257       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:29.581876771Z E1024 13:15:29.581820       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:29.581876771Z E1024 13:15:29.581846       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:29.583363580Z E1024 13:15:29.583328       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:33.302703131Z I1024 13:15:33.302639       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:33.311041601Z E1024 13:15:33.310997       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:41.657408563Z E1024 13:15:41.657338       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:41.657408563Z E1024 13:15:41.657367       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:41.659235353Z E1024 13:15:41.659200       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:41.666921002Z I1024 13:15:41.665984       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:41.678248472Z E1024 13:15:41.678202       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:41.679975852Z I1024 13:15:41.679932       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:41.692989331Z E1024 13:15:41.692948       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:42.832164212Z E1024 13:15:42.832085       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:42.832164212Z E1024 13:15:42.832112       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:42.834155682Z E1024 13:15:42.834112       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:44.032673031Z I1024 13:15:44.032603       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:44.041711600Z E1024 13:15:44.041646       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:44.043446380Z I1024 13:15:44.043401       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:44.065441419Z E1024 13:15:44.065376       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:44.224933282Z I1024 13:15:44.224869       1 request.go:700] Waited for 1.122947132s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2024-10-24T13:15:44.632801105Z E1024 13:15:44.632700       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:44.632801105Z E1024 13:15:44.632734       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:44.634849245Z E1024 13:15:44.634798       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:46.232782396Z E1024 13:15:46.232695       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:46.232782396Z E1024 13:15:46.232726       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:46.234489366Z E1024 13:15:46.234442       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:46.433533978Z I1024 13:15:46.433448       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:46.442184208Z E1024 13:15:46.442116       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:46.444183317Z I1024 13:15:46.444133       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:46.454113457Z E1024 13:15:46.454083       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:47.233422434Z I1024 13:15:47.233359       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:47.242448573Z E1024 13:15:47.242404       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:47.245492973Z I1024 13:15:47.245443       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:47.254597493Z E1024 13:15:47.254566       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:48.635738423Z E1024 13:15:48.635683       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:15:48.635738423Z E1024 13:15:48.635716       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:15:48.637484753Z E1024 13:15:48.637439       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]"
2024-10-24T13:15:49.033967027Z I1024 13:15:49.033906       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:49.042582946Z E1024 13:15:49.042541       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:49.045184996Z I1024 13:15:49.045135       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:15:49.054274046Z E1024 13:15:49.054234       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io \"kube-apiserver\": the object has been modified; please apply your changes to the latest version and try again"
2024-10-24T13:15:50.231882385Z I1024 13:15:50.231818       1 installer_controller.go:515] "ci-op-2fcpj5j6-f6035-2lklf-master-0" moving to (v1.NodeStatus) {
2024-10-24T13:15:50.231882385Z  NodeName: (string) (len=35) "ci-op-2fcpj5j6-f6035-2lklf-master-0",
2024-10-24T13:15:50.231882385Z  CurrentRevision: (int32) 0,
2024-10-24T13:15:50.231882385Z  TargetRevision: (int32) 7,
2024-10-24T13:15:50.231882385Z  LastFailedRevision: (int32) 0,
2024-10-24T13:15:50.231882385Z  LastFailedTime: (*v1.Time)(<nil>),
2024-10-24T13:15:50.231882385Z  LastFailedReason: (string) "",
2024-10-24T13:15:50.231882385Z  LastFailedCount: (int) 0,
2024-10-24T13:15:50.231882385Z  LastFallbackCount: (int) 0,
2024-10-24T13:15:50.231882385Z  LastFailedRevisionErrors: ([]string) <nil>
2024-10-24T13:15:50.231882385Z }
2024-10-24T13:15:50.231882385Z  because new revision pending
2024-10-24T13:15:50.233666255Z W1024 13:15:50.233607       1 dynamic_operator_client.go:352] .status.conditions["InstallerControllerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:15:50.233666255Z W1024 13:15:50.233634       1 dynamic_operator_client.go:355] .status.conditions["InstallerControllerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:15:50.233666255Z W1024 13:15:50.233640       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:15:50.233666255Z W1024 13:15:50.233643       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:15:50.233666255Z W1024 13:15:50.233647       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:15:51.147312656Z I1024 13:15:51.147247       1 reflector.go:368] Caches populated for *v1.Authentication from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:51.429038814Z I1024 13:15:51.428581       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:52.086110646Z I1024 13:15:52.086031       1 reflector.go:368] Caches populated for *v1.CustomResourceDefinition from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:52.086697766Z I1024 13:15:52.086631       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:15:52.087144395Z I1024 13:15:52.087117       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:15:52.087530306Z I1024 13:15:52.087507       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:15:52.177501062Z W1024 13:15:52.177427       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:52.469670929Z I1024 13:15:52.469600       1 reflector.go:368] Caches populated for *v1.MutatingWebhookConfiguration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:52.672239471Z I1024 13:15:52.672176       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:53.180578779Z W1024 13:15:53.180502       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:15:54.752448091Z I1024 13:15:54.752395       1 reflector.go:368] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:55.821421516Z W1024 13:15:55.821356       1 reflector.go:484] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: watch of *v1.Event ended with: Internal error occurred: etcdserver: no leader
2024-10-24T13:15:55.934250940Z I1024 13:15:55.934170       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:55.934701711Z I1024 13:15:55.934670       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:15:57.639550397Z I1024 13:15:57.639498       1 reflector.go:368] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:58.090225698Z I1024 13:15:58.090148       1 reflector.go:368] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:58.207862603Z I1024 13:15:58.207806       1 reflector.go:368] Caches populated for *v1.Endpoints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:59.126169294Z I1024 13:15:59.126092       1 reflector.go:368] Caches populated for *v1.ValidatingWebhookConfiguration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:59.193363441Z I1024 13:15:59.193312       1 reflector.go:368] Caches populated for *v1.APIServer from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:59.584265404Z I1024 13:15:59.583866       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:15:59.934127259Z I1024 13:15:59.934061       1 reflector.go:368] Caches populated for *v1.Image from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:00.497706975Z I1024 13:16:00.497645       1 reflector.go:368] Caches populated for *v1.Infrastructure from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:00.497952675Z I1024 13:16:00.497925       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.ci-op-2fcpj5j6-f6035.XXXXXXXXXXXXXXXXXXXXXX
2024-10-24T13:16:00.499141675Z I1024 13:16:00.499094       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:16:01.320531589Z I1024 13:16:01.320474       1 reflector.go:368] Caches populated for *v1.Network from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:01.320860550Z I1024 13:16:01.320808       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2024-10-24T13:16:01.917743674Z I1024 13:16:01.917382       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:02.044003529Z I1024 13:16:02.043956       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:02.044621749Z I1024 13:16:02.044539       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:16:02.145866764Z I1024 13:16:02.145469       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:03.049412675Z I1024 13:16:03.049011       1 reflector.go:368] Caches populated for *v1.SecurityContextConstraints from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:03.501251016Z I1024 13:16:03.501204       1 reflector.go:368] Caches populated for operator.openshift.io/v1, Resource=kubeapiservers from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:03.506892606Z I1024 13:16:03.506822       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:03.529083445Z I1024 13:16:03.508744       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:16:03.552519074Z I1024 13:16:03.552472       1 reflector.go:368] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:03.553347504Z I1024 13:16:03.553303       1 connectivity_check_controller.go:172] ConnectivityCheckController is waiting for transition to desired version (4.18.0-0.ci.test-2024-10-24-124520-ci-op-2fcpj5j6-latest) to be completed.
2024-10-24T13:16:03.745092295Z I1024 13:16:03.745024       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:04.529818372Z I1024 13:16:04.529766       1 reflector.go:368] Caches populated for *v1.Namespace from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:04.767791802Z I1024 13:16:04.767728       1 reflector.go:368] Caches populated for *v1.Service from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:04.881694957Z I1024 13:16:04.881346       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:05.728474330Z I1024 13:16:05.728398       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:05.888802014Z I1024 13:16:05.888709       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:06.176896391Z I1024 13:16:06.176820       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:06.512825937Z I1024 13:16:06.512742       1 reflector.go:368] Caches populated for *v1.Proxy from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:09.454624931Z I1024 13:16:09.454559       1 reflector.go:368] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:10.280044825Z I1024 13:16:10.280000       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:10.294279625Z I1024 13:16:10.294199       1 reflector.go:368] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:10.626120091Z E1024 13:16:10.625940       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:10.633991320Z I1024 13:16:10.633909       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:10.706274207Z E1024 13:16:10.706227       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:11.080456091Z I1024 13:16:11.080365       1 reflector.go:368] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:13.787858965Z I1024 13:16:13.787789       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:13.788317235Z E1024 13:16:13.788268       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1
2024-10-24T13:16:13.788317235Z E1024 13:16:13.788292       1 guard_controller.go:294] Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2
2024-10-24T13:16:13.950130938Z I1024 13:16:13.950028       1 reflector.go:368] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:15.053678171Z I1024 13:16:15.053611       1 reflector.go:368] Caches populated for *v1.OAuth from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:15.096806019Z I1024 13:16:15.096728       1 reflector.go:368] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:15.649116975Z I1024 13:16:15.647798       1 reflector.go:368] Caches populated for *v1.ClusterRoleBinding from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:16.219093971Z I1024 13:16:16.217563       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:16.836449054Z I1024 13:16:16.836398       1 reflector.go:368] Caches populated for *v1alpha1.StorageVersionMigration from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:17.705011497Z E1024 13:16:17.704930       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:17.718456276Z I1024 13:16:17.717850       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:17.896794679Z E1024 13:16:17.896719       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:18.543166741Z E1024 13:16:18.543122       1 leaderelection.go:429] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:16:18.650324316Z I1024 13:16:18.650276       1 reflector.go:368] Caches populated for *v1.Secret from k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243
2024-10-24T13:16:18.756793792Z I1024 13:16:18.756713       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:16:24.820233992Z E1024 13:16:24.820131       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:24.843546951Z I1024 13:16:24.843495       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:24.991987745Z E1024 13:16:24.991934       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:25.237464614Z W1024 13:16:25.237387       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerDegraded"].reason is missing; this will eventually be fatal
2024-10-24T13:16:25.237464614Z W1024 13:16:25.237420       1 dynamic_operator_client.go:355] .status.conditions["NodeInstallerDegraded"].message is missing; this will eventually be fatal
2024-10-24T13:16:25.237464614Z W1024 13:16:25.237434       1 dynamic_operator_client.go:352] .status.conditions["NodeInstallerProgressing"].reason is missing; this will eventually be fatal
2024-10-24T13:16:29.193934680Z E1024 13:16:29.193867       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: Timeout: request did not complete within requested timeout - context deadline exceeded"
2024-10-24T13:16:29.310053731Z W1024 13:16:29.309978       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:16:30.313148109Z W1024 13:16:30.313076       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:16:31.859806854Z E1024 13:16:31.859737       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:31.902056601Z I1024 13:16:31.902009       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:32.103626704Z E1024 13:16:32.103559       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:32.395799791Z E1024 13:16:32.395645       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Installer reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Installer\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:35.787123437Z E1024 13:16:35.787054       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:35.787189026Z I1024 13:16:35.787111       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 8: Internal error occurred: resource quota evaluation timed out
2024-10-24T13:16:35.791280776Z I1024 13:16:35.791214       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:16:38.911492644Z E1024 13:16:38.911414       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:38.994469347Z I1024 13:16:38.994416       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:39.128349157Z E1024 13:16:39.128278       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:39.338148519Z E1024 13:16:39.338080       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:39.423114743Z W1024 13:16:39.423034       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:16:40.426388671Z W1024 13:16:40.426310       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:16:46.003153849Z E1024 13:16:46.003104       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:46.151105387Z E1024 13:16:46.151053       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:46.165281765Z I1024 13:16:46.165227       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:49.450118799Z E1024 13:16:49.450051       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:49.537858652Z W1024 13:16:49.537789       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:16:50.541070201Z W1024 13:16:50.540993       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:16:52.762009442Z E1024 13:16:52.761953       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" event="&Event{ObjectMeta:{kube-apiserver-operator.180165e7f1d4c29e  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:StartingNewRevision,Message:new revision 8 triggered by \"required secret/localhost-recovery-client-token has changed\",Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:16:18.756084382 +0000 UTC m=+679.084932019,LastTimestamp:2024-10-24 13:16:18.756084382 +0000 UTC m=+679.084932019,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:16:52.817243897Z E1024 13:16:52.817159       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:52.817243897Z I1024 13:16:52.817209       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 8: Internal error occurred: resource quota evaluation timed out
2024-10-24T13:16:52.820711737Z I1024 13:16:52.820643       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:16:53.174365438Z E1024 13:16:53.174287       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:53.175130458Z E1024 13:16:53.175087       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:53.497294602Z I1024 13:16:53.497215       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:16:59.566023891Z E1024 13:16:59.565962       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:16:59.644572504Z W1024 13:16:59.644515       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:00.069884710Z E1024 13:17:00.069795       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request (get pods)"
2024-10-24T13:17:00.124773975Z E1024 13:17:00.124697       1 base_controller.go:271] "Unhandled Error" err="auditPolicyController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-AuditPolicy\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:00.194273590Z E1024 13:17:00.194204       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:00.507658484Z E1024 13:17:00.507588       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:00.508375994Z E1024 13:17:00.508343       1 base_controller.go:271] "Unhandled Error" err="highCPUUsageAlertController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request"
2024-10-24T13:17:00.510152754Z I1024 13:17:00.510084       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:17:00.647633433Z W1024 13:17:00.647565       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:07.215292421Z E1024 13:17:07.215224       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:07.519493316Z E1024 13:17:07.519429       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:07.521490766Z I1024 13:17:07.521426       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:17:07.528892825Z W1024 13:17:07.528836       1 base_controller.go:234] Updating status of "NodeKubeconfigController" failed: unable to ApplyStatus for operator using fieldManager "NodeKubeconfigController-reportDegraded": rpc error: code = DeadlineExceeded desc = context deadline exceeded
2024-10-24T13:17:07.528892825Z E1024 13:17:07.528882       1 base_controller.go:271] "Unhandled Error" err="NodeKubeconfigController reconciliation failed: \"secret/node-kubeconfigs\": the server was unable to return a response in the time allotted, but may still be processing the request (get secrets node-kubeconfigs)"
2024-10-24T13:17:09.671789531Z E1024 13:17:09.671689       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:09.761147774Z W1024 13:17:09.761081       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:09.845209678Z E1024 13:17:09.845118       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:09.845209678Z I1024 13:17:09.845165       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 8: Internal error occurred: resource quota evaluation timed out
2024-10-24T13:17:09.848865957Z I1024 13:17:09.848797       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:17:10.763829113Z W1024 13:17:10.763736       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:14.237127301Z E1024 13:17:14.237069       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:14.529813408Z E1024 13:17:14.529745       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:14.531589228Z I1024 13:17:14.531553       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:17:18.559025762Z E1024 13:17:18.558962       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io kube-apiserver-operator-lock)
2024-10-24T13:17:19.793639542Z E1024 13:17:19.793581       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:19.879058085Z W1024 13:17:19.878980       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:20.819211109Z W1024 13:17:20.819143       1 base_controller.go:234] Updating status of "GuardController" failed: unable to ApplyStatus for operator using fieldManager "GuardController-reportDegraded": rpc error: code = DeadlineExceeded desc = context deadline exceeded
2024-10-24T13:17:20.819211109Z E1024 13:17:20.819191       1 base_controller.go:271] "Unhandled Error" err="GuardController reconciliation failed: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2, the server was unable to return a response in the time allotted, but may still be processing the request (get pods kube-apiserver-guard-ci-op-2fcpj5j6-f6035-2lklf-master-0)]"
2024-10-24T13:17:20.882680514Z W1024 13:17:20.882556       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:21.256955994Z E1024 13:17:21.256883       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:21.540720580Z E1024 13:17:21.540652       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:21.543411220Z I1024 13:17:21.543368       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:17:26.765271088Z E1024 13:17:26.765211       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" event="&Event{ObjectMeta:{kube-apiserver-operator.180165ebe8f3e998  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:RevisionCreateFailed,Message:Failed to create revision 8: Internal error occurred: resource quota evaluation timed out,Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:16:35.787000216 +0000 UTC m=+696.115847823,LastTimestamp:2024-10-24 13:16:35.787000216 +0000 UTC m=+696.115847823,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:17:26.872528919Z E1024 13:17:26.872465       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:26.872580819Z I1024 13:17:26.872514       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 8: Internal error occurred: resource quota evaluation timed out
2024-10-24T13:17:26.875793919Z I1024 13:17:26.875739       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:17:28.293281444Z E1024 13:17:28.293222       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:28.552486443Z E1024 13:17:28.552431       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:28.554354593Z I1024 13:17:28.554318       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:17:29.910069994Z E1024 13:17:29.910007       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:29.999250716Z W1024 13:17:29.999163       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:31.002934875Z W1024 13:17:31.002856       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:32.752283483Z W1024 13:17:32.752216       1 reflector.go:561] k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: failed to list *v1.Event: the server was unable to return a response in the time allotted, but may still be processing the request (get events)
2024-10-24T13:17:32.752323323Z I1024 13:17:32.752285       1 trace.go:236] Trace[1553752963]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243 (24-Oct-2024 13:16:32.749) (total time: 60003ms):
2024-10-24T13:17:32.752323323Z Trace[1553752963]: ---"Objects listed" error:the server was unable to return a response in the time allotted, but may still be processing the request (get events) 60003ms (13:17:32.752)
2024-10-24T13:17:32.752323323Z Trace[1553752963]: [1m0.003159281s] [1m0.003159281s] END
2024-10-24T13:17:32.752323323Z E1024 13:17:32.752315       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go@v0.31.1/tools/cache/reflector.go:243: Failed to watch *v1.Event: failed to list *v1.Event: the server was unable to return a response in the time allotted, but may still be processing the request (get events)"
2024-10-24T13:17:35.316323795Z E1024 13:17:35.316255       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:35.562519335Z E1024 13:17:35.562456       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:37.531254684Z E1024 13:17:37.531187       1 base_controller.go:271] "Unhandled Error" err=<
2024-10-24T13:17:37.531254684Z 	PruneController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request (get pods revision-pruner-7-ci-op-2fcpj5j6-f6035-2lklf-master-0)
2024-10-24T13:17:37.531254684Z 	Timeout: request did not complete within requested timeout - context deadline exceeded
2024-10-24T13:17:37.531254684Z  >
2024-10-24T13:17:38.795526652Z I1024 13:17:38.795468       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:17:40.026650452Z E1024 13:17:40.026577       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:40.108673825Z W1024 13:17:40.108619       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:41.111686234Z W1024 13:17:41.111617       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:43.899662088Z E1024 13:17:43.899596       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:43.899662088Z I1024 13:17:43.899639       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 8: Internal error occurred: resource quota evaluation timed out
2024-10-24T13:17:43.902923448Z I1024 13:17:43.902820       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:17:45.556977164Z E1024 13:17:45.556905       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-Node reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-Node\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:45.804391193Z E1024 13:17:45.804335       1 base_controller.go:271] "Unhandled Error" err="StatusSyncer_kube-apiserver reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:50.135019883Z E1024 13:17:50.134938       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:17:50.229783035Z W1024 13:17:50.229687       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:51.232998964Z W1024 13:17:51.232933       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:17:51.567058097Z E1024 13:17:51.566984       1 leaderelection.go:429] Failed to update lock optimitically: rpc error: code = DeadlineExceeded desc = context deadline exceeded, falling back to slow path
2024-10-24T13:17:58.508523464Z E1024 13:17:58.508457       1 leaderelection.go:436] error retrieving resource lock openshift-kube-apiserver-operator/kube-apiserver-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-apiserver-operator/leases/kube-apiserver-operator-lock?timeout=1m47s": context deadline exceeded
2024-10-24T13:17:58.508523464Z I1024 13:17:58.508505       1 leaderelection.go:297] failed to renew lease openshift-kube-apiserver-operator/kube-apiserver-operator-lock: timed out waiting for the condition
2024-10-24T13:18:00.073591028Z E1024 13:18:00.073516       1 base_controller.go:271] "Unhandled Error" err="kube-apiserver-InstallerState reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request (get pods)"
2024-10-24T13:18:00.256953483Z E1024 13:18:00.256897       1 base_controller.go:271] "Unhandled Error" err="webhookSupportabilityController reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:18:00.260029853Z I1024 13:18:00.259968       1 status_controller.go:225] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2024-10-24T13:05:35Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): rpc error: code = Unknown desc = malformed header: missing HTTP content-type\nBackingResourceControllerDegraded: \nGuardControllerDegraded: [Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-1, Missing operand on node ci-op-2fcpj5j6-f6035-2lklf-master-2]\nKubeAPIServerStaticResourcesDegraded: \"assets/kube-apiserver/ns.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces openshift-kube-apiserver)\nKubeAPIServerStaticResourcesDegraded: \nTargetConfigControllerDegraded: \"configmap/kube-apiserver-pod\": the server was unable to return a response in the time allotted, but may still be processing the request (get configmaps kube-apiserver-pod)","reason":"BackingResourceController_SyncError::GuardController_SyncError::KubeAPIServerStaticResources_SyncError::TargetConfigController_SynchronizationError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-10-24T13:05:18Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-10-24T13:03:33Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 7","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-10-24T13:03:30Z","message":"All is well","reason":"AsExpected","status":"False","type":"EvaluationConditionsDetected"}]}}
2024-10-24T13:18:00.341150726Z W1024 13:18:00.341079       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:18:00.513165882Z E1024 13:18:00.513096       1 base_controller.go:271] "Unhandled Error" err="highCPUUsageAlertController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request"
2024-10-24T13:18:00.768389972Z E1024 13:18:00.768326       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" event="&Event{ObjectMeta:{kube-apiserver-operator.180165e7f1d4c29e  openshift-kube-apiserver-operator    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Deployment,Namespace:openshift-kube-apiserver-operator,Name:kube-apiserver-operator,UID:25c703bd-fced-4623-ab7c-91be89cc0822,APIVersion:apps/v1,ResourceVersion:,FieldPath:,},Reason:StartingNewRevision,Message:new revision 8 triggered by \"required secret/localhost-recovery-client-token has changed\",Source:EventSource{Component:kube-apiserver-operator-revisioncontroller,Host:,},FirstTimestamp:2024-10-24 13:16:18.756084382 +0000 UTC m=+679.084932019,LastTimestamp:2024-10-24 13:16:35.791104926 +0000 UTC m=+696.119952533,Count:2,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kube-apiserver-operator-revisioncontroller,ReportingInstance:,}"
2024-10-24T13:18:00.926189259Z E1024 13:18:00.926120       1 base_controller.go:271] "Unhandled Error" err="RevisionController reconciliation failed: unable to ApplyStatus for operator using fieldManager \"kube-apiserver-RevisionController\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
2024-10-24T13:18:00.926236909Z I1024 13:18:00.926179       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 8: Internal error occurred: resource quota evaluation timed out
2024-10-24T13:18:00.930126408Z I1024 13:18:00.929791       1 event.go:377] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"25c703bd-fced-4623-ab7c-91be89cc0822", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/localhost-recovery-client-token has changed"
2024-10-24T13:18:01.344769675Z W1024 13:18:01.344672       1 degraded_webhook.go:147] failed to connect to webhook "alertmanagerconfigs.monitoring.coreos.com" via service "prometheus-operator-admission-webhook.openshift-monitoring.svc:8443": dial tcp 172.30.254.42:8443: connect: connection refused
2024-10-24T13:18:05.117092889Z E1024 13:18:05.117016       1 base_controller.go:271] "Unhandled Error" err="BackingResourceController-StaticResources reconciliation failed: [\"manifests/installer-sa.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get serviceaccounts installer-sa), \"manifests/installer-cluster-rolebinding.yaml\" (string): the server was unable to return a response in the time allotted, but may still be processing the request (get clusterrolebindings.rbac.authorization.k8s.io system:openshift:operator:openshift-kube-apiserver-installer), unable to ApplyStatus for operator using fieldManager \"BackingResourceController-StaticResources\": rpc error: code = DeadlineExceeded desc = context deadline exceeded]"
2024-10-24T13:18:05.515720787Z E1024 13:18:05.515648       1 leaderelection.go:322] Failed to release lock: rpc error: code = DeadlineExceeded desc = context deadline exceeded
2024-10-24T13:18:05.515796957Z W1024 13:18:05.515721       1 leaderelection.go:84] leader election lost
